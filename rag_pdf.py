import base64
import gradio as gr
import os
import shutil
import pymupdf as fitz  # PyMuPDF
import numpy as np
from PIL import Image
import io
import chromadb
from chromadb.config import Settings
from sentence_transformers import SentenceTransformer
from pythainlp.tokenize import word_tokenize
from transformers import MT5Tokenizer, MT5ForConditionalGeneration

import torch
import ollama
import shortuuid
import logging
import re
import time
import discord

def log_with_time(message):
    """Log message with timestamp and timing information"""
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    return logging.info(f"[{timestamp}] {message}")

def measure_time(start_time, label):
    """Measure and log elapsed time from start_time"""
    elapsed = time.time() - start_time
    log_with_time(f"‚è±Ô∏è {label}: {elapsed:.2f}s")
    return elapsed
import pandas as pd
import asyncio
import requests
from dotenv import load_dotenv
from flask import Flask, request, abort
from linebot import LineBotApi, WebhookHandler
from linebot.exceptions import InvalidSignatureError
from linebot.models import MessageEvent, TextMessage, TextSendMessage
from typing import List, Dict, Tuple, Union
import threading
import docx
from pathlib import Path
import json
import hashlib
from collections import deque
from datetime import datetime

# Authentication imports
try:
    from auth_models import auth_manager, require_auth
    from login_page import get_current_user_info, logout_current_user
    AUTH_ENABLED = True
    logging.info("‚úÖ Authentication system loaded successfully")
except ImportError as e:
    AUTH_ENABLED = False
    logging.warning(f"‚ö†Ô∏è Authentication system not available: {e}")
    # Fallback functions if auth models not available
    def auth_manager():
        return None
    def get_current_user_info():
        return {"authenticated": False, "user": None, "token": None}
    def require_auth(func):
        return func
    def logout_current_user():
        return "Authentication not available"

# Additional AI Provider imports
try:
    import openai
    OPENAI_AVAILABLE = True
except ImportError:
    OPENAI_AVAILABLE = False

try:
    import google.generativeai as genai
    GEMINI_AVAILABLE = True
    logging.info("‚úÖ Google Generative AI library available")
except ImportError as e:
    GEMINI_AVAILABLE = False
    logging.warning(f"‚ö†Ô∏è Google Generative AI library not available: {e}")
    logging.info("Install with: pip install google-generativeai")

# LightRAG imports for graph reasoning
try:
    from lightrag_integration import initialize_lightrag_system, query_with_graph_reasoning, multi_hop_reasoning, get_lightrag_status
    LIGHT_RAG_AVAILABLE = True
    logging.info("‚úÖ LightRAG integration loaded successfully")
except ImportError as e:
    logging.warning(f"‚ö†Ô∏è LightRAG integration not available: {e}")
    LIGHT_RAG_AVAILABLE = False

# Load environment variables from .env file
load_dotenv()

# Image folder
TEMP_IMG="./data/images"
TEMP_VECTOR="./data/chromadb"
TEMP_VECTOR_BACKUP="./data/chromadb_backup"
# ‡∏£‡∏≤‡∏¢‡∏ä‡∏∑‡πà‡∏≠ Model ‡∏ó‡∏µ‡πà‡∏Ñ‡∏∏‡∏ì‡∏°‡∏µ‡∏ö‡∏ô Ollama
AVAILABLE_MODELS = ["gemma3:latest", "qwen3:latest","llama3.2:latest"]

# AI Provider Configuration
AI_PROVIDERS = {
    "ollama": {
        "name": "Ollama (Local)",
        "models": AVAILABLE_MODELS,
        "api_key_required": False,
        "default_model": "gemma3:latest"
    },
    "minimax": {
        "name": "Minimax",
        "models": ["abab6.5", "abab6.5s", "abab5.5"],
        "api_key_required": True,
        "api_key_env": "MINIMAX_API_KEY",
        "base_url": "https://api.minimax.chat/v1",
        "default_model": "abab6.5"
    },
    "manus": {
        "name": "Manus",
        "models": ["manus-code", "manus-reasoning", "manus-vision"],
        "api_key_required": True,
        "api_key_env": "MANUS_API_KEY",
        "base_url": "https://api.manus.ai/v1",
        "default_model": "manus-code"
    },
    "gemini": {
        "name": "Google Gemini",
        "models": ["gemini-2.5-pro", "gemini-2.5-flash", "gemini-2.0-flash-exp", "gemini-1.5-pro", "gemini-1.5-flash", "gemini-1.0-pro"],
        "api_key_required": True,
        "api_key_env": "GEMINI_API_KEY",
        "base_url": "https://generativelanguage.googleapis.com/v1",
        "default_model": "gemini-2.5-pro"
    },
    "zhipu": {
        "name": "Zhipu AI (GLM)",
        "models": ["GLM-4.6", "glm-4.6", "glm-4", "glm-4v", "glm-3-turbo"],
        "api_key_required": True,
        "api_key_env": "ZHIPU_API_KEY",
        "base_url": "https://api.z.ai/api/paas/v4",
        "default_model": "GLM-4.6"
    },
    "chatgpt": {
        "name": "ChatGPT (OpenAI)",
        "models": ["gpt-4o", "gpt-4o-mini", "gpt-4-turbo", "gpt-3.5-turbo"],
        "api_key_required": True,
        "api_key_env": "OPENAI_API_KEY",
        "base_url": "https://api.openai.com/v1",
        "default_model": "gpt-4o"
    }
}

# Default AI Provider
DEFAULT_AI_PROVIDER = os.getenv("DEFAULT_AI_PROVIDER", "gemini")

def get_ai_provider_config(provider_name: str) -> dict:
    """Get AI provider configuration"""
    return AI_PROVIDERS.get(provider_name, AI_PROVIDERS["ollama"])

def get_available_providers() -> list:
    """Get list of available AI providers"""
    providers = []
    for key, config in AI_PROVIDERS.items():
        if key == "ollama":
            providers.append(key)
        elif config["api_key_required"]:
            api_key = os.getenv(config["api_key_env"])
            if api_key and api_key.strip():
                # Special check for Gemini - require library too
                if key == "gemini" and not GEMINI_AVAILABLE:
                    continue
                providers.append(key)
        else:
            providers.append(key)
    return providers

def get_provider_models(provider_name: str) -> list:
    """Get available models for a provider"""
    config = get_ai_provider_config(provider_name)
    return config.get("models", [])

def call_ai_provider(provider_name: str, model: str, messages: list, stream: bool = True, **kwargs):
    """
    Unified function to call different AI providers

    Args:
        provider_name: Name of the AI provider (ollama, minimax, manus, gemini, chatgpt)
        model: Model name to use
        messages: List of messages in format [{"role": "user", "content": "..."}]
        stream: Whether to stream the response
        **kwargs: Additional parameters for the specific provider

    Returns:
        Response stream or response object
    """
    config = get_ai_provider_config(provider_name)

    if provider_name == "ollama":
        # Use existing Ollama implementation
        try:
            return ollama.chat(
                model=model,
                messages=messages,
                stream=stream,
                options={
                    "temperature": kwargs.get("temperature", 0.3),
                    "top_p": kwargs.get("top_p", 0.9),
                    "max_tokens": kwargs.get("max_tokens", 2000),
                    "num_predict": kwargs.get("num_predict", 1500)
                }
            )
        except Exception as e:
            logging.error(f"Ollama API error: {e}")
            raise

    elif provider_name == "chatgpt":
        if not OPENAI_AVAILABLE:
            raise ImportError("OpenAI library not available. Install with: pip install openai")

        api_key = os.getenv(config["api_key_env"])
        if not api_key:
            raise ValueError(f"API key not found for {provider_name}. Set {config['api_key_env']} environment variable.")

        try:
            client = openai.OpenAI(api_key=api_key)

            if stream:
                return client.chat.completions.create(
                    model=model,
                    messages=messages,
                    stream=True,
                    temperature=kwargs.get("temperature", 0.3),
                    max_tokens=kwargs.get("max_tokens", 2000),
                    top_p=kwargs.get("top_p", 0.9)
                )
            else:
                return client.chat.completions.create(
                    model=model,
                    messages=messages,
                    temperature=kwargs.get("temperature", 0.3),
                    max_tokens=kwargs.get("max_tokens", 2000),
                    top_p=kwargs.get("top_p", 0.9)
                )
        except Exception as e:
            logging.error(f"OpenAI API error: {e}")
            raise

    elif provider_name == "gemini":
        if not GEMINI_AVAILABLE:
            error_msg = "Google Generative AI library not available. Please install with: pip install google-generativeai"
            logging.error(error_msg)
            return ({"message": {"content": error_msg}} for _ in range(1))

        api_key = os.getenv(config["api_key_env"])
        if not api_key:
            raise ValueError(f"API key not found for {provider_name}. Set {config['api_key_env']} environment variable.")

        try:
            genai.configure(api_key=api_key)
            model_obj = genai.GenerativeModel(model)

            # Convert messages to Gemini format
            prompt = ""
            for msg in messages:
                if msg["role"] == "user":
                    prompt += f"User: {msg['content']}\n"
                elif msg["role"] == "assistant":
                    prompt += f"Assistant: {msg['content']}\n"

            prompt += "Assistant: "

            response = model_obj.generate_content(
                prompt,
                stream=stream,
                generation_config=genai.types.GenerationConfig(
                    temperature=kwargs.get("temperature", 0.3),
                    max_output_tokens=kwargs.get("max_tokens", 2000),
                    top_p=kwargs.get("top_p", 0.9)
                )
            )

            # Convert Gemini stream to match Ollama/OpenAI format
            def gemini_stream_converter(gemini_response):
                for chunk in gemini_response:
                    if chunk.text:
                        yield {"message": {"content": chunk.text}}

            return gemini_stream_converter(response)
        except Exception as e:
            logging.error(f"Gemini API error: {e}")
            raise

    elif provider_name in ["minimax", "manus", "zhipu"]:
        # Generic OpenAI-compatible API implementation
        api_key = os.getenv(config["api_key_env"])
        if not api_key:
            raise ValueError(f"API key not found for {provider_name}. Set {config['api_key_env']} environment variable.")

        try:
            client = openai.OpenAI(
                api_key=api_key,
                base_url=config["base_url"]
            )

            if stream:
                return client.chat.completions.create(
                    model=model,
                    messages=messages,
                    stream=True,
                    temperature=kwargs.get("temperature", 0.3),
                    max_tokens=kwargs.get("max_tokens", 2000),
                    top_p=kwargs.get("top_p", 0.9)
                )
            else:
                return client.chat.completions.create(
                    model=model,
                    messages=messages,
                    temperature=kwargs.get("temperature", 0.3),
                    max_tokens=kwargs.get("max_tokens", 2000),
                    top_p=kwargs.get("top_p", 0.9)
                )
        except Exception as e:
            logging.error(f"{provider_name} API error: {e}")
            raise

    else:
        raise ValueError(f"Unsupported AI provider: {provider_name}")

# Discord Configuration
DISCORD_WEBHOOK_URL = os.getenv("DISCORD_WEBHOOK_URL", "YOUR_WEBHOOK_URL_HERE")  # ‡πÉ‡∏™‡πà Webhook URL ‡∏ó‡∏µ‡πà‡∏ô‡∏µ‡πà
DISCORD_BOT_TOKEN = os.getenv("DISCORD_BOT_TOKEN", "YOUR_BOT_TOKEN_HERE")  # ‡πÉ‡∏™‡πà Bot Token ‡∏ó‡∏µ‡πà‡∏ô‡∏µ‡πà
DISCORD_CHANNEL_ID = os.getenv("DISCORD_CHANNEL_ID", "YOUR_CHANNEL_ID_HERE")  # ‡πÉ‡∏™‡πà Channel ID ‡∏ó‡∏µ‡πà‡∏ô‡∏µ‡πà
DISCORD_ENABLED = os.getenv("DISCORD_ENABLED", "false").lower() == "true"  # ‡πÄ‡∏õ‡∏¥‡∏î/‡∏õ‡∏¥‡∏î‡∏Å‡∏≤‡∏£‡πÅ‡∏à‡πâ‡∏á‡πÄ‡∏ï‡∏∑‡∏≠‡∏ô Discord

# Discord Bot Configuration
DISCORD_BOT_ENABLED = os.getenv("DISCORD_BOT_ENABLED", "false").lower() == "true"  # ‡πÄ‡∏õ‡∏¥‡∏î/‡∏õ‡∏¥‡∏î Bot ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏£‡∏±‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°
DISCORD_BOT_PREFIX = os.getenv("DISCORD_BOT_PREFIX", "!ask ")  # ‡∏Ñ‡∏≥‡∏ô‡∏≥‡∏´‡∏ô‡πâ‡∏≤‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á
DISCORD_DEFAULT_MODEL = os.getenv("DISCORD_DEFAULT_MODEL", "gemma3:latest")  # ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Discord
DISCORD_RESPOND_NO_PREFIX = os.getenv("DISCORD_RESPOND_NO_PREFIX", "true").lower() == "true"  # ‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏°‡∏µ prefix ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
DISCORD_REPLY_MODE = os.getenv("DISCORD_REPLY_MODE", "channel")  # ‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£‡∏ï‡∏≠‡∏ö‡∏Å‡∏•‡∏±‡∏ö: channel/dm/both

# LINE OA Configuration
LINE_CHANNEL_ACCESS_TOKEN = os.getenv("LINE_CHANNEL_ACCESS_TOKEN", "YOUR_LINE_CHANNEL_ACCESS_TOKEN")
LINE_CHANNEL_SECRET = os.getenv("LINE_CHANNEL_SECRET", "YOUR_LINE_CHANNEL_SECRET")
LINE_ENABLED = os.getenv("LINE_ENABLED", "false").lower() == "true"  # ‡πÄ‡∏õ‡∏¥‡∏î/‡∏õ‡∏¥‡∏î LINE OA
LINE_DEFAULT_MODEL = os.getenv("LINE_DEFAULT_MODEL", "gemma3:latest")  # ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö LINE
LINE_WEBHOOK_PORT = int(os.getenv("LINE_WEBHOOK_PORT", "5000"))  # Port ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö LINE webhook

# Global state for current model and provider (shared across chat and bots)
current_model = None  # Will be set from chat interface
current_provider = None  # Will be set from chat interface

# Facebook Messenger Configuration
FB_PAGE_ACCESS_TOKEN = os.getenv("FB_PAGE_ACCESS_TOKEN", "YOUR_FB_PAGE_ACCESS_TOKEN")
FB_VERIFY_TOKEN = os.getenv("FB_VERIFY_TOKEN", "YOUR_FB_VERIFY_TOKEN")
FB_ENABLED = os.getenv("FB_ENABLED", "false").lower() == "true"  # ‡πÄ‡∏õ‡∏¥‡∏î/‡∏õ‡∏¥‡∏î Facebook Messenger
FB_DEFAULT_MODEL = os.getenv("FB_DEFAULT_MODEL", "gemma3:latest")  # ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö FB
FB_WEBHOOK_PORT = int(os.getenv("FB_WEBHOOK_PORT", "5001"))  # Port ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö FB webhook
FB_WEBHOOK = int(os.getenv("FB_WEBHOOK", "5001"))  # Port ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö FB webhook

# Enhanced RAG Configuration (MemoRAG-like features)
RAG_MODE = os.getenv("RAG_MODE", "enhanced")  # standard, enhanced
MEMORY_WINDOW_SIZE = int(os.getenv("MEMORY_WINDOW_SIZE", "5"))  # ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô conversations ‡∏ó‡∏µ‡πà‡∏à‡∏≥‡πÑ‡∏ß‡πâ
ENABLE_CONTEXT_CHAINING = os.getenv("ENABLE_CONTEXT_CHAINING", "true").lower() == "true"
ENABLE_REASONING = os.getenv("ENABLE_REASONING", "true").lower() == "true"
ENABLE_SESSION_MEMORY = os.getenv("ENABLE_SESSION_MEMORY", "true").lower() == "true"

# Setup logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


# Initialize Chroma client Disable telemetry
os.environ["CHROMA_TELEMETRY_ENABLED"] = "false"

# ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö database ‡∏ñ‡πâ‡∏≤‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ
os.makedirs(TEMP_VECTOR, exist_ok=True)
os.makedirs(TEMP_VECTOR_BACKUP, exist_ok=True)

# ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ ChromaDB ‡πÉ‡∏´‡πâ‡πÄ‡∏Å‡πá‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏ö‡∏ö‡∏ñ‡∏≤‡∏ß‡∏£‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏î‡∏µ‡∏Ç‡∏∂‡πâ‡∏ô
settings = Settings(
    anonymized_telemetry=False,
    allow_reset=False,
    is_persistent=True
)

def cleanup_database_locks():
    """Clean up ChromaDB lock files that may cause access issues"""
    import glob
    import os
    import time
    import shutil

    try:
        db_path = TEMP_VECTOR
        if not os.path.exists(db_path):
            return True

        log_with_time("üîß Cleaning up database lock files...")

        # More aggressive lock file removal
        lock_patterns = [
            os.path.join(db_path, "**", "*.lock"),
            os.path.join(db_path, "**", "*-wal"),
            os.path.join(db_path, "**", "*-shm"),
            os.path.join(db_path, "**", "data_level0.bin"),
            os.path.join(db_path, "**", "*.db"),
            os.path.join(db_path, "**", "*.sqlite")
        ]

        removed_count = 0
        for pattern in lock_patterns:
            for file_path in glob.glob(pattern, recursive=True):
                try:
                    # Try normal removal first
                    os.remove(file_path)
                    log_with_time(f"   Removed: {os.path.basename(file_path)}")
                    removed_count += 1
                except PermissionError as pe:
                    # File is locked, try to force unlock by changing attributes
                    try:
                        import stat
                        os.chmod(file_path, stat.S_IWRITE)
                        os.remove(file_path)
                        log_with_time(f"   Force removed: {os.path.basename(file_path)}")
                        removed_count += 1
                    except:
                        log_with_time(f"   ‚ö†Ô∏è Cannot remove (locked): {os.path.basename(file_path)}")
                        # Try renaming as last resort
                        try:
                            backup_name = f"{file_path}.locked_{int(time.time())}"
                            os.rename(file_path, backup_name)
                            log_with_time(f"   Renamed locked file: {os.path.basename(file_path)}")
                        except:
                            log_with_time(f"   ‚ùå Cannot access: {os.path.basename(file_path)}")
                except Exception as e:
                    log_with_time(f"   Could not remove {file_path}: {e}")

        if removed_count > 0:
            log_with_time(f"‚úÖ Removed {removed_count} lock files")
            time.sleep(2)  # Wait longer for file system to release
        else:
            log_with_time("‚ÑπÔ∏è No lock files found")

        return True

    except Exception as e:
        log_with_time(f"Error cleaning lock files: {e}")
        return False

def force_release_chromadb():
    """Force release ChromaDB by killing related connections and files"""
    try:
        log_with_time("üö® Force releasing ChromaDB...")

        # Close any existing connections
        try:
            if 'collection' in globals():
                collection = None
                del collection
            if 'chroma_client' in globals():
                chroma_client = None
                del chroma_client
            log_with_time("Closed existing ChromaDB connections")
        except:
            pass

        # Cleanup lock files more aggressively
        cleanup_database_locks()

        # Kill any Python processes that might be holding the lock
        import subprocess
        import psutil
        import os

        try:
            current_pid = os.getpid()
            for proc in psutil.process_iter(['pid', 'name', 'cmdline']):
                try:
                    if proc.info['pid'] != current_pid and proc.info['name'] == 'python.exe':
                        cmdline = ' '.join(proc.info['cmdline'] or [])
                        if 'rag_pdf.py' in cmdline or 'chromadb' in cmdline.lower():
                            log_with_time(f"Killing process {proc.info['pid']} that might be holding DB lock")
                            proc.terminate()
                            proc.wait(timeout=3)
                except:
                    pass
        except:
            pass

        # Force garbage collection multiple times
        import gc
        gc.collect()
        gc.collect()

        log_with_time("‚úÖ Force release completed")
        return True
    except Exception as e:
        log_with_time(f"Force release failed: {e}")
        return False

def move_database_to_temp():
    """Move locked database to temp location to avoid conflicts"""
    try:
        import shutil
        import tempfile

        if os.path.exists(TEMP_VECTOR):
            # Create temp directory
            temp_dir = tempfile.mkdtemp(prefix="chromadb_backup_")
            temp_db_path = os.path.join(temp_dir, "chromadb")

            log_with_time(f"Moving locked DB to temp location: {temp_db_path}")

            # Move the entire database directory
            shutil.move(TEMP_VECTOR, temp_db_path)

            # Create empty database directory
            os.makedirs(TEMP_VECTOR, exist_ok=True)

            log_with_time(f"‚úÖ Database moved to: {temp_db_path}")
            return temp_db_path
        return None
    except Exception as e:
        log_with_time(f"Failed to move database: {e}")
        return None

def create_fresh_database():
    """Create a completely fresh database to avoid lock issues"""
    try:
        import shutil
        import tempfile

        log_with_time("Creating fresh database to avoid lock issues...")

        # If database exists, move it to temp first
        moved_path = move_database_to_temp()

        # Create new empty database directory
        os.makedirs(TEMP_VECTOR, exist_ok=True)

        # Wait a moment for file system to sync
        import time
        time.sleep(1)

        log_with_time("‚úÖ Fresh database created successfully")
        return True
    except Exception as e:
        log_with_time(f"Failed to create fresh database: {e}")
        return False

# Clean up locks before initializing database
cleanup_database_locks()

chroma_client = chromadb.PersistentClient(
    path=TEMP_VECTOR,
    settings=settings
)

# ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏´‡∏£‡∏∑‡∏≠‡∏î‡∏∂‡∏á collection ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß
try:
    # ‡∏û‡∏¢‡∏≤‡∏¢‡∏≤‡∏°‡πÇ‡∏´‡∏•‡∏î collection ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡∏Å‡πà‡∏≠‡∏ô
    collection = chroma_client.get_collection(name="pdf_data")
    count = collection.count()
    logging.info(f"‚úÖ ‡πÇ‡∏´‡∏•‡∏î collection 'pdf_data' ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à - ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô {count} ‡πÄ‡∏£‡∏Ñ‡∏Ñ‡∏≠‡∏£‡πå‡∏î")
    logging.info(f"üìÅ Database path: {TEMP_VECTOR}")
    logging.info(f"üíæ Database exists: {os.path.exists(TEMP_VECTOR)}")

    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏£‡∏¥‡∏á‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
    if count == 0:
        logging.warning("‚ö†Ô∏è Collection is empty! Checking for data persistence issues...")

        # ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö
        try:
            from fix_database_persistence import scan_collection_directories, get_sqlite_collection_info, reconstruct_collection
        except ImportError:
            logging.warning("‚ö†Ô∏è fix_database_persistence module not found, skipping persistence check")
            count = 0  # Set to 0 to indicate no data available

        collection_dirs = scan_collection_directories()
        sqlite_collections = get_sqlite_collection_info()

        sqlite_ids = {coll[0] for coll in sqlite_collections}
        has_data_dirs = [col['id'] for col in collection_dirs if col['has_data']]

        if sqlite_collections and has_data_dirs:
            logging.info("üîß Attempting to recover data from file system...")
            # ‡∏û‡∏¢‡∏≤‡∏¢‡∏≤‡∏Å‡πÉ‡∏ä‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å directories ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
            for coll_id, coll_name in sqlite_collections:
                if coll_id in has_data_dirs:
                    logging.info(f"   Trying to restore: {coll_name}")
                    # TODO: Implement data recovery from directories
        else:
            logging.warning("‚ùå No recoverable data found - database appears to be empty")

except Exception as e:
    # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÉ‡∏´‡πâ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÉ‡∏´‡∏°‡πà
    logging.info(f"‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö collection ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà - ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÉ‡∏´‡∏°‡πà: {str(e)}")
    collection = chroma_client.get_or_create_collection(name="pdf_data")
    logging.info(f"‚úÖ ‡∏™‡∏£‡πâ‡∏≤‡∏á collection 'pdf_data' ‡πÉ‡∏´‡∏°‡πà‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à")

# Feedback Database Setup
FEEDBACK_DB_PATH = "./data/feedback.db"
os.makedirs(os.path.dirname(FEEDBACK_DB_PATH), exist_ok=True)

import sqlite3
from datetime import datetime

def init_feedback_db():
    """‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• feedback ‡∏ñ‡πâ‡∏≤‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ"""
    conn = sqlite3.connect(FEEDBACK_DB_PATH)
    cursor = conn.cursor()

    cursor.execute('''
        CREATE TABLE IF NOT EXISTS feedback (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            question TEXT NOT NULL,
            answer TEXT NOT NULL,
            feedback_type TEXT NOT NULL,  -- 'good' or 'bad'
            user_comment TEXT,
            corrected_answer TEXT,
            timestamp DATETIME DEFAULT CURRENT_TIMESTAMP,
            model_used TEXT,
            sources TEXT  -- JSON array of source info
        )
    ''')

    cursor.execute('''
        CREATE INDEX IF NOT EXISTS idx_feedback_timestamp ON feedback(timestamp)
    ''')

    cursor.execute('''
        CREATE INDEX IF NOT EXISTS idx_feedback_type ON feedback(feedback_type)
    ''')

    cursor.execute('''
        CREATE INDEX IF NOT EXISTS idx_feedback_question ON feedback(question)
    ''')

    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÅ‡∏•‡∏∞‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡πÉ‡∏´‡∏°‡πà‡∏ñ‡πâ‡∏≤‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô
    try:
        cursor.execute('ALTER TABLE feedback ADD COLUMN applied BOOLEAN DEFAULT FALSE')
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_feedback_applied ON feedback(applied)')
        logging.info("‚úÖ Added 'applied' column to feedback table")
    except sqlite3.OperationalError as e:
        if "duplicate column name" in str(e):
            logging.info("‚úÖ 'applied' column already exists in feedback table")
        else:
            logging.warning(f"‚ö†Ô∏è Error adding 'applied' column: {str(e)}")

    # ‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏Å‡πá‡∏ö‡∏Ñ‡∏π‡πà‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°-‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS corrected_answers (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            original_question TEXT NOT NULL,
            original_answer TEXT NOT NULL,
            corrected_answer TEXT NOT NULL,
            feedback_id INTEGER,
            question_embedding TEXT,  -- embedding ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏Ñ‡∏•‡πâ‡∏≤‡∏¢‡∏Å‡∏±‡∏ô
            created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
            applied_count INTEGER DEFAULT 0,
            FOREIGN KEY (feedback_id) REFERENCES feedback (id)
        )
    ''')

    cursor.execute('''
        CREATE INDEX IF NOT EXISTS idx_corrected_question ON corrected_answers(original_question)
    ''')

    cursor.execute('''
        CREATE INDEX IF NOT EXISTS idx_corrected_created ON corrected_answers(created_at)
    ''')

    # Tag System Tables
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS tags (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            name TEXT UNIQUE NOT NULL,
            color TEXT DEFAULT '#007bff',
            description TEXT,
            created_at DATETIME DEFAULT CURRENT_TIMESTAMP
        )
    ''')

    cursor.execute('''
        CREATE TABLE IF NOT EXISTS document_tags (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            document_id TEXT NOT NULL,  -- ChromaDB ID
            tag_id INTEGER NOT NULL,
            created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
            FOREIGN KEY (tag_id) REFERENCES tags (id),
            UNIQUE(document_id, tag_id)
        )
    ''')

    cursor.execute('''
        CREATE TABLE IF NOT EXISTS feedback_tags (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            feedback_id INTEGER NOT NULL,
            tag_id INTEGER NOT NULL,
            created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
            FOREIGN KEY (feedback_id) REFERENCES feedback (id),
            FOREIGN KEY (tag_id) REFERENCES tags (id),
            UNIQUE(feedback_id, tag_id)
        )
    ''')

    # Indexes for tag performance
    cursor.execute('''
        CREATE INDEX IF NOT EXISTS idx_document_tags_doc_id ON document_tags(document_id)
    ''')

    cursor.execute('''
        CREATE INDEX IF NOT EXISTS idx_document_tags_tag_id ON document_tags(tag_id)
    ''')

    cursor.execute('''
        CREATE INDEX IF NOT EXISTS idx_feedback_tags_feedback_id ON feedback_tags(feedback_id)
    ''')

    # Insert default tags if empty
    cursor.execute("SELECT COUNT(*) FROM tags")
    if cursor.fetchone()[0] == 0:
        default_tags = [
            ('‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ', '#6c757d', '‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ'),
            ('‡πÄ‡∏ó‡∏Ñ‡∏ô‡∏¥‡∏Ñ', '#007bff', '‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏î‡πâ‡∏≤‡∏ô‡πÄ‡∏ó‡∏Ñ‡∏ô‡∏¥‡∏Ñ'),
            ('‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô', '#28a745', '‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô'),
            ('‡∏õ‡∏±‡∏ç‡∏´‡∏≤', '#dc3545', '‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏õ‡∏±‡∏ç‡∏´‡∏≤'),
            ('‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•', '#17a2b8', '‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•'),
            ('‡∏™‡∏≠‡∏ö‡∏ñ‡∏≤‡∏°', '#ffc107', '‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏™‡∏≠‡∏ö‡∏ñ‡∏≤‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•'),
            ('‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç', '#fd7e14', '‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç'),
            ('‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç', '#e83e8c', '‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç')
        ]

        cursor.executemany('''
            INSERT INTO tags (name, color, description) VALUES (?, ?, ?)
        ''', default_tags)
        logging.info("‚úÖ Created default tags")

    # Enhanced Memory System Tables
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS session_memory (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            session_id TEXT NOT NULL,
            user_id TEXT DEFAULT 'default',
            question TEXT NOT NULL,
            answer TEXT NOT NULL,
            question_embedding BLOB,  -- Store as bytes
            contexts TEXT,  -- JSON array
            timestamp DATETIME DEFAULT CURRENT_TIMESTAMP,
            relevance_score REAL DEFAULT 0.0
        )
    ''')

    cursor.execute('''
        CREATE TABLE IF NOT EXISTS rag_performance_log (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            session_id TEXT,
            rag_mode TEXT,  -- 'standard' or 'enhanced'
            question TEXT,
            response_time REAL,  -- milliseconds
            context_count INTEGER,
            memory_hit BOOLEAN,
            success BOOLEAN,
            error_message TEXT,
            timestamp DATETIME DEFAULT CURRENT_TIMESTAMP
        )
    ''')

    cursor.execute('''
        CREATE TABLE IF NOT EXISTS context_cache (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            question_hash TEXT UNIQUE NOT NULL,
            question TEXT NOT NULL,
            contexts TEXT,  -- JSON array of cached contexts
            embedding BLOB,
            timestamp DATETIME DEFAULT CURRENT_TIMESTAMP,
            access_count INTEGER DEFAULT 1
        )
    ''')

    # Indexes for performance
    cursor.execute('''
        CREATE INDEX IF NOT EXISTS idx_session_memory_session ON session_memory(session_id)
    ''')

    cursor.execute('''
        CREATE INDEX IF NOT EXISTS idx_session_memory_timestamp ON session_memory(timestamp)
    ''')

    cursor.execute('''
        CREATE INDEX IF NOT EXISTS idx_rag_performance_session ON rag_performance_log(session_id)
    ''')

    cursor.execute('''
        CREATE INDEX IF NOT EXISTS idx_context_cache_hash ON context_cache(question_hash)
    ''')

    conn.commit()
    conn.close()
    logging.info("‚úÖ Feedback database initialized with learning, tag, and enhanced memory features")

# Initialize feedback database
init_feedback_db()

# ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ device
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
logging.info(f"Using device: {device}")

# ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏• embedding
# SentenceTransformer ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏•‡∏≤‡∏¢‡∏†‡∏≤‡∏©‡∏≤ (‡πÄ‡∏ô‡πâ‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢)
sentence_model = SentenceTransformer('intfloat/multilingual-e5-base', device=device)

# Create directory for storing images
os.makedirs(TEMP_IMG, exist_ok=True)

try:
    sum_tokenizer = MT5Tokenizer.from_pretrained('StelleX/mt5-base-thaisum-text-summarization')
    logging.info("‚úÖ MT5 Thai summarization tokenizer loaded successfully")
except Exception as e:
    logging.warning(f"‚ö†Ô∏è Failed to load MT5 tokenizer: {e}")
    sum_tokenizer = None
sum_model = MT5ForConditionalGeneration.from_pretrained('StelleX/mt5-base-thaisum-text-summarization')

def summarize_content(content: str) -> str:
    """
        ‡∏™‡∏£‡∏∏‡∏õ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤ 
    """
    logging.info("%%%%%%%%%%%%%% SUMMARY %%%%%%%%%%%%%%%%%%%%%")    
       
    input_ = sum_tokenizer(content, truncation=True, max_length=1024, return_tensors="pt")
    with torch.no_grad():
        preds = sum_model.generate(
            input_['input_ids'].to('cpu'),
            num_beams=15,
            num_return_sequences=1,
            no_repeat_ngram_size=1,
            remove_invalid_values=True,
            max_length=250
        )

    summary = sum_tokenizer.decode(preds[0], skip_special_tokens=True)

    logging.info(f" summary: {summary}.")
    logging.info("%%%%%%%%%%%%%% SUMMARY %%%%%%%%%%%%%%%%%%%%%")
    return summary

# ‡πÅ‡∏¢‡∏Å‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤, ‡∏£‡∏π‡∏õ ‡∏≠‡∏≠‡∏Å‡∏à‡∏≤‡∏Å PDF
def extract_pdf_content(pdf_path: str) -> List[Dict]:
    """
    ‡πÅ‡∏¢‡∏Å‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏•‡∏∞‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û‡∏à‡∏≤‡∏Å PDF ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ PyMuPDF
    """
    try:
        doc = fitz.open(pdf_path)
        content_chunks = []
        all_text=[]

        for page_num in range(len(doc)):
            page = doc[page_num]
            # Extract text
            text = page.get_text("text").strip()
            all_text.append(f"{text} \n\n\n")
            if not text:
                text = f"‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÉ‡∏ô‡∏´‡∏ô‡πâ‡∏≤ {page_num + 1}"
            
            logging.info("################# Text data ##################")
            chunk_data = {"text": f"‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å‡∏´‡∏ô‡πâ‡∏≤ {page_num + 1} : {text}" , "images": []}
            
            # Extract images
            image_list = page.get_images(full=True)
            logging.info("################# images list ##################")
            for img_index, img in enumerate(image_list):
                xref = img[0]
                base_image = doc.extract_image(xref)
                image_bytes = base_image["image"]
                image_ext = base_image["ext"]
                
                # Convert to PIL Image
                try:
                    image = Image.open(io.BytesIO(image_bytes))
                    if image.mode != "RGB":
                        image = image.convert("RGB")
                    
                    img_id = f"pic_{str(page_num+1)}_{str(img_index+1)}"
                    img_path = f"{TEMP_IMG}/{img_id}.{image_ext}"
                    image.save(img_path, format=image_ext.upper())

                    img_desc = f"‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û ‡∏à‡∏≤‡∏Å‡∏´‡∏ô‡πâ‡∏≤ {str(page_num+1)} ‡∏Ç‡∏≠‡∏á ‡∏£‡∏π‡∏õ‡∏ó‡∏µ‡πà {str(img_index+1)}, ‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°: {text[:80]}..."  
                    chunk_data["text"] += f"\n[‡∏†‡∏≤‡∏û: {img_id}.{image_ext}]"                    
                    chunk_data["images"].append({
                        "data": image,
                        "path": img_path,
                        "description": img_desc
                    })
                except Exception as e:
                    logger.warning(f"‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û‡∏ó‡∏µ‡πà‡∏´‡∏ô‡πâ‡∏≤ {str(page_num+1)}, ‡∏£‡∏π‡∏õ‡∏ó‡∏µ‡πà {str(img_index+1)}: {str(e)}")
            
            if chunk_data["text"]:
                content_chunks.append(chunk_data)
        
        if not any(chunk["images"] for chunk in content_chunks):
            logger.warning("‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û‡πÉ‡∏ô PDF: %s", pdf_path)
        
        doc.close()
        content_text= "".join(all_text)
        # ‡∏ï‡∏±‡∏î‡∏Ñ‡∏≥‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢
        thaitoken_text = preprocess_thai_text(content_text) if any(ord(c) >= 0x0E00 and ord(c) <= 0x0E7F for c in text) else text
        print("################################")
        print(f"{ thaitoken_text }")
        print("################################")
        global summarize
        summarize = summarize_content(thaitoken_text)
        return content_chunks
    except Exception as e:
        logger.error("‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÅ‡∏¢‡∏Å PDF: %s", str(e))
        raise

# ‡∏ï‡∏±‡∏î‡∏Ñ‡∏≥‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ 
def preprocess_thai_text(text: str) -> str:
    """
    ‡∏ï‡∏±‡∏î‡∏Ñ‡∏≥‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡∏î‡πâ‡∏ß‡∏¢ pythainlp ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°

    Args:
        text (str): ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢

    Returns:
        str: ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏ï‡∏±‡∏î‡∏Ñ‡∏≥‡πÅ‡∏•‡πâ‡∏ß
    """
    return " ".join(word_tokenize(text, engine="newmm"))


def embed_text(text: str) -> np.ndarray:
    """
    ‡∏™‡∏£‡πâ‡∏≤‡∏á embedding ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ SentenceTransformer 

    Args:
        text (str): ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á embedding        

    Returns:
        np.ndarray: Embedding vector ‡∏ó‡∏µ‡πà‡∏£‡∏ß‡∏°‡∏à‡∏≤‡∏Å‡∏´‡∏•‡∏≤‡∏¢‡πÇ‡∏°‡πÄ‡∏î‡∏•
    """
    logging.info("-------------- start embed text  -------------------")
    
    # ‡∏ï‡∏±‡∏î‡∏Ñ‡∏≥‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢
    processed_text = preprocess_thai_text(text) if any(ord(c) >= 0x0E00 and ord(c) <= 0x0E7F for c in text) else text
    
    # ‡∏™‡∏£‡πâ‡∏≤‡∏á embedding ‡∏î‡πâ‡∏ß‡∏¢ SentenceTransformer
    sentence_embedding = sentence_model.encode(processed_text, normalize_embeddings=True, device=device)    
        
    return sentence_embedding

def store_in_chroma(content_chunks: List[Dict], source_name: str):
    """
    ‡πÄ‡∏Å‡πá‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏•‡∏∞‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û‡πÉ‡∏ô Chroma ‡∏û‡∏£‡πâ‡∏≠‡∏° embedding
    ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡πÉ‡∏´‡∏°‡πà (chunks with metadata) ‡πÅ‡∏•‡∏∞‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡πÄ‡∏Å‡πà‡∏≤ (backward compatibility)
    """
    logging.info(f"##### Start store {len(content_chunks)} chunks in chroma #########")

    if not content_chunks:
        logging.warning("No chunks to store")
        return

    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡πÉ‡∏´‡∏°‡πà‡∏´‡∏£‡∏∑‡∏≠‡πÄ‡∏Å‡πà‡∏≤
    if isinstance(content_chunks[0], dict) and "text" in content_chunks[0] and "metadata" in content_chunks[0]:
        # ‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡πÉ‡∏´‡∏°‡πà: chunks ‡∏û‡∏£‡πâ‡∏≠‡∏° metadata ‡πÅ‡∏ö‡∏ö‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î
        store_chunks_modern(content_chunks)
    else:
        # ‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡πÄ‡∏Å‡πà‡∏≤: chunks ‡∏û‡∏£‡πâ‡∏≠‡∏° images (‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏Ç‡πâ‡∏≤‡∏Å‡∏±‡∏ô‡πÑ‡∏î‡πâ)
        store_chunks_legacy(content_chunks, source_name)


def store_chunks_modern(chunks: List[Dict]):
    """‡πÄ‡∏Å‡πá‡∏ö chunks ‡πÅ‡∏ö‡∏ö‡πÉ‡∏´‡∏°‡πà‡∏ó‡∏µ‡πà‡∏°‡∏µ metadata ‡πÅ‡∏ö‡∏ö‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î"""
    logging.info("Storing chunks in modern format")

    for chunk in chunks:
        try:
            text = chunk["text"]
            chunk_metadata = chunk["metadata"]
            chunk_id = chunk["id"]

            logging.info(f"Processing chunk: {chunk_id} ({len(text)} chars)")

            # ‡∏™‡∏£‡πâ‡∏≤‡∏á embedding
            text_embedding = embed_text(text)

            # ‡∏™‡∏£‡πâ‡∏≤‡∏á metadata ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö ChromaDB
            metadata = {
                "type": "text",
                "source": chunk_metadata.get("source", "unknown"),
                "file_type": chunk_metadata.get("file_type", "unknown"),
                "start": chunk_metadata.get("start", 0),
                "end": chunk_metadata.get("end", 0),
                "chunk_id": chunk_id
            }

            # ‡πÄ‡∏Å‡πá‡∏ö‡πÉ‡∏ô ChromaDB
            collection.add(
                documents=[text],
                metadatas=[metadata],
                embeddings=[text_embedding.tolist()],
                ids=[chunk_id]
            )

            logging.info(f"‚úÖ Stored chunk {chunk_id}")

        except Exception as e:
            logging.error(f"‚ùå Failed to store chunk {chunk.get('id', 'unknown')}: {str(e)}")


def store_chunks_legacy(chunks: List[Dict], source_name: str):
    """‡πÄ‡∏Å‡πá‡∏ö chunks ‡πÅ‡∏ö‡∏ö‡πÄ‡∏Å‡πà‡∏≤ (‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏Ç‡πâ‡∏≤‡∏Å‡∏±‡∏ô‡πÑ‡∏î‡πâ‡∏Å‡∏±‡∏ö PDF ‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡πÄ‡∏î‡∏¥‡∏°)"""
    logging.info("Storing chunks in legacy format")

    for chunk in chunks:
        try:
            text = chunk["text"]
            images = chunk.get("images", [])

            logging.info(f"Processing legacy chunk ({len(text)} chars)")

            # ‡∏™‡∏£‡πâ‡∏≤‡∏á embedding
            text_embedding = embed_text(text)
            text_id = shortuuid.uuid()[:8]

            # ‡πÄ‡∏Å‡πá‡∏ö‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°
            collection.add(
                documents=[text],
                metadatas=[{
                    "type": "text",
                    "source": source_name,
                    "format": "legacy"
                }],
                embeddings=[text_embedding.tolist()],
                ids=[text_id]
            )

            # ‡πÄ‡∏Å‡πá‡∏ö‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ)
            for img in images:
                try:
                    img_id = shortuuid.uuid()[:8]
                    img_path = img.get("path", "")

                    collection.add(
                        documents=[text],
                        metadatas=[{
                            "type": "image",
                            "source": source_name,
                            "image_path": img_path,
                            "description": img.get("description", ""),
                            "format": "legacy"
                        }],
                        embeddings=[text_embedding.tolist()],
                        ids=[img_id]
                    )
                    logging.info(f"‚úÖ Stored image {img_id}")

                except Exception as e:
                    logging.error(f"‚ùå Failed to store image: {str(e)}")

        except Exception as e:
            logging.error(f"‚ùå Failed to store legacy chunk: {str(e)}")

def extract_text_from_file(file_path: str) -> str:
    """
    ‡πÅ‡∏¢‡∏Å‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏≤‡∏Å‡πÑ‡∏ü‡∏•‡πå‡∏ï‡πà‡∏≤‡∏á‡πÜ (PDF, TXT, MD, DOCX)

    Args:
        file_path: ‡∏û‡∏≤‡∏ò‡∏Ç‡∏≠‡∏á‡πÑ‡∏ü‡∏•‡πå

    Returns:
        str: ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏µ‡πà‡πÅ‡∏¢‡∏Å‡πÑ‡∏î‡πâ
    """
    try:
        file_ext = Path(file_path).suffix.lower()

        if file_ext == '.pdf':
            return extract_pdf_text(file_path)
        elif file_ext in ['.txt', '.md']:
            return extract_text_file(file_path)
        elif file_ext == '.docx':
            return extract_docx_text(file_path)
        else:
            logging.warning(f"‡πÑ‡∏°‡πà‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡πÑ‡∏ü‡∏•‡πå‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó: {file_ext}")
            return ""

    except Exception as e:
        logging.error(f"‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÅ‡∏¢‡∏Å‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏≤‡∏Å {file_path}: {str(e)}")
        return ""


def extract_pdf_text(pdf_path: str) -> str:
    """‡πÅ‡∏¢‡∏Å‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏≤‡∏Å PDF"""
    try:
        doc = fitz.open(pdf_path)
        text = ""
        for page_num in range(doc.page_count):
            page = doc[page_num]
            text += page.get_text()
        doc.close()
        return text
    except Exception as e:
        logging.error(f"‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÅ‡∏¢‡∏Å‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° PDF: {str(e)}")
        return ""


def extract_text_file(file_path: str) -> str:
    """‡πÅ‡∏¢‡∏Å‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏≤‡∏Å‡πÑ‡∏ü‡∏•‡πå .txt ‡∏´‡∏£‡∏∑‡∏≠ .md"""
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            return f.read()
    except UnicodeDecodeError:
        try:
            with open(file_path, 'r', encoding='cp1252') as f:
                return f.read()
        except Exception as e:
            logging.error(f"‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå {file_path}: {str(e)}")
            return ""
    except Exception as e:
        logging.error(f"‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå {file_path}: {str(e)}")
        return ""


def extract_docx_text(docx_path: str) -> str:
    """‡πÅ‡∏¢‡∏Å‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏≤‡∏Å‡πÑ‡∏ü‡∏•‡πå .docx"""
    try:
        doc = docx.Document(docx_path)
        text = ""
        for paragraph in doc.paragraphs:
            text += paragraph.text + "\n"
        return text
    except Exception as e:
        logging.error(f"‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÅ‡∏¢‡∏Å‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° DOCX: {str(e)}")
        return ""


def process_multiple_files(files, clear_before_upload: bool = False):
    """
    ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÑ‡∏ü‡∏•‡πå‡∏´‡∏•‡∏≤‡∏¢‡πÑ‡∏ü‡∏•‡πå (PDF, TXT, MD, DOCX)

    Args:
        files: list ‡∏Ç‡∏≠‡∏á‡πÑ‡∏ü‡∏•‡πå‡∏à‡∏≤‡∏Å Gradio
        clear_before_upload: ‡∏•‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏Å‡πà‡∏≤‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
    """
    try:
        if not files:
            return "‚ùå ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î"

        current_count = collection.count()
        logging.info(f"‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏• {len(files)} ‡πÑ‡∏ü‡∏•‡πå")

        # ‡∏™‡∏≥‡∏£‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Å‡πà‡∏≠‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á (Enhanced Backup)
        auto_backup_result = auto_backup_before_operation()
        if not auto_backup_result["success"]:
            logging.warning(f"Auto backup failed: {auto_backup_result.get('error')}")
        else:
            logging.info(f"Auto backup created: {auto_backup_result['backup_name']}")

        # ‡∏ñ‡πâ‡∏≤‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏•‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏Å‡πà‡∏≤
        if clear_before_upload:
            logging.info("‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏•‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏Å‡πà‡∏≤‡∏ï‡∏≤‡∏°‡∏Ñ‡∏≥‡∏£‡πâ‡∏≠‡∏á...")
            clear_vector_db()

        total_chunks = 0
        successful_files = []
        failed_files = []

        # ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏ó‡∏µ‡∏•‡∏∞‡πÑ‡∏ü‡∏•‡πå
        for file_obj in files:
            try:
                file_path = file_obj.name
                file_name = os.path.basename(file_path)
                file_ext = Path(file_path).suffix.lower()

                logging.info(f"#### ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÑ‡∏ü‡∏•‡πå: {file_name} ({file_ext}) ####")

                # ‡πÅ‡∏¢‡∏Å‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏≤‡∏Å‡πÑ‡∏ü‡∏•‡πå
                text_content = extract_text_from_file(file_path)

                if not text_content.strip():
                    failed_files.append(f"{file_name}: ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÅ‡∏¢‡∏Å‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÑ‡∏î‡πâ")
                    continue

                # ‡πÅ‡∏¢‡∏Å‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏õ‡πá‡∏ô chunks
                content_chunks = chunk_text(text_content, file_name)

                if not content_chunks:
                    failed_files.append(f"{file_name}: ‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏ó‡∏µ‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÑ‡∏î‡πâ")
                    continue

                # ‡πÄ‡∏Å‡πá‡∏ö‡πÉ‡∏ô ChromaDB
                store_in_chroma(content_chunks, file_name)

                total_chunks += len(content_chunks)
                successful_files.append(file_name)

                logging.info(f"‚úÖ ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏• {file_name} ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à - {len(content_chunks)} chunks")

            except Exception as e:
                error_msg = f"{file_name}: {str(e)}"
                failed_files.append(error_msg)
                logging.error(f"‚ùå ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏• {file_name} ‡∏•‡πâ‡∏°‡πÄ‡∏´‡∏•‡∏ß: {str(e)}")

        # ‡∏™‡∏≥‡∏£‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥‡∏´‡∏•‡∏±‡∏á‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à
        backup_vector_db()

        new_count = collection.count()
        added_records = new_count - current_count

        # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå
        result = f"üéâ ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô!\n\n"
        result += f"üìä ‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•:\n"
        result += f"‚Ä¢ ‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î: {len(files)} ‡πÑ‡∏ü‡∏•‡πå\n"
        result += f"‚Ä¢ ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à: {len(successful_files)} ‡πÑ‡∏ü‡∏•‡πå\n"
        result += f"‚Ä¢ ‡∏•‡πâ‡∏°‡πÄ‡∏´‡∏•‡∏ß: {len(failed_files)} ‡πÑ‡∏ü‡∏•‡πå\n"
        result += f"‚Ä¢ Chunks ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î: {total_chunks} ‡∏ä‡∏¥‡πâ‡∏ô\n"
        result += f"‚Ä¢ Records ‡∏ó‡∏µ‡πà‡πÄ‡∏û‡∏¥‡πà‡∏°: {added_records} ‡πÄ‡∏£‡∏Ñ‡∏Ñ‡∏≠‡∏£‡πå‡∏î\n"
        result += f"‚Ä¢ Records ‡∏£‡∏ß‡∏°: {new_count} ‡πÄ‡∏£‡∏Ñ‡∏Ñ‡∏≠‡∏£‡πå‡∏î\n\n"

        if successful_files:
            result += f"‚úÖ ‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à:\n"
            for file_name in successful_files:
                result += f"  ‚Ä¢ {file_name}\n"
            result += "\n"

        if failed_files:
            result += f"‚ùå ‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏•‡πâ‡∏°‡πÄ‡∏´‡∏•‡∏ß:\n"
            for error in failed_files:
                result += f"  ‚Ä¢ {error}\n"
            result += "\n"

        result += f"üíæ ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ñ‡∏π‡∏Å‡∏™‡∏≥‡∏£‡∏≠‡∏á‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥"

        return result

    except Exception as e:
        logging.error(f"‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏´‡∏•‡∏≤‡∏¢‡πÑ‡∏ü‡∏•‡πå: {str(e)}")
        return f"‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î: {str(e)}"


# Google Sheets Integration
def extract_google_sheets_data(sheets_url: str) -> str:
    """
    ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å Google Sheets

    Args:
        sheets_url: URL ‡∏Ç‡∏≠‡∏á Google Sheets

    Returns:
        ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏à‡∏±‡∏î‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡πÅ‡∏•‡πâ‡∏ß
    """
    try:
        # ‡πÅ‡∏õ‡∏•‡∏á URL ‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô export URL
        sheet_id = extract_sheet_id_from_url(sheets_url)
        if not sheet_id:
            return "‚ùå ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÅ‡∏¢‡∏Å Sheet ID ‡∏à‡∏≤‡∏Å URL ‡πÑ‡∏î‡πâ"

        # Export ‡∏´‡∏ô‡πâ‡∏≤‡πÅ‡∏£‡∏Å‡πÄ‡∏õ‡πá‡∏ô CSV
        export_url = f"https://docs.google.com/spreadsheets/d/{sheet_id}/export?format=csv&gid=0"

        # ‡∏≠‡πà‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏õ‡πá‡∏ô DataFrame
        df = pd.read_csv(export_url)

        # ‡πÅ‡∏õ‡∏•‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°
        text_content = format_dataframe_to_text(df, sheets_url)

        return text_content

    except Exception as e:
        logging.error(f"Error extracting Google Sheets data: {str(e)}")
        return f"‚ùå ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å Google Sheets ‡πÑ‡∏î‡πâ: {str(e)}"


def extract_sheet_id_from_url(url: str) -> str:
    """
    ‡πÅ‡∏¢‡∏Å Sheet ID ‡∏à‡∏≤‡∏Å Google Sheets URL

    Args:
        url: Google Sheets URL

    Returns:
        Sheet ID
    """
    try:
        # Pattern ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Google Sheets URL
        patterns = [
            r"/d/([a-zA-Z0-9-_]+)",
            r"id=([a-zA-Z0-9-_]+)"
        ]

        for pattern in patterns:
            match = re.search(pattern, url)
            if match:
                return match.group(1)

        return ""
    except:
        return ""


def format_dataframe_to_text(df, source_url: str) -> str:
    """
    ‡πÅ‡∏õ‡∏•‡∏á DataFrame ‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏≠‡πà‡∏≤‡∏ô‡∏á‡πà‡∏≤‡∏¢‡πÅ‡∏•‡∏∞‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏Å‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤
    """
    try:
        text_content = f"# ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å Google Sheets\n"
        text_content += f"‡∏ó‡∏µ‡πà‡∏°‡∏≤: {source_url}\n"
        text_content += f"‡πÅ‡∏ñ‡∏ß: {len(df)}, ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå: {len(df.columns)}\n\n"

        # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå
        col_descriptions = []
        for i, col in enumerate(df.columns):
            col_descriptions.append(f"{col}")

        text_content += f"## ‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡∏ó‡∏µ‡πà‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏•‡∏∏‡∏°: {', '.join(col_descriptions)}\n\n"

        # ‡πÅ‡∏õ‡∏•‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏õ‡πá‡∏ô‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö Q&A ‡πÅ‡∏•‡∏∞‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡∏ó‡∏µ‡πà‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏á‡πà‡∏≤‡∏¢
        text_content += "## ‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•:\n\n"

        for index, row in df.iterrows():
            # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡∏£‡∏ß‡∏°‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏ñ‡∏ß‡∏ô‡∏µ‡πâ
            row_content = []
            for col in df.columns:
                value = row[col]
                if pd.isna(value) or value == "":
                    continue

                # ‡∏à‡∏±‡∏î‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏Ñ‡πà‡∏≤
                if isinstance(value, str) and len(value) > 100:
                    # ‡∏ñ‡πâ‡∏≤‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß ‡πÉ‡∏´‡πâ‡∏ï‡∏±‡∏î‡πÅ‡∏•‡∏∞‡πÄ‡∏û‡∏¥‡πà‡∏° "..."
                    value = value[:150] + "..." if len(value) > 150 else value

                row_content.append(f"{col}: {value}")

            if row_content:
                # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏µ‡πà‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡πÇ‡∏¢‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏ô‡πÅ‡∏ñ‡∏ß‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô
                text_content += f"### ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà {index + 1}:\n"
                text_content += f"{' '.join(row_content)}.\n\n"

                # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö Q&A ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏ó‡∏µ‡πà‡∏á‡πà‡∏≤‡∏¢‡∏Ç‡∏∂‡πâ‡∏ô
                if len(df.columns) >= 2:
                    # ‡∏™‡∏°‡∏°‡∏ï‡∏¥‡∏ß‡πà‡∏≤‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡πÅ‡∏£‡∏Å‡∏Ñ‡∏∑‡∏≠‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏° ‡πÅ‡∏•‡∏∞‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏≠‡∏∑‡πà‡∏ô‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö
                    first_col = df.columns[0]
                    question_value = row[first_col]
                    if not pd.isna(question_value) and str(question_value).strip():
                        text_content += f"**‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°/‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠:** {question_value}\n"

                        # ‡∏£‡∏ß‡∏ö‡∏£‡∏ß‡∏°‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏à‡∏≤‡∏Å‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏≠‡∏∑‡πà‡∏ô‡πÜ
                        answers = []
                        for col in df.columns[1:]:
                            answer_value = row[col]
                            if not pd.isna(answer_value) and str(answer_value).strip():
                                answers.append(f"{answer_value}")

                        if answers:
                            text_content += f"**‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö/‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î:** {' '.join(answers)}\n"

                        text_content += "\n"

        return text_content

    except Exception as e:
        logging.error(f"Error formatting DataFrame: {str(e)}")
        return f"‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•: {str(e)}"


def process_google_sheets_url(sheets_url: str, clear_before_upload: bool = False):
    """
    ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏• Google Sheets URL

    Args:
        sheets_url: URL ‡∏Ç‡∏≠‡∏á Google Sheets
        clear_before_upload: ‡∏•‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏Å‡πà‡∏≤‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
    """
    try:
        logging.info(f"Starting to process Google Sheets: {sheets_url}")

        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö URL
        if not sheets_url.strip():
            return "‚ùå ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡πÉ‡∏™‡πà Google Sheets URL"

        # ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å Google Sheets
        text_content = extract_google_sheets_data(sheets_url)

        if text_content.startswith("‚ùå"):
            return text_content

        # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ä‡∏∑‡πà‡∏≠‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö source
        sheet_name = f"Google_Sheets_{datetime.now().strftime('%Y%m%d_%H%M%S')}"

        # ‡πÅ‡∏ö‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏õ‡πá‡∏ô chunks
        chunks = chunk_text(text_content, sheet_name)

        # ‡∏•‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏Å‡πà‡∏≤‡∏ñ‡πâ‡∏≤‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô
        if clear_before_upload:
            clear_vector_db()
            logging.info("Cleared vector database before upload")

        # ‡πÄ‡∏Å‡πá‡∏ö chunks ‡∏•‡∏á‡πÉ‡∏ô ChromaDB
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡πÉ‡∏´‡∏°‡πà‡∏´‡∏£‡∏∑‡∏≠‡πÄ‡∏Å‡πà‡∏≤
        if chunks and isinstance(chunks[0], dict) and "text" in chunks[0] and "metadata" in chunks[0]:
            # ‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡πÉ‡∏´‡∏°‡πà: chunks ‡∏û‡∏£‡πâ‡∏≠‡∏° metadata ‡πÅ‡∏ö‡∏ö‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î
            store_chunks_modern(chunks)
        else:
            # ‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡πÄ‡∏Å‡πà‡∏≤: chunks ‡∏û‡∏£‡πâ‡∏≠‡∏° source name
            store_chunks_legacy(chunks, sheet_name)

        # ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏£‡∏∏‡∏õ
        update_summary_data(chunks)

        result_msg = f"""‚úÖ ‡∏ô‡∏≥‡πÄ‡∏Ç‡πâ‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å Google Sheets ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à!
üìä URL: {sheets_url}
üìù ‡∏ä‡∏∑‡πà‡∏≠‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡πá‡∏ö: {sheet_name}
üìÑ ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô chunks: {len(chunks)}
üìè ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î: {len(text_content):,} ‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£

‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡πÉ‡∏ô‡∏£‡∏∞‡∏ö‡∏ö RAG ‡πÅ‡∏•‡πâ‡∏ß!"""

        logging.info(f"Successfully processed Google Sheets: {sheet_name}")
        return result_msg

    except Exception as e:
        logging.error(f"Error processing Google Sheets URL: {str(e)}")
        return f"‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏• Google Sheets: {str(e)}"


def chunk_text(text: str, source_file: str, chunk_size: int = 1000, overlap: int = 200) -> List[Dict]:
    """
    ‡πÅ‡∏¢‡∏Å‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏õ‡πá‡∏ô chunks ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• metadata

    Args:
        text: ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
        source_file: ‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå‡∏ï‡πâ‡∏ô‡∏ó‡∏≤‡∏á
        chunk_size: ‡∏Ç‡∏ô‡∏≤‡∏î‡∏Ç‡∏≠‡∏á chunk
        overlap: ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà‡∏ó‡∏±‡∏ö‡∏ã‡πâ‡∏≠‡∏ô‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á chunks

    Returns:
        List[Dict]: list ‡∏Ç‡∏≠‡∏á chunks ‡∏û‡∏£‡πâ‡∏≠‡∏° metadata
    """
    if not text or len(text.strip()) < 50:
        return []

    chunks = []
    start = 0

    while start < len(text):
        end = start + chunk_size

        # ‡∏´‡∏≤‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡∏ï‡∏±‡∏î‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°
        if end < len(text):
            # ‡∏û‡∏¢‡∏≤‡∏¢‡∏≤‡∏°‡∏ï‡∏±‡∏î‡∏ó‡∏µ‡πà‡∏ß‡∏£‡∏£‡∏Ñ ‡∏´‡∏£‡∏∑‡∏≠ ‡∏à‡∏ö‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î
            for i in range(end, max(start, end - 100), -1):
                if text[i] in [' ', '\n', '.', '!', '?']:
                    end = i + 1
                    break

        chunk_text = text[start:end].strip()

        if len(chunk_text) > 50:  # ‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ô‡πâ‡∏≠‡∏¢ 50 ‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£
            chunk_id = f"{source_file}_{start}_{end}"

            chunks.append({
                "text": chunk_text,
                "id": chunk_id,
                "metadata": {
                    "source": source_file,
                    "start": start,
                    "end": end,
                    "file_type": Path(source_file).suffix.lower()
                }
            })

        start = end - overlap if end - overlap > start else end

    return chunks


class EnhancedRAG:
    """
    Enhanced RAG System with MemoRAG-like features:
    - Memory management for conversations
    - Context chaining
    - Session-based reasoning
    - Cross-reference analysis
    """

    def __init__(self):
        self.session_memory = deque(maxlen=MEMORY_WINDOW_SIZE)
        self.session_context = []
        self.conversation_history = []
        self.current_session_id = self._generate_session_id()

    def _generate_session_id(self) -> str:
        """Generate unique session ID"""
        timestamp = datetime.now().isoformat()
        session_hash = hashlib.md5(timestamp.encode()).hexdigest()[:8]
        return f"session_{session_hash}"

    def add_to_memory(self, question: str, answer: str, contexts: List[str]):
        """Add conversation to memory"""
        memory_entry = {
            "session_id": self.current_session_id,
            "timestamp": datetime.now().isoformat(),
            "question": question,
            "answer": answer,
            "contexts": contexts[:3]  # Store top 3 contexts
        }
        self.session_memory.append(memory_entry)

        # Add to conversation history
        self.conversation_history.append({
            "role": "user",
            "content": question,
            "timestamp": memory_entry["timestamp"]
        })
        self.conversation_history.append({
            "role": "assistant",
            "content": answer,
            "timestamp": memory_entry["timestamp"]
        })

        logging.info(f"Added to memory: Session {self.current_session_id}")

    def get_relevant_memory(self, current_question: str) -> List[Dict]:
        """Get relevant memory entries based on current question"""
        if not ENABLE_SESSION_MEMORY:
            return []

        relevant_memories = []
        current_embedding = embed_text(current_question)

        for memory_entry in self.session_memory:
            # Calculate similarity between current question and memory
            memory_question = memory_entry["question"]
            memory_embedding = embed_text(memory_question)

            # Simple similarity check (can be improved with proper similarity calculation)
            similarity = np.dot(current_embedding, memory_embedding)

            if similarity > 0.7:  # Threshold for relevance
                relevant_memories.append({
                    "question": memory_entry["question"],
                    "answer": memory_entry["answer"],
                    "similarity": float(similarity),
                    "contexts": memory_entry["contexts"]
                })

        # Sort by similarity and return top results
        relevant_memories.sort(key=lambda x: x["similarity"], reverse=True)
        return relevant_memories[:3]  # Return top 3 relevant memories

    def build_context_prompt(self, question: str, contexts: List[str], relevant_memories: List[Dict],
                           show_source: bool = False, formal_style: bool = False) -> str:
        """Build enhanced prompt with memory and context chaining"""

        if not ENABLE_CONTEXT_CHAINING and not ENABLE_REASONING:
            # ‡∏™‡∏£‡πâ‡∏≤‡∏á prompt ‡∏ï‡∏≤‡∏°‡∏™‡πÑ‡∏ï‡∏•‡πå‡∏ó‡∏µ‡πà‡πÄ‡∏•‡∏∑‡∏≠‡∏Å
            if formal_style:
                style_instruction = "‡∏ï‡∏≠‡∏ö‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏£ ‡πÉ‡∏ä‡πâ‡∏†‡∏≤‡∏©‡∏≤‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏†‡∏≤‡∏û ‡πÅ‡∏•‡∏∞‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô"
                source_phrase = ""
                response_prefix = "‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö:"
            else:
                style_instruction = "‡∏ï‡∏≠‡∏ö‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏Å‡∏±‡∏ô‡πÄ‡∏≠‡∏á ‡πÉ‡∏ä‡πâ‡∏†‡∏≤‡∏©‡∏≤‡∏ó‡∏µ‡πà‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏á‡πà‡∏≤‡∏¢"
                source_phrase = ""
                response_prefix = "‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö:"

            source_instruction = ""
            if show_source:
                source_instruction = f"\n- ‡∏´‡∏≤‡∏Å‡∏ï‡∏≠‡∏ö‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å‡∏ö‡∏£‡∏¥‡∏ö‡∏ó ‡πÉ‡∏´‡πâ‡∏£‡∏∞‡∏ö‡∏∏‡∏ß‡πà‡∏≤ '{source_phrase}'" if source_phrase else "" if source_phrase else ""

            return f"""‡∏Ñ‡∏∏‡∏ì‡πÄ‡∏õ‡πá‡∏ô‡∏ú‡∏π‡πâ‡∏ä‡πà‡∏ß‡∏¢‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏π‡πâ‡∏î‡πâ‡∏≤‡∏ô‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ó‡∏µ‡πà‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ß‡πâ ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÇ‡∏î‡∏¢‡∏≠‡∏≤‡∏®‡∏±‡∏¢‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡∏ó‡∏µ‡πà‡πÉ‡∏´‡πâ‡∏°‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏´‡∏•‡∏±‡∏Å

**‡πÅ‡∏ô‡∏ß‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡∏ï‡∏≠‡∏ö:**
- {style_instruction}
- ‡πÉ‡∏´‡πâ‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏ó‡∏µ‡πà‡∏™‡∏≠‡∏î‡∏Ñ‡∏•‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏ô‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡πÄ‡∏õ‡πá‡∏ô‡∏´‡∏•‡∏±‡∏Å
- ‡∏´‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏ô‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡πÑ‡∏°‡πà‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏û‡∏≠ ‡πÉ‡∏´‡πâ‡∏ï‡∏≠‡∏ö‡∏ï‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏°‡∏µ‡πÅ‡∏•‡∏∞‡∏£‡∏∞‡∏ö‡∏∏‡∏Ç‡πâ‡∏≠‡∏à‡∏≥‡∏Å‡∏±‡∏î
- ‡∏ï‡∏≠‡∏ö‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå‡πÅ‡∏•‡∏∞‡∏°‡∏µ‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏ä‡∏ô‡πå{source_instruction}
- ‡∏≠‡∏≤‡∏à‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡πÄ‡∏•‡πá‡∏Å‡∏ô‡πâ‡∏≠‡∏¢‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô ‡πÅ‡∏ï‡πà‡πÑ‡∏°‡πà‡∏ï‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ

**‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°:** {question}

**‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡∏à‡∏≤‡∏Å‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£:**
{summarize}

{context}

{response_prefix}"""

        # Enhanced prompt with memory and reasoning
        prompt_parts = []

        # Add context about memory if available
        if relevant_memories:
            prompt_parts.append("## ‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á (‡∏à‡∏≤‡∏Å Memory):")
            for i, memory in enumerate(relevant_memories, 1):
                prompt_parts.append(f"‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏ó‡∏µ‡πà {i}:")
                prompt_parts.append(f"‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°: {memory['question']}")
                prompt_parts.append(f"‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö: {memory['answer'][:200]}...")
                prompt_parts.append("")

        # Add current contexts
        if contexts:
            prompt_parts.append("## ‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô‡∏à‡∏≤‡∏Å‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£:")
            prompt_parts.extend(contexts)

        # Add reasoning prompt if enabled
        if ENABLE_REASONING:
            # ‡∏™‡∏£‡πâ‡∏≤‡∏á prompt ‡∏ï‡∏≤‡∏°‡∏™‡πÑ‡∏ï‡∏•‡πå‡∏ó‡∏µ‡πà‡πÄ‡∏•‡∏∑‡∏≠‡∏Å
            if formal_style:
                style_instruction = "- ‡∏ï‡∏≠‡∏ö‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏£ ‡πÉ‡∏ä‡πâ‡∏†‡∏≤‡∏©‡∏≤‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏†‡∏≤‡∏û‡πÅ‡∏•‡∏∞‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô"
                source_phrase = ""
                response_prefix = "‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö:"
            else:
                style_instruction = "- ‡∏ï‡∏≠‡∏ö‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏Å‡∏±‡∏ô‡πÄ‡∏≠‡∏á ‡πÉ‡∏ä‡πâ‡∏†‡∏≤‡∏©‡∏≤‡∏ó‡∏µ‡πà‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏á‡πà‡∏≤‡∏¢"
                source_phrase = ""
                response_prefix = "‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö:"

            source_instruction = ""
            if show_source:
                source_instruction = f"\n- ‡∏´‡∏≤‡∏Å‡∏ï‡∏≠‡∏ö‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å‡∏ö‡∏£‡∏¥‡∏ö‡∏ó ‡πÉ‡∏´‡πâ‡∏£‡∏∞‡∏ö‡∏∏‡∏ß‡πà‡∏≤ '{source_phrase}'" if source_phrase else "" if source_phrase else ""

            prompt_parts.extend([
                "",
                "## ‡πÅ‡∏ô‡∏ß‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°:",
                style_instruction,
                "- ‡πÉ‡∏´‡πâ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏Å‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡πÅ‡∏•‡∏∞‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡∏ó‡∏µ‡πà‡∏à‡∏î‡∏à‡∏≥‡πÑ‡∏ß‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏´‡∏•‡∏±‡∏Å",
                "- ‡∏ï‡∏≠‡∏ö‡πÉ‡∏´‡πâ‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå‡πÅ‡∏•‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏ä‡∏ô‡πå ‡πÇ‡∏î‡∏¢‡∏¢‡∏±‡∏á‡∏Ñ‡∏á‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡∏Ç‡∏≠‡∏ö‡πÄ‡∏Ç‡∏ï‡∏Ç‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏°‡∏µ",
                "- ‡∏´‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏°‡πà‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏û‡∏≠ ‡πÉ‡∏´‡πâ‡∏ï‡∏≠‡∏ö‡∏ï‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏°‡∏µ‡πÅ‡∏•‡∏∞‡∏£‡∏∞‡∏ö‡∏∏‡∏Ç‡πâ‡∏≠‡∏à‡∏≥‡∏Å‡∏±‡∏î",
                "- ‡∏≠‡∏≤‡∏à‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡πÇ‡∏¢‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏≥‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏™‡∏≠‡∏î‡∏Ñ‡∏•‡πâ‡∏≠‡∏á‡∏°‡∏≤‡∏Å‡∏Ç‡∏∂‡πâ‡∏ô" + source_instruction,
                "",
                "## ‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå:",
                "1. ‡∏û‡∏¥‡∏à‡∏≤‡∏£‡∏ì‡∏≤‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô‡∏£‡πà‡∏ß‡∏°‡∏Å‡∏±‡∏ö‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á",
                "2. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≠‡∏î‡∏Ñ‡∏•‡πâ‡∏≠‡∏á‡πÅ‡∏•‡∏∞‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡πÇ‡∏¢‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡πà‡∏≤‡∏á‡πÜ",
                "3. ‡πÉ‡∏´‡πâ‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏ó‡∏µ‡πà‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏•‡∏∏‡∏°‡πÅ‡∏•‡∏∞‡∏°‡∏µ‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏ä‡∏ô‡πå‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î",
                "4. ‡πÉ‡∏ä‡πâ‡∏†‡∏≤‡∏©‡∏≤‡∏ó‡∏µ‡πà‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏á‡πà‡∏≤‡∏¢‡πÅ‡∏•‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡∏ò‡∏£‡∏£‡∏°‡∏ä‡∏≤‡∏ï‡∏¥"
            ])

        prompt_parts.extend([
            "",
            f"## ‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô: {question}",
            "",
            f"## {response_prefix}:",
            "‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÇ‡∏î‡∏¢‡∏û‡∏¥‡∏à‡∏≤‡∏£‡∏ì‡∏≤‡∏à‡∏≤‡∏Å‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏ó‡∏µ‡πà‡πÉ‡∏´‡πâ‡∏°‡∏≤ ‡πÉ‡∏´‡πâ‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏ó‡∏µ‡πà‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå‡πÅ‡∏•‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏ä‡∏ô‡πå‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î"
        ])

        return "\n".join(prompt_parts)

    def reset_session(self):
        """Reset current session"""
        self.current_session_id = self._generate_session_id()
        self.session_context = []
        self.conversation_history = []
        logging.info(f"Session reset: New session ID = {self.current_session_id}")

    def get_session_info(self) -> Dict:
        """Get current session information"""
        return {
            "session_id": self.current_session_id,
            "memory_size": len(self.session_memory),
            "conversation_length": len(self.conversation_history),
            "current_context_size": len(self.session_context)
        }


# Global Enhanced RAG instances
enhanced_rag = EnhancedRAG()  # Legacy for backward compatibility

# Initialize RAG Manager after class definition
rag_manager = None

def initialize_rag_manager():
    """Initialize RAG Manager after all classes are defined"""
    global rag_manager
    if rag_manager is None:
        rag_manager = RAGManager()
    return rag_manager


def process_pdf_upload(pdf_file):
    """
    ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡πÄ‡∏Å‡πà‡∏≤‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏Ç‡πâ‡∏≤‡∏Å‡∏±‡∏ô‡πÑ‡∏î‡πâ (deprecated - ‡πÉ‡∏ä‡πâ process_multiple_files ‡πÅ‡∏ó‡∏ô)
    """
    if pdf_file:
        return process_multiple_files([pdf_file], False)
    return "‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÑ‡∏ü‡∏•‡πå"

def clear_vector_db():
    try:
        
       # Clear existing collection to avoid duplicates
        try:
            chroma_client.delete_collection(name="pdf_data")
        except:
            pass  # Collection might not exist
        global collection
        collection = chroma_client.get_or_create_collection(name="pdf_data")

    except Exception as e:
        return f"‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏•‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•: {str(e)}"
    
def update_summary_data(chunks: List[Dict]):
    """
    ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏£‡∏∏‡∏õ‡∏à‡∏≤‡∏Å chunks ‡∏ó‡∏µ‡πà‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î
    """
    try:
        global summarize

        # ‡∏™‡∏£‡∏∏‡∏õ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å chunks
        total_chars = sum(len(chunk.get('text', '')) for chunk in chunks)
        source_files = set(chunk.get('metadata', {}).get('source', 'Unknown') for chunk in chunks)

        # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏£‡∏∏‡∏õ
        summary_text = f"""üìä ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î:
‚Ä¢ ‡πÅ‡∏´‡∏•‡πà‡∏á‡∏ó‡∏µ‡πà‡∏°‡∏≤: {', '.join(source_files)}
‚Ä¢ ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô chunks: {len(chunks)}
‚Ä¢ ‡∏Ç‡∏ô‡∏≤‡∏î‡∏£‡∏ß‡∏°: {total_chars:,} ‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£
‚Ä¢ ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
"""

        summarize = summary_text
        logging.info(f"Updated summary data: {len(chunks)} chunks from {len(source_files)} sources")

    except Exception as e:
        logging.error(f"Error updating summary data: {str(e)}")
        # ‡∏ñ‡πâ‡∏≤‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ‡∏Ñ‡πà‡∏≤‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô
        summarize = f"üìä ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• PDF: {len(chunks)} chunks, ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}"

def clear_vector_db_and_images():
    """
    ‡∏•‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏ô Chroma vector database ‡πÅ‡∏•‡∏∞‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏ô‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå images
    """
    
    try:
        clear_vector_db()
        
        pdf_input.clear()
        if os.path.exists(TEMP_IMG):
            shutil.rmtree(TEMP_IMG)
            os.makedirs(TEMP_IMG, exist_ok=True)
        
        return "‡∏•‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏ô vector database ‡πÅ‡∏•‡∏∞‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå images ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à"
    except Exception as e:
        return f"‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏•‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•: {str(e)}"


def extract_images_from_answer(answer: str):
    """
    ‡∏î‡∏∂‡∏á‡∏û‡∏≤‡∏ò‡∏Ç‡∏≠‡∏á‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏à‡∏≤‡∏Å‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö

    Args:
        answer: ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö

    Returns:
        list: ‡∏£‡∏≤‡∏¢‡∏ä‡∏∑‡πà‡∏≠‡∏û‡∏≤‡∏ò‡∏Ç‡∏≠‡∏á‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û‡∏ó‡∏µ‡πà‡∏û‡∏ö
    """
    import re

    # ‡πÉ‡∏ä‡πâ regex ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏î‡∏∂‡∏á‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô [‡∏†‡∏≤‡∏û: ...]
    pattern1 = r"\[(?:‡∏†‡∏≤‡∏û:\s*)?(pic_\w+[-_]?\w*\.(?:jpe?g|png))\]"
    pattern2 = r"(pic_\w+[-_]?\w*\.(?:jpe?g|png))"

    # ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û‡πÉ‡∏ô‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö
    image_list = re.findall(pattern1, answer)
    if len(image_list) == 0:
        image_list = re.findall(pattern2, answer)

    # ‡∏î‡∏∂‡∏á‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏£‡∏π‡∏õ‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏ã‡πâ‡∏≥‡∏Å‡∏±‡∏ô
    image_list_unique = list(dict.fromkeys(image_list))

    # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏û‡∏≤‡∏ò‡πÄ‡∏ï‡πá‡∏°‡πÅ‡∏•‡∏∞‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏°‡∏µ‡πÑ‡∏ü‡∏•‡πå‡∏≠‡∏¢‡∏π‡πà‡∏à‡∏£‡∏¥‡∏á
    valid_image_paths = []
    for img in image_list_unique:
        img_path = f"{TEMP_IMG}/{img}"
        if os.path.exists(img_path):
            valid_image_paths.append(img_path)
            logging.info(f"Found relevant image: {img_path}")

    return valid_image_paths


async def send_to_discord(question: str, answer: str):
    """
    ‡∏™‡πà‡∏á‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÅ‡∏•‡∏∞‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡πÑ‡∏õ‡∏¢‡∏±‡∏á Discord channel ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ)
    """
    if not DISCORD_ENABLED or DISCORD_WEBHOOK_URL == "YOUR_WEBHOOK_URL_HERE":
        logging.info("Discord integration is disabled or not configured")
        return

    try:
        # ‡∏î‡∏∂‡∏á‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏à‡∏≤‡∏Å‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö
        image_paths = extract_images_from_answer(answer)

        # ‡πÉ‡∏ä‡πâ Webhook URL ‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á
        webhook_url = DISCORD_WEBHOOK_URL

        embed = discord.Embed(
            title="üìö RAG PDF Bot - ‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÉ‡∏´‡∏°‡πà",
            color=discord.Color.blue()
        )
        embed.add_field(name="‚ùì ‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°", value=question, inline=False)
        embed.add_field(name="üí¨ ‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö", value=answer[:1024] + "..." if len(answer) > 1024 else answer, inline=False)

        # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ß‡πà‡∏≤‡∏°‡∏µ‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û‡∏õ‡∏£‡∏∞‡∏Å‡∏≠‡∏ö‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
        if image_paths:
            embed.add_field(name="üñºÔ∏è ‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á", value=f"‡∏û‡∏ö {len(image_paths)} ‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á", inline=False)

        embed.set_footer(text="PDF RAG Assistant")

        # ‡∏™‡∏£‡πâ‡∏≤‡∏á payload ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Discord webhook
        payload_data = {
            "embeds": [embed.to_dict()]
        }

        # ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û ‡πÉ‡∏´‡πâ‡πÅ‡∏ô‡∏ö‡πÑ‡∏õ‡∏Å‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°
        if image_paths:
            # Discord webhook ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡πÅ‡∏ô‡∏ö‡πÑ‡∏ü‡∏•‡πå‡πÑ‡∏î‡πâ‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î 10 ‡πÑ‡∏ü‡∏•‡πå
            files_to_send = image_paths[:10]  # ‡∏à‡∏≥‡∏Å‡∏±‡∏î‡πÑ‡∏ß‡πâ 10 ‡∏£‡∏π‡∏õ

            # ‡∏™‡∏£‡πâ‡∏≤‡∏á multipart/form-data payload
            files = {}
            for i, img_path in enumerate(files_to_send):
                try:
                    with open(img_path, 'rb') as f:
                        files[f'file{i}'] = (os.path.basename(img_path), f.read(), 'image/png')
                except Exception as e:
                    logging.error(f"Failed to read image {img_path}: {str(e)}")

            if files:
                # ‡∏™‡πà‡∏á‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÑ‡∏ü‡∏•‡πå‡πÅ‡∏ô‡∏ö
                response = requests.post(
                    webhook_url,
                    files=files,
                    data={'payload_json': json.dumps(payload_data)},
                    timeout=30
                )
            else:
                # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå‡πÑ‡∏î‡πâ ‡∏™‡πà‡∏á‡πÄ‡∏â‡∏û‡∏≤‡∏∞ embed
                response = requests.post(webhook_url, json=payload_data, timeout=10)
        else:
            # ‡∏™‡πà‡∏á‡πÄ‡∏â‡∏û‡∏≤‡∏∞ embed ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏£‡∏π‡∏õ
            response = requests.post(webhook_url, json=payload_data, timeout=10)

        if response.status_code == 204:
            logging.info("‡∏™‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÑ‡∏õ‡∏¢‡∏±‡∏á Discord ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à")
            if image_paths:
                logging.info(f"‡∏™‡πà‡∏á‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û {len(image_paths)} ‡∏£‡∏π‡∏õ‡πÑ‡∏õ‡∏¢‡∏±‡∏á Discord ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à")
        else:
            logging.error(f"‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏™‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÑ‡∏õ‡∏¢‡∏±‡∏á Discord: {response.status_code} - {response.text}")

    except Exception as e:
        logging.error(f"‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÑ‡∏õ‡∏¢‡∏±‡∏á Discord: {str(e)}")


def send_to_discord_sync(question: str, answer: str):
    """
    ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ discord ‡πÅ‡∏ö‡∏ö synchronous
    """
    try:
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á event loop ‡πÉ‡∏´‡∏°‡πà‡∏ñ‡πâ‡∏≤‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô
        try:
            loop = asyncio.get_event_loop()
        except RuntimeError:
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)

        if loop.is_running():
            # ‡∏ñ‡πâ‡∏≤ loop ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏≠‡∏¢‡∏π‡πà ‡πÉ‡∏ä‡πâ create_task
            asyncio.create_task(send_to_discord(question, answer))
        else:
            # ‡∏ñ‡πâ‡∏≤ loop ‡πÑ‡∏°‡πà‡∏ó‡∏≥‡∏á‡∏≤‡∏ô ‡πÉ‡∏ä‡πâ run_until_complete
            loop.run_until_complete(send_to_discord(question, answer))
    except Exception as e:
        logging.error(f"‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏Å Discord: {str(e)}")


def backup_vector_db():
    """
    ‡∏™‡∏≥‡∏£‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• vector database
    """
    try:
        if not os.path.exists(TEMP_VECTOR):
            log_with_time("‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå database ‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏™‡∏≥‡∏£‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•")
            return False

        # Force release ChromaDB before backup
        force_release_chromadb()
        time.sleep(1)  # Wait for files to be released

        # ‡∏™‡∏£‡πâ‡∏≤‡∏á timestamp ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö backup ‡πÅ‡∏•‡∏∞‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏ã‡πâ‡∏≥‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        base_backup_name = f"backup_{timestamp}"
        backup_folder = os.path.join(TEMP_VECTOR_BACKUP, base_backup_name)

        # ‡∏´‡∏≤‡∏ä‡∏∑‡πà‡∏≠‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏ã‡πâ‡∏≥‡∏Å‡∏±‡∏ô
        counter = 1
        while os.path.exists(backup_folder):
            backup_name = f"{base_backup_name}_{counter}"
            backup_folder = os.path.join(TEMP_VECTOR_BACKUP, backup_name)
            counter += 1

        # ‡∏Ñ‡∏±‡∏î‡∏•‡∏≠‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏õ‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå backup
        shutil.copytree(TEMP_VECTOR, backup_folder)
        log_with_time(f"‡∏™‡∏≥‡∏£‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à: {backup_folder}")

        # ‡∏•‡∏ö backup ‡πÄ‡∏Å‡πà‡∏≤‡πÄ‡∏Å‡∏¥‡∏ô‡∏Å‡∏ß‡πà‡∏≤ 7 ‡∏ß‡∏±‡∏ô
        cleanup_old_backups()

        return True
    except Exception as e:
        log_with_time(f"‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏™‡∏≥‡∏£‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏î‡πâ: {str(e)}")
        return False


def cleanup_old_backups(days_to_keep=7):
    """
    ‡∏•‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• backup ‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡πà‡∏≤‡πÄ‡∏Å‡∏¥‡∏ô‡∏Å‡∏ß‡πà‡∏≤‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏´‡∏ô‡∏î
    """
    try:
        now = datetime.now()

        for backup_name in os.listdir(TEMP_VECTOR_BACKUP):
            backup_path = os.path.join(TEMP_VECTOR_BACKUP, backup_name)
            if os.path.isdir(backup_path) and backup_name.startswith("backup_"):
                # ‡∏î‡∏∂‡∏á timestamp ‡∏à‡∏≤‡∏Å‡∏ä‡∏∑‡πà‡∏≠‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå
                try:
                    timestamp_str = backup_name.replace("backup_", "")
                    backup_time = datetime.strptime(timestamp_str, "%Y%m%d_%H%M%S")

                    # ‡∏•‡∏ö‡∏ñ‡πâ‡∏≤‡πÄ‡∏Å‡∏¥‡∏ô‡∏Å‡∏ß‡πà‡∏≤‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏´‡∏ô‡∏î
                    if (now - backup_time).days > days_to_keep:
                        shutil.rmtree(backup_path)
                        logging.info(f"‡∏•‡∏ö backup ‡πÄ‡∏Å‡πà‡∏≤: {backup_name}")
                except ValueError:
                    continue
    except Exception as e:
        logging.error(f"‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏•‡∏ö backup ‡πÄ‡∏Å‡πà‡∏≤‡πÑ‡∏î‡πâ: {str(e)}")


def backup_database_enhanced(backup_name=None, include_memory=False):
    """
    ‡∏™‡∏≥‡∏£‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• database ‡πÅ‡∏ö‡∏ö‡∏Ç‡∏±‡πâ‡∏ô‡∏™‡∏π‡∏á
    """
    try:
        import json

        if backup_name is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            backup_name = f"enhanced_backup_{timestamp}"

        # Ensure unique backup name
        backup_path = os.path.join(TEMP_VECTOR_BACKUP, backup_name)
        counter = 1
        original_backup_name = backup_name
        while os.path.exists(backup_path):
            backup_name = f"{original_backup_name}_{counter}"
            backup_path = os.path.join(TEMP_VECTOR_BACKUP, backup_name)
            counter += 1

        os.makedirs(backup_path, exist_ok=False)  # Don't allow exist to prevent conflicts

        # Backup main database
        if os.path.exists(TEMP_VECTOR):
            backup_db_path = os.path.join(backup_path, "chromadb")
            shutil.copytree(TEMP_VECTOR, backup_db_path)
            logging.info(f"‡∏™‡∏≥‡∏£‡∏≠‡∏á ChromaDB: {backup_db_path}")

        # Backup Enhanced RAG memory if requested
        if include_memory and RAG_MODE == "enhanced":
            try:
                memory_info = enhanced_rag.get_memory_info()
                backup_memory_path = os.path.join(backup_path, "memory_info.json")

                with open(backup_memory_path, 'w', encoding='utf-8') as f:
                    json.dump(memory_info, f, ensure_ascii=False, indent=2)

                # Also backup memory collection if it exists
                if hasattr(enhanced_rag, 'memory_collection'):
                    memory_collection_backup = os.path.join(backup_path, "memory_collection")
                    os.makedirs(memory_collection_backup, exist_ok=True)

                    # Get all memories from collection
                    memories = enhanced_rag.memory_collection.get()
                    memory_json_path = os.path.join(memory_collection_backup, "memories.json")

                    with open(memory_json_path, 'w', encoding='utf-8') as f:
                        json.dump(memories, f, ensure_ascii=False, indent=2)

                logging.info("‡∏™‡∏≥‡∏£‡∏≠‡∏á Enhanced RAG Memory ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à")
            except Exception as e:
                logging.warning(f"‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏™‡∏≥‡∏£‡∏≠‡∏á Memory: {str(e)}")

        # Create backup metadata
        metadata = {
            "backup_name": backup_name,
            "created_at": datetime.now().isoformat(),
            "type": "enhanced",
            "includes_memory": include_memory,
            "rag_mode": RAG_MODE,
            "database_info": get_database_info() if os.path.exists(TEMP_VECTOR) else None
        }

        metadata_path = os.path.join(backup_path, "backup_metadata.json")
        with open(metadata_path, 'w', encoding='utf-8') as f:
            json.dump(metadata, f, ensure_ascii=False, indent=2)

        logging.info(f"‡∏™‡∏£‡πâ‡∏≤‡∏á Enhanced Backup ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à: {backup_path}")

        # Clean old backups
        cleanup_old_backups()

        return {
            "success": True,
            "backup_name": backup_name,
            "backup_path": backup_path,
            "metadata": metadata
        }

    except Exception as e:
        logging.error(f"‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏™‡∏£‡πâ‡∏≤‡∏á Enhanced Backup: {str(e)}")
        return {
            "success": False,
            "error": str(e)
        }


def restore_database_enhanced(backup_name=None):
    """
    ‡∏Å‡∏π‡πâ‡∏Ñ‡∏∑‡∏ô database ‡πÅ‡∏ö‡∏ö‡∏Ç‡∏±‡πâ‡∏ô‡∏™‡∏π‡∏á
    """
    try:
        import json

        if backup_name is None:
            # ‡∏´‡∏≤ backup ‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î
            backups = [d for d in os.listdir(TEMP_VECTOR_BACKUP)
                      if os.path.isdir(os.path.join(TEMP_VECTOR_BACKUP, d))]
            if not backups:
                return {"success": False, "error": "‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• backup"}
            backup_name = sorted(backups)[-1]

        backup_path = os.path.join(TEMP_VECTOR_BACKUP, backup_name)

        if not os.path.exists(backup_path):
            return {"success": False, "error": f"‡πÑ‡∏°‡πà‡∏û‡∏ö backup: {backup_name}"}

        # Validate backup integrity before restore
        is_valid, validation_message = validate_backup_integrity(backup_path)
        if not is_valid:
            logging.error(f"Backup validation failed: {validation_message}")
            return {"success": False, "error": f"Backup ‡πÑ‡∏°‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á: {validation_message}"}

        logging.info(f"Backup validation passed: {backup_name}")

        # Load backup metadata
        metadata_path = os.path.join(backup_path, "backup_metadata.json")
        metadata = {}
        if os.path.exists(metadata_path):
            try:
                with open(metadata_path, 'r', encoding='utf-8') as f:
                    metadata = json.load(f)
                logging.info(f"Loaded backup metadata from: {metadata_path}")
            except json.JSONDecodeError as e:
                logging.warning(f"Invalid JSON in backup metadata: {str(e)}")
                metadata = {
                    "backup_name": backup_name,
                    "type": "unknown",
                    "includes_memory": False,
                    "rag_mode": "unknown"
                }
            except Exception as e:
                logging.warning(f"Could not read backup metadata: {str(e)}")
                metadata = {
                    "backup_name": backup_name,
                    "type": "unknown",
                    "includes_memory": False,
                    "rag_mode": "unknown"
                }

        # Create emergency backup of current data
        try:
            emergency_backup = backup_database_enhanced(
                f"emergency_restore_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
            )
            if not emergency_backup["success"]:
                logging.warning(f"Emergency backup failed: {emergency_backup.get('error')}")
                emergency_backup = {"success": False, "error": "Emergency backup failed"}
        except Exception as e:
            logging.error(f"Emergency backup creation failed: {str(e)}")
            emergency_backup = {"success": False, "error": str(e)}

        # Restore main database
        backup_db_path = os.path.join(backup_path, "chromadb")
        if os.path.exists(backup_db_path):
            if os.path.exists(TEMP_VECTOR):
                shutil.rmtree(TEMP_VECTOR)
            shutil.copytree(backup_db_path, TEMP_VECTOR)
            logging.info(f"‡∏Å‡∏π‡πâ‡∏Ñ‡∏∑‡∏ô ChromaDB ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à")

        # Restore Enhanced RAG memory if available
        memory_collection_backup = os.path.join(backup_path, "memory_collection")
        if os.path.exists(memory_collection_backup) and RAG_MODE == "enhanced":
            try:
                memory_json_path = os.path.join(memory_collection_backup, "memories.json")
                if os.path.exists(memory_json_path):
                    try:
                        with open(memory_json_path, 'r', encoding='utf-8') as f:
                            memories = json.load(f)

                        # Validate memories structure
                        if all(key in memories for key in ['ids', 'documents', 'metadatas']):
                            # Clear current memory and restore from backup
                            if hasattr(enhanced_rag, 'memory_collection'):
                                enhanced_rag.memory_collection.delete()
                                enhanced_rag.memory_collection.add(
                                    ids=memories['ids'],
                                    documents=memories['documents'],
                                    metadatas=memories['metadatas']
                                )
                                logging.info("‡∏Å‡∏π‡πâ‡∏Ñ‡∏∑‡∏ô Enhanced RAG Memory ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à")
                        else:
                            logging.warning("Invalid memory backup structure - missing required keys")
                    except json.JSONDecodeError as e:
                        logging.warning(f"Invalid JSON in memory backup: {str(e)}")
                    except Exception as e:
                        logging.warning(f"Could not restore memory backup: {str(e)}")

            except Exception as e:
                logging.warning(f"‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏Å‡∏π‡πâ‡∏Ñ‡∏∑‡∏ô Memory: {str(e)}")

        # Reload collections
        global collection
        collection = chroma_client.get_or_create_collection(name="pdf_data")

        result = {
            "success": True,
            "backup_name": backup_name,
            "restored_at": datetime.now().isoformat(),
            "metadata": metadata,
            "emergency_backup": emergency_backup
        }

        logging.info(f"‡∏Å‡∏π‡πâ‡∏Ñ‡∏∑‡∏ô Database ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à‡∏à‡∏≤‡∏Å: {backup_name}")
        return result

    except Exception as e:
        logging.error(f"‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏Å‡∏π‡πâ‡∏Ñ‡∏∑‡∏ô Database: {str(e)}")
        return {
            "success": False,
            "error": str(e)
        }


def is_valid_backup_folder(backup_path):
    """
    ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡πÄ‡∏õ‡πá‡∏ô backup ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
    """
    if not os.path.isdir(backup_path):
        return False

    # Check if it's a valid backup folder (has chromadb or backup_metadata.json)
    chromadb_path = os.path.join(backup_path, "chromadb")
    metadata_path = os.path.join(backup_path, "backup_metadata.json")

    return os.path.exists(chromadb_path) or os.path.exists(metadata_path)


def list_available_backups():
    """
    ‡πÅ‡∏™‡∏î‡∏á‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£ backup ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
    """
    try:
        import json

        backups = []

        if not os.path.exists(TEMP_VECTOR_BACKUP):
            return backups

        for backup_name in sorted(os.listdir(TEMP_VECTOR_BACKUP), reverse=True):
            backup_path = os.path.join(TEMP_VECTOR_BACKUP, backup_name)

            # Only include valid backup folders
            if not is_valid_backup_folder(backup_path):
                continue

            # Try to load metadata
            metadata = {}
            metadata_path = os.path.join(backup_path, "backup_metadata.json")
            if os.path.exists(metadata_path):
                try:
                    with open(metadata_path, 'r', encoding='utf-8') as f:
                        metadata = json.load(f)
                except:
                    pass

            # Get backup size
            total_size = 0
            for root, dirs, files in os.walk(backup_path):
                for file in files:
                    file_path = os.path.join(root, file)
                    if os.path.exists(file_path):
                        total_size += os.path.getsize(file_path)

            backup_info = {
                "name": backup_name,
                "path": backup_path,
                "size_mb": round(total_size / (1024 * 1024), 2),
                "created_at": metadata.get("created_at", "Unknown"),
                "type": metadata.get("type", "standard"),
                "includes_memory": metadata.get("includes_memory", False),
                "rag_mode": metadata.get("rag_mode", "unknown"),
                "database_info": metadata.get("database_info", {})
            }

            backups.append(backup_info)

        return backups

    except Exception as e:
        logging.error(f"‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÅ‡∏™‡∏î‡∏á‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£ backup: {str(e)}")
        return []


def delete_backup(backup_name):
    """
    ‡∏•‡∏ö backup ‡∏ó‡∏µ‡πà‡∏£‡∏∞‡∏ö‡∏∏
    """
    try:
        backup_path = os.path.join(TEMP_VECTOR_BACKUP, backup_name)

        if not os.path.exists(backup_path):
            return {"success": False, "error": f"‡πÑ‡∏°‡πà‡∏û‡∏ö backup: {backup_name}"}

        if not os.path.isdir(backup_path):
            return {"success": False, "error": f"‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå backup: {backup_name}"}

        shutil.rmtree(backup_path)
        logging.info(f"‡∏•‡∏ö backup ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à: {backup_name}")

        return {"success": True, "message": f"‡∏•‡∏ö backup {backup_name} ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à"}

    except Exception as e:
        logging.error(f"‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏•‡∏ö backup: {str(e)}")
        return {"success": False, "error": str(e)}


def validate_backup_integrity(backup_path):
    """
    ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå‡∏Ç‡∏≠‡∏á backup
    """
    try:
        # Check main database
        chromadb_path = os.path.join(backup_path, "chromadb")
        if os.path.exists(chromadb_path):
            # Check if essential ChromaDB files exist
            sqlite_file = os.path.join(chromadb_path, "chroma.sqlite3")
            if not os.path.exists(sqlite_file):
                return False, "Missing chroma.sqlite3 file"

        # Check backup metadata JSON
        metadata_path = os.path.join(backup_path, "backup_metadata.json")
        if os.path.exists(metadata_path):
            try:
                with open(metadata_path, 'r', encoding='utf-8') as f:
                    json.load(f)  # Try to parse JSON
            except json.JSONDecodeError:
                return False, "Invalid backup metadata JSON"

        # Check memory backup JSON if exists
        memory_collection_path = os.path.join(backup_path, "memory_collection")
        if os.path.exists(memory_collection_path):
            memory_json_path = os.path.join(memory_collection_path, "memories.json")
            if os.path.exists(memory_json_path):
                try:
                    with open(memory_json_path, 'r', encoding='utf-8') as f:
                        memories = json.load(f)
                        # Check if memory structure is valid
                        required_keys = ['ids', 'documents', 'metadatas']
                        if not all(key in memories for key in required_keys):
                            return False, "Invalid memory backup structure"
                except json.JSONDecodeError:
                    return False, "Invalid memory backup JSON"

        return True, "Backup is valid"

    except Exception as e:
        return False, f"Validation error: {str(e)}"


def cleanup_invalid_backups():
    """
    ‡∏•‡∏ö‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà backup ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á
    """
    try:
        if not os.path.exists(TEMP_VECTOR_BACKUP):
            return

        cleaned_count = 0
        for folder_name in os.listdir(TEMP_VECTOR_BACKUP):
            folder_path = os.path.join(TEMP_VECTOR_BACKUP, folder_name)
            if os.path.isdir(folder_path):
                # Check if it's a valid backup folder
                if not is_valid_backup_folder(folder_path):
                    shutil.rmtree(folder_path)
                    cleaned_count += 1
                    logging.info(f"‡∏•‡∏ö‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á: {folder_name}")
                else:
                    # Additional validation for backup integrity
                    is_valid, message = validate_backup_integrity(folder_path)
                    if not is_valid:
                        logging.warning(f"Backup {folder_name} failed validation: {message}")
                        # Don't delete automatically, just warn

        if cleaned_count > 0:
            logging.info(f"‡∏•‡∏ö‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î {cleaned_count} ‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå")

    except Exception as e:
        logging.error(f"‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ cleanup invalid backups: {str(e)}")


def auto_backup_before_operation():
    """
    ‡∏™‡∏£‡πâ‡∏≤‡∏á backup ‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥‡∏Å‡πà‡∏≠‡∏ô‡∏Å‡∏≤‡∏£‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏Å‡∏≤‡∏£‡∏ó‡∏µ‡πà‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç
    """
    try:
        # Clean invalid backups first
        cleanup_invalid_backups()

        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        backup_name = f"auto_backup_{timestamp}"

        result = backup_database_enhanced(
            backup_name=backup_name,
            include_memory=(RAG_MODE == "enhanced")
        )

        if result["success"]:
            logging.info(f"‡∏™‡∏£‡πâ‡∏≤‡∏á Auto Backup ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à: {backup_name}")
        else:
            logging.warning(f"‡∏™‡∏£‡πâ‡∏≤‡∏á Auto Backup ‡πÑ‡∏°‡πà‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à: {result.get('error')}")

        return result

    except Exception as e:
        logging.error(f"‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏™‡∏£‡πâ‡∏≤‡∏á Auto Backup: {str(e)}")
        return {"success": False, "error": str(e)}


def restore_vector_db(backup_name=None):
    """
    ‡∏Å‡∏π‡πâ‡∏Ñ‡∏∑‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å backup

    Args:
        backup_name: ‡∏ä‡∏∑‡πà‡∏≠‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå backup (‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏£‡∏∞‡∏ö‡∏∏‡∏à‡∏∞‡πÉ‡∏ä‡πâ‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î)
    """
    global TEMP_VECTOR
    try:
        if backup_name is None:
            # ‡∏´‡∏≤ backup ‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î
            backups = [d for d in os.listdir(TEMP_VECTOR_BACKUP)
                      if d.startswith("backup_") and os.path.isdir(os.path.join(TEMP_VECTOR_BACKUP, d))]
            if not backups:
                logging.error("‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• backup")
                return False
            backup_name = sorted(backups)[-1]

        backup_path = os.path.join(TEMP_VECTOR_BACKUP, backup_name)

        if not os.path.exists(backup_path):
            logging.error(f"‡πÑ‡∏°‡πà‡∏û‡∏ö backup: {backup_name}")
            return False

        # Try multiple strategies to restore database
        restore_success = False
        restore_method = ""

        # Strategy 1: Force release and restore
        try:
            log_with_time("Strategy 1: Force release and restore")
            force_release_chromadb()
            time.sleep(2)  # Wait longer for files to be released

            # ‡∏™‡∏≥‡∏£‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô‡∏Å‡πà‡∏≠‡∏ô restore
            backup_vector_db()

            # ‡∏•‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô‡πÅ‡∏•‡∏∞‡∏Å‡∏π‡πâ‡∏Ñ‡∏∑‡∏ô‡∏à‡∏≤‡∏Å backup
            if os.path.exists(TEMP_VECTOR):
                shutil.rmtree(TEMP_VECTOR)

            shutil.copytree(backup_path, TEMP_VECTOR)
            log_with_time(f"‡∏Å‡∏π‡πâ‡∏Ñ‡∏∑‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à‡∏à‡∏≤‡∏Å: {backup_name}")
            restore_success = True
            restore_method = "force_release"
        except Exception as e1:
            log_with_time(f"Strategy 1 failed: {e1}")

        # Strategy 2: Move database and create fresh
        if not restore_success:
            try:
                log_with_time("Strategy 2: Move database and create fresh")

                # Move locked database to temp
                moved_path = move_database_to_temp()

                # Create fresh database
                create_fresh_database()

                # Copy backup to fresh database
                if os.path.exists(TEMP_VECTOR):
                    shutil.rmtree(TEMP_VECTOR)
                shutil.copytree(backup_path, TEMP_VECTOR)

                log_with_time(f"‡∏Å‡∏π‡πâ‡∏Ñ‡∏∑‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à‡∏à‡∏≤‡∏Å: {backup_name} (fresh database)")
                restore_success = True
                restore_method = "fresh_database"
            except Exception as e2:
                log_with_time(f"Strategy 2 failed: {e2}")

        # Strategy 3: Use new timestamp path
        if not restore_success:
            try:
                log_with_time("Strategy 3: Use new timestamp path")

                # Create new database path with timestamp
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                new_db_path = f"./data/chromadb_restored_{timestamp}"

                if os.path.exists(new_db_path):
                    shutil.rmtree(new_db_path)

                shutil.copytree(backup_path, new_db_path)

                # Update global TEMP_VECTOR to new path
                TEMP_VECTOR = new_db_path

                log_with_time(f"‡∏Å‡∏π‡πâ‡∏Ñ‡∏∑‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à‡∏à‡∏≤‡∏Å: {backup_name} (new path: {new_db_path})")
                restore_success = True
                restore_method = "new_path"
            except Exception as e3:
                log_with_time(f"Strategy 3 failed: {e3}")

        if not restore_success:
            log_with_time("‚ùå All restore strategies failed")
            return False

        log_with_time(f"‚úÖ Restore successful using method: {restore_method}")

        # ‡∏£‡∏µ‡πÇ‡∏´‡∏•‡∏î collection
        global collection
        collection = chroma_client.get_or_create_collection(name="pdf_data")

        return True
    except Exception as e:
        logging.error(f"‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏Å‡∏π‡πâ‡∏Ñ‡∏∑‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏î‡πâ: {str(e)}")
        return False


def get_database_info():
    """
    ‡πÅ‡∏™‡∏î‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞ database ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î
    """
    try:
        # ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô
        count = collection.count()

        # ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏ü‡∏•‡πå
        db_exists = os.path.exists(TEMP_VECTOR)
        sqlite_exists = os.path.exists(os.path.join(TEMP_VECTOR, "chroma.sqlite3"))

        # ‡∏Ç‡∏ô‡∏≤‡∏î database
        db_size = 0
        if db_exists:
            for root, dirs, files in os.walk(TEMP_VECTOR):
                db_size += sum(os.path.getsize(os.path.join(root, name)) for name in files)

        # ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô backup
        backup_count = 0
        if os.path.exists(TEMP_VECTOR_BACKUP):
            backup_count = len([d for d in os.listdir(TEMP_VECTOR_BACKUP)
                               if d.startswith("backup_")])

        info = {
            "total_records": count,
            "database_path": TEMP_VECTOR,
            "database_exists": db_exists,
            "sqlite_exists": sqlite_exists,
            "database_size_mb": round(db_size / (1024 * 1024), 2),
            "backup_count": backup_count,
            "collections": [{"name": coll.name, "count": coll.count()} for coll in chroma_client.list_collections()]
        }

        logging.info(f"üìä Database Info: {count} records, {round(db_size/(1024*1024),2)}MB, {backup_count} backups")
        return info

    except Exception as e:
        logging.error(f"‚ùå ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• database ‡πÑ‡∏î‡πâ: {str(e)}")
        return {"error": str(e)}


def inspect_database():
    """
    ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏†‡∏≤‡∏¢‡πÉ‡∏ô database ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î
    """
    try:
        count = collection.count()
        if count == 0:
            return "Database ‡∏ß‡πà‡∏≤‡∏á‡πÄ‡∏õ‡∏•‡πà‡∏≤ - ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•"

        # ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á 3 records
        sample_data = collection.get(limit=3, include=["documents", "metadatas"])

        result = f"üìä Database Inspection:\n"
        result += f"üìù Total Records: {count}\n"
        result += f"üìÅ Collections: {list(chroma_client.list_collections())}\n\n"

        result += "üìã Sample Data (first 3 records):\n"
        for i, (doc, meta) in enumerate(zip(sample_data["documents"][:3], sample_data["metadatas"][:3])):
            result += f"\n{i+1}. Document: {doc[:100]}...\n"
            result += f"   Metadata: {meta}\n"
            result += f"   ---"

        return result

    except Exception as e:
        return f"‚ùå ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö database ‡∏•‡πâ‡∏°‡πÄ‡∏´‡∏•‡∏ß: {str(e)}"


# Discord Bot for receiving questions
class RAGPDFBot(discord.Client):
    def __init__(self):
        intents = discord.Intents.default()
        intents.message_content = True  # ‡πÄ‡∏õ‡∏¥‡∏î‡∏Å‡∏≤‡∏£‡∏≠‡πà‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°
        super().__init__(intents=intents)
        self.is_ready = False

    async def on_ready(self):
        """‡πÄ‡∏°‡∏∑‡πà‡∏≠ Bot ‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à"""
        logging.info(f'Bot ‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à‡πÄ‡∏õ‡πá‡∏ô {self.user}')
        self.is_ready = True
        # ‡∏ï‡∏±‡πâ‡∏á‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡∏Ç‡∏≠‡∏á Bot
        await self.change_presence(
            activity=discord.Activity(
                type=discord.ActivityType.listening,
                name=f"{DISCORD_BOT_PREFIX}‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°"
            )
        )

    async def on_message(self, message):
        """‡∏£‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏≤‡∏Å Discord"""
        # ‡πÑ‡∏°‡πà‡∏ï‡∏≠‡∏ö‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ç‡∏≠‡∏á‡∏ï‡∏±‡∏ß‡πÄ‡∏≠‡∏á
        if message.author == self.user:
            return

        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏Ñ‡∏ß‡∏£‡∏ï‡∏≠‡∏ö‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
        should_respond = False
        question = ""
        response_type = ""

        # ‡∏Å‡∏£‡∏ì‡∏µ‡∏ó‡∏µ‡πà 1: ‡∏°‡∏µ prefix (‡πÄ‡∏ä‡πà‡∏ô !ask)
        if message.content.startswith(DISCORD_BOT_PREFIX):
            question = message.content[len(DISCORD_BOT_PREFIX):].strip()
            should_respond = True
            response_type = "prefix"

        # ‡∏Å‡∏£‡∏ì‡∏µ‡∏ó‡∏µ‡πà 2: ‡∏ñ‡∏π‡∏Å mention bot (‡πÄ‡∏ä‡πà‡∏ô @RAGPDFBot)
        elif self.user.mentioned_in(message):
            # ‡∏•‡∏ö mention ‡∏≠‡∏≠‡∏Å‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°
            question = message.content.replace(f'<@!{self.user.id}>', '').replace(f'<@{self.user.id}>', '').strip()
            should_respond = True
            response_type = "mention"

        # ‡∏Å‡∏£‡∏ì‡∏µ‡∏ó‡∏µ‡πà 3: ‡πÑ‡∏°‡πà‡∏°‡∏µ prefix ‡πÅ‡∏ï‡πà‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡πÉ‡∏´‡πâ‡∏ï‡∏≠‡∏ö‡∏ó‡∏∏‡∏Å‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°
        elif DISCORD_RESPOND_NO_PREFIX:
            question = message.content.strip()
            should_respond = True
            response_type = "auto"

        # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏ï‡∏≠‡∏ö ‡πÉ‡∏´‡πâ‡∏à‡∏ö‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô
        if not should_respond or not question:
            return

        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏°‡∏µ‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
        if not question:
            if response_type == "prefix":
                await message.reply(
                    "‚ùå ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏£‡∏∞‡∏ö‡∏∏‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°\n"
                    f"‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á: `{DISCORD_BOT_PREFIX}PDF ‡∏ô‡∏µ‡πâ‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏≠‡∏∞‡πÑ‡∏£`"
                )
            elif response_type == "mention":
                await message.reply(
                    "‚ùå ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏£‡∏∞‡∏ö‡∏∏‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏´‡∏•‡∏±‡∏á‡∏à‡∏≤‡∏Å mention\n"
                    f"‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á: `@{self.user.name} PDF ‡∏ô‡∏µ‡πâ‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏≠‡∏∞‡πÑ‡∏£`"
                )
            else:
                await message.reply(
                    "‚ùå ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏£‡∏∞‡∏ö‡∏∏‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°\n"
                    f"‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á: `PDF ‡∏ô‡∏µ‡πâ‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏≠‡∏∞‡πÑ‡∏£`"
                )
            return

        # ‡πÅ‡∏™‡∏î‡∏á‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•
        logging.info(f"Discord Bot: ‡∏£‡∏±‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏° ({response_type}) - {question}")
        processing_msg = await message.reply("üîç ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö...")

        try:
            # ‡πÉ‡∏ä‡πâ model ‡πÅ‡∏•‡∏∞ provider ‡∏à‡∏≤‡∏Å chat interface (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ) ‡∏´‡∏£‡∏∑‡∏≠ fallback ‡πÄ‡∏õ‡πá‡∏ô default
            global current_model, current_provider
            model = current_model if current_model else DISCORD_DEFAULT_MODEL
            provider = current_provider if current_provider else "ollama"

            logging.info(f"Discord Bot using model: {model}, provider: {provider}")

            # ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ RAG system
            stream = query_rag(question, chat_llm=model, ai_provider=provider)

            # ‡∏£‡∏ß‡∏ö‡∏£‡∏ß‡∏°‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö
            full_answer = ""
            for chunk in stream:
                content = chunk["message"]["content"]
                full_answer += content

            # ‡∏à‡∏±‡∏î‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö (‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Discord)
            if len(full_answer) > 1990:  # Discord ‡∏à‡∏≥‡∏Å‡∏±‡∏î 2000 ‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£
                full_answer = full_answer[:1980] + "...\n\n*‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏ñ‡∏π‡∏Å‡∏ï‡∏±‡∏î‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡∏à‡∏≤‡∏Å‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß‡πÄ‡∏Å‡∏¥‡∏ô‡∏Ç‡∏µ‡∏î‡∏à‡∏≥‡∏Å‡∏±‡∏î*"

            # ‡∏î‡∏∂‡∏á‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏à‡∏≤‡∏Å‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö
            image_paths = extract_images_from_answer(full_answer)

            # ‡∏™‡∏£‡πâ‡∏≤‡∏á embed ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö
            embed = discord.Embed(
                title="",
                description=full_answer,
                color=discord.Color.blue()
            )

            # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ß‡πà‡∏≤‡∏°‡∏µ‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û‡∏õ‡∏£‡∏∞‡∏Å‡∏≠‡∏ö‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
            if image_paths:
                embed.add_field(name="üñºÔ∏è ‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á", value=f"‡∏û‡∏ö {len(image_paths)} ‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á", inline=False)

            # embed.add_field(name="‚ùì ‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°", value=question, inline=False)
            # embed.set_footer(text="PDF RAG Assistant ‚Ä¢ ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å PDF ‡∏ó‡∏µ‡πà‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î")
            # embed.set_thumbnail(url="https://cdn-icons-png.flaticon.com/512/2951/2951136.png")

            # ‡∏•‡∏ö‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•
            await processing_msg.delete()

            # ‡∏™‡πà‡∏á‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏ï‡∏≤‡∏°‡πÇ‡∏´‡∏°‡∏î‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏´‡∏ô‡∏î ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û
            await respond_to_discord_message_with_images(message, embed, image_paths, DISCORD_REPLY_MODE)

            logging.info(f"Discord Bot: ‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏£‡πâ‡∏≠‡∏¢ (‡πÇ‡∏´‡∏°‡∏î: {DISCORD_REPLY_MODE})")

        except Exception as e:
            # ‡∏•‡∏ö‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•
            await processing_msg.delete()

            error_embed = discord.Embed(
                title="‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î",
                description=f"‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÑ‡∏î‡πâ: {str(e)}",
                color=discord.Color.red()
            )
            await respond_to_discord_message(message, error_embed, DISCORD_REPLY_MODE)
            logging.error(f"Discord Bot error: {str(e)}")


async def send_discord_dm(user, embed):
    """‡∏™‡πà‡∏á Direct Message ‡πÉ‡∏´‡πâ‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ Discord"""
    try:
        await user.send(embed=embed)
        return True
    except discord.Forbidden:
        logging.warning(f"‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏™‡πà‡∏á DM ‡πÉ‡∏´‡πâ‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ {user.name} ‡πÑ‡∏î‡πâ (‡∏≠‡∏≤‡∏à‡∏õ‡∏¥‡∏î‡∏Å‡∏≤‡∏£‡∏£‡∏±‡∏ö DM)")
        return False
    except Exception as e:
        logging.error(f"‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡πà‡∏á DM: {str(e)}")
        return False


async def respond_to_discord_message(message, embed, reply_type="channel"):
    """‡∏ï‡∏≠‡∏ö‡∏Å‡∏•‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÉ‡∏ô Discord ‡∏ï‡∏≤‡∏°‡πÇ‡∏´‡∏°‡∏î‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏´‡∏ô‡∏î"""
    success_dm = False
    success_channel = False

    # ‡∏™‡πà‡∏á‡πÉ‡∏ô channel (‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÇ‡∏´‡∏°‡∏î channel ‡∏´‡∏£‡∏∑‡∏≠ both)
    if reply_type in ["channel", "both"]:
        try:
            await message.reply(embed=embed)
            success_channel = True
        except Exception as e:
            logging.error(f"‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ï‡∏≠‡∏ö‡πÉ‡∏ô channel: {str(e)}")

    # ‡∏™‡πà‡∏á DM (‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÇ‡∏´‡∏°‡∏î dm ‡∏´‡∏£‡∏∑‡∏≠ both)
    if reply_type in ["dm", "both"]:
        success_dm = await send_discord_dm(message.author, embed)

    # ‡∏ñ‡πâ‡∏≤‡∏™‡πà‡∏á DM ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡πÅ‡∏ï‡πà‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÇ‡∏´‡∏°‡∏î dm ‡πÉ‡∏´‡πâ‡∏™‡πà‡∏á‡πÉ‡∏ô channel ‡πÅ‡∏ó‡∏ô
    if reply_type == "dm" and not success_dm:
        try:
            fallback_embed = discord.Embed(
                title="üì¨ ‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì",
                description=embed.description,
                color=embed.color
            )
            await message.reply(embed=fallback_embed)
            logging.info("‡∏™‡πà‡∏á‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡πÉ‡∏ô channel ‡πÅ‡∏ó‡∏ô ‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡∏à‡∏≤‡∏Å‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏™‡πà‡∏á DM ‡πÑ‡∏î‡πâ")
        except Exception as e:
            logging.error(f"‡∏™‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° fallback ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ: {str(e)}")


async def respond_to_discord_message_with_images(message, embed, image_paths, reply_type="channel"):
    """‡∏ï‡∏≠‡∏ö‡∏Å‡∏•‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÉ‡∏ô Discord ‡∏ï‡∏≤‡∏°‡πÇ‡∏´‡∏°‡∏î‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏´‡∏ô‡∏î ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏™‡πà‡∏á‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û"""
    success_dm = False
    success_channel = False

    # ‡∏™‡πà‡∏á‡πÉ‡∏ô channel (‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÇ‡∏´‡∏°‡∏î channel ‡∏´‡∏£‡∏∑‡∏≠ both)
    if reply_type in ["channel", "both"]:
        try:
            if image_paths:
                # ‡∏™‡πà‡∏á embed ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û‡πÉ‡∏ô channel
                await send_message_with_images(message.channel, embed, image_paths, reply_to=message)
            else:
                # ‡∏™‡πà‡∏á‡πÄ‡∏â‡∏û‡∏≤‡∏∞ embed ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏£‡∏π‡∏õ
                await message.reply(embed=embed)
            success_channel = True
        except Exception as e:
            logging.error(f"‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ï‡∏≠‡∏ö‡πÉ‡∏ô channel: {str(e)}")

    # ‡∏™‡πà‡∏á DM (‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÇ‡∏´‡∏°‡∏î dm ‡∏´‡∏£‡∏∑‡∏≠ both)
    if reply_type in ["dm", "both"]:
        success_dm = await send_discord_dm_with_images(message.author, embed, image_paths)

    # ‡∏ñ‡πâ‡∏≤‡∏™‡πà‡∏á DM ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡πÅ‡∏ï‡πà‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÇ‡∏´‡∏°‡∏î dm ‡πÉ‡∏´‡πâ‡∏™‡πà‡∏á‡πÉ‡∏ô channel ‡πÅ‡∏ó‡∏ô
    if reply_type == "dm" and not success_dm:
        try:
            fallback_embed = discord.Embed(
                title="üì¨ ‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì",
                description=embed.description,
                color=embed.color
            )
            if image_paths:
                await send_message_with_images(message.channel, fallback_embed, image_paths, reply_to=message)
            else:
                await message.reply(embed=fallback_embed)
            logging.info("‡∏™‡πà‡∏á‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡πÉ‡∏ô channel ‡πÅ‡∏ó‡∏ô ‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡∏à‡∏≤‡∏Å‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏™‡πà‡∏á DM ‡πÑ‡∏î‡πâ")
        except Exception as e:
            logging.error(f"‡∏™‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° fallback ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ: {str(e)}")


async def send_message_with_images(channel, embed, image_paths, reply_to=None):
    """‡∏™‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û‡πÉ‡∏ô Discord channel"""
    try:
        # Discord ‡∏à‡∏≥‡∏Å‡∏±‡∏î‡πÑ‡∏ü‡∏•‡πå‡πÅ‡∏ô‡∏ö‡πÑ‡∏î‡πâ 10 ‡πÑ‡∏ü‡∏•‡πå‡∏ï‡πà‡∏≠‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°
        files_to_send = image_paths[:10]

        # ‡∏™‡∏£‡πâ‡∏≤‡∏á discord.File objects
        files = []
        for img_path in files_to_send:
            try:
                # Discord ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ç‡∏ô‡∏≤‡∏î‡πÑ‡∏°‡πà‡πÄ‡∏Å‡∏¥‡∏ô 8MB
                file_size = os.path.getsize(img_path)
                if file_size > 8 * 1024 * 1024:  # 8MB
                    logging.warning(f"‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û {img_path} ‡∏°‡∏µ‡∏Ç‡∏ô‡∏≤‡∏î‡πÉ‡∏´‡∏ç‡πà‡πÄ‡∏Å‡∏¥‡∏ô 8MB ‡∏à‡∏∞‡∏Ç‡πâ‡∏≤‡∏°‡πÑ‡∏õ")
                    continue

                file = discord.File(img_path, filename=os.path.basename(img_path))
                files.append(file)
            except Exception as e:
                logging.error(f"‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå {img_path}: {str(e)}")

        # ‡∏™‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÑ‡∏ü‡∏•‡πå
        if files:
            if reply_to:
                await reply_to.reply(embed=embed, files=files)
            else:
                await channel.send(embed=embed, files=files)
            logging.info(f"‡∏™‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û {len(files)} ‡∏£‡∏π‡∏õ‡πÑ‡∏õ‡∏¢‡∏±‡∏á Discord ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à")
        else:
            # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏™‡πà‡∏á‡πÑ‡∏î‡πâ ‡∏™‡πà‡∏á‡πÄ‡∏â‡∏û‡∏≤‡∏∞ embed
            if reply_to:
                await reply_to.reply(embed=embed)
            else:
                await channel.send(embed=embed)

    except Exception as e:
        logging.error(f"‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û: {str(e)}")
        # ‡∏™‡πà‡∏á‡πÄ‡∏â‡∏û‡∏≤‡∏∞ embed ‡∏ñ‡πâ‡∏≤‡∏™‡πà‡∏á‡∏£‡∏π‡∏õ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ
        try:
            if reply_to:
                await reply_to.reply(embed=embed)
            else:
                await channel.send(embed=embed)
        except Exception as e2:
            logging.error(f"‡∏™‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° fallback ‡∏Å‡πá‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ: {str(e2)}")


async def send_discord_dm_with_images(user, embed, image_paths):
    """‡∏™‡πà‡∏á Direct Message ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û‡πÉ‡∏´‡πâ‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ Discord"""
    try:
        # Discord ‡∏à‡∏≥‡∏Å‡∏±‡∏î‡πÑ‡∏ü‡∏•‡πå‡πÅ‡∏ô‡∏ö‡πÑ‡∏î‡πâ 10 ‡πÑ‡∏ü‡∏•‡πå‡∏ï‡πà‡∏≠‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°
        files_to_send = image_paths[:10]

        # ‡∏™‡∏£‡πâ‡∏≤‡∏á discord.File objects
        files = []
        for img_path in files_to_send:
            try:
                file_size = os.path.getsize(img_path)
                if file_size > 8 * 1024 * 1024:  # 8MB
                    logging.warning(f"‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û {img_path} ‡∏°‡∏µ‡∏Ç‡∏ô‡∏≤‡∏î‡πÉ‡∏´‡∏ç‡πà‡πÄ‡∏Å‡∏¥‡∏ô 8MB ‡∏à‡∏∞‡∏Ç‡πâ‡∏≤‡∏°‡πÑ‡∏õ")
                    continue

                file = discord.File(img_path, filename=os.path.basename(img_path))
                files.append(file)
            except Exception as e:
                logging.error(f"‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå {img_path}: {str(e)}")

        # ‡∏™‡πà‡∏á DM ‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÑ‡∏ü‡∏•‡πå
        if files:
            await user.send(embed=embed, files=files)
        else:
            await user.send(embed=embed)

        logging.info(f"‡∏™‡πà‡∏á DM ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û‡πÉ‡∏´‡πâ‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ {user.name} ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à")
        return True

    except discord.Forbidden:
        logging.warning(f"‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏™‡πà‡∏á DM ‡πÉ‡∏´‡πâ‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ {user.name} ‡πÑ‡∏î‡πâ (‡∏≠‡∏≤‡∏à‡∏õ‡∏¥‡∏î‡∏Å‡∏≤‡∏£‡∏£‡∏±‡∏ö DM)")
        return False
    except Exception as e:
        logging.error(f"‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡πà‡∏á DM: {str(e)}")
        return False


# Global variables for Discord Bot
discord_bot = None
discord_bot_thread = None


def start_discord_bot():
    """‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ó‡∏≥‡∏á‡∏≤‡∏ô Discord Bot"""
    global discord_bot

    if not DISCORD_BOT_ENABLED or DISCORD_BOT_TOKEN == "YOUR_BOT_TOKEN_HERE":
        logging.info("Discord Bot ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡πÄ‡∏õ‡∏¥‡∏î‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏´‡∏£‡∏∑‡∏≠‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤")
        return False

    try:
        discord_bot = RAGPDFBot()

        # ‡∏™‡∏£‡πâ‡∏≤‡∏á event loop ‡πÉ‡∏´‡∏°‡πà‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö bot
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)

        # ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ bot
        loop.run_until_complete(discord_bot.start(DISCORD_BOT_TOKEN))

    except Exception as e:
        logging.error(f"‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÄ‡∏£‡∏¥‡πà‡∏° Discord Bot ‡πÑ‡∏î‡πâ: {str(e)}")
        return False


def start_discord_bot_thread():
    """‡πÄ‡∏£‡∏¥‡πà‡∏° Discord Bot ‡πÉ‡∏ô thread ‡πÅ‡∏¢‡∏Å"""
    global discord_bot_thread

    if discord_bot_thread and discord_bot_thread.is_alive():
        logging.warning("Discord Bot ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß")
        return False

    discord_bot_thread = threading.Thread(target=start_discord_bot, daemon=True)
    discord_bot_thread.start()

    # ‡∏£‡∏≠‡∏™‡∏±‡∏Å‡∏Ñ‡∏£‡∏π‡πà‡πÉ‡∏´‡πâ bot ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ó‡∏≥‡∏á‡∏≤‡∏ô
    import time
    time.sleep(2)

    return True


def stop_discord_bot():
    """‡∏´‡∏¢‡∏∏‡∏î Discord Bot"""
    global discord_bot

    if discord_bot and discord_bot.is_ready:
        try:
            loop = asyncio.get_event_loop()
            if loop.is_running():
                loop.create_task(discord_bot.close())
            else:
                loop.run_until_complete(discord_bot.close())
            logging.info("Discord Bot ‡∏´‡∏¢‡∏∏‡∏î‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡πÅ‡∏•‡πâ‡∏ß")
            return True
        except Exception as e:
            logging.error(f"‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏´‡∏¢‡∏∏‡∏î Discord Bot ‡πÑ‡∏î‡πâ: {str(e)}")
            return False

    return False


# Flask App for LINE OA and Facebook Messenger
app = Flask(__name__)

# LINE OA Setup
line_bot_api = None
line_handler = None
line_thread = None

# Facebook Messenger Setup
fb_thread = None


def setup_line_bot():
    """‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ LINE Bot"""
    global line_bot_api, line_handler
    if LINE_ENABLED and LINE_CHANNEL_ACCESS_TOKEN != "YOUR_LINE_CHANNEL_ACCESS_TOKEN":
        try:
            line_bot_api = LineBotApi(LINE_CHANNEL_ACCESS_TOKEN)
            line_handler = WebhookHandler(LINE_CHANNEL_SECRET)

            # Register handlers ‡∏´‡∏•‡∏±‡∏á‡∏à‡∏≤‡∏Å setup ‡πÅ‡∏•‡πâ‡∏ß‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô
            register_line_handlers()

            logging.info("‚úÖ LINE Bot setup completed")
            return True
        except Exception as e:
            logging.error(f"‚ùå LINE Bot setup failed: {str(e)}")
            return False
    else:
        logging.info("LINE Bot is disabled or not configured")
        return False


def register_line_handlers():
    """Register LINE message handlers"""
    if line_handler:
        @line_handler.add(MessageEvent, message=TextMessage)
        def handle_line_message(event):
            """‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏≤‡∏Å LINE"""
            try:
                user_message = event.message.text
                user_id = event.source.user_id

                logging.info(f"LINE Bot: ‡∏£‡∏±‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏à‡∏≤‡∏Å {user_id} - {user_message}")

                # ‡∏ï‡∏≠‡∏ö‡∏Å‡∏•‡∏±‡∏ö‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÅ‡∏™‡∏î‡∏á‡∏ß‡πà‡∏≤‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•
                line_bot_api.reply_message(
                    event.reply_token,
                    TextSendMessage(text="üîç ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö...")
                )

                # ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÉ‡∏ô background
                threading.Thread(
                    target=process_line_question,
                    args=(event, user_message, user_id)
                ).start()

            except Exception as e:
                logging.error(f"LINE Bot error: {str(e)}")
                try:
                    line_bot_api.reply_message(
                        event.reply_token,
                        TextSendMessage(text="‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏•‡∏≠‡∏á‡πÉ‡∏´‡∏°‡πà")
                    )
                except:
                    pass


def setup_facebook_bot():
    """‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ Facebook Messenger Bot"""
    if FB_ENABLED and FB_PAGE_ACCESS_TOKEN != "YOUR_FB_PAGE_ACCESS_TOKEN":
        logging.info("‚úÖ Facebook Messenger Bot setup completed")
        return True
    else:
        logging.info("Facebook Messenger Bot is disabled or not configured")
        return False


@app.route("/callback", methods=['POST'])
def line_callback():
    """LINE Webhook Callback"""
    if not LINE_ENABLED or not line_handler:
        abort(400)

    signature = request.headers['X-Line-Signature']
    body = request.get_data(as_text=True)

    try:
        line_handler.handle(body, signature)
    except InvalidSignatureError:
        abort(400)

    return 'OK'




def process_line_question(event, question: str, user_id: str):
    """‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏à‡∏≤‡∏Å LINE"""
    global current_model, current_provider
    try:
        # ‡πÉ‡∏ä‡πâ model ‡πÅ‡∏•‡∏∞ provider ‡∏à‡∏≤‡∏Å chat interface (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ) ‡∏´‡∏£‡∏∑‡∏≠ fallback ‡πÄ‡∏õ‡πá‡∏ô default
        model = current_model if current_model else LINE_DEFAULT_MODEL
        provider = current_provider if current_provider else "ollama"

        logging.info(f"LINE Bot using model: {model}, provider: {provider}")

        # ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô RAG ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°
        response = query_rag(question, chat_llm=model, ai_provider=provider, show_source=False)
        answer = response.get('answer', '‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö')

        # ‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö LINE (‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î 5000 ‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£)
        if len(answer) > 4900:
            answer = answer[:4900] + "\n\n... (‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏ñ‡∏π‡∏Å‡∏ï‡∏±‡∏î‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡∏à‡∏≤‡∏Å‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß)"

        # ‡∏™‡πà‡∏á‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏Å‡∏•‡∏±‡∏ö‡πÑ‡∏õ‡∏¢‡∏±‡∏á LINE
        line_bot_api.push_message(
            user_id,
            TextSendMessage(text=answer)
        )

        logging.info(f"LINE Bot: ‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏£‡πâ‡∏≠‡∏¢")

    except Exception as e:
        logging.error(f"LINE processing error: {str(e)}")
        try:
            line_bot_api.push_message(
                user_id,
                TextSendMessage(text="‡∏Ç‡∏≠‡∏≠‡∏†‡∏±‡∏¢ ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏•‡∏≠‡∏á‡πÉ‡∏´‡∏°‡πà‡∏≠‡∏µ‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á")
            )
        except:
            pass


@app.route("/webhook", methods=['GET', 'POST'])
def facebook_webhook():
    """Facebook Messenger Webhook"""
    if not FB_ENABLED:
        abort(403)

    if request.method == 'GET':
        if request.args.get("hub.mode") == "subscribe" and request.args.get("hub.challenge"):
            if not request.args.get("hub.verify_token") == FB_VERIFY_TOKEN:
                return "Verification token mismatch", 403
            return request.args.get("hub.challenge"), 200
        return "Hello", 200

    elif request.method == 'POST':
        data = request.get_json()

        if data and "object" in data and data["object"] == "page":
            for entry in data["entry"]:
                for messaging_event in entry["messaging"]:
                    if messaging_event.get("message"):
                        sender_id = messaging_event["sender"]["id"]
                        message_text = messaging_event["message"].get("text")

                        if message_text:
                            # ‡πÉ‡∏ä‡πâ thread ‡πÅ‡∏¢‡∏Å‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°
                            threading.Thread(
                                target=process_facebook_question,
                                args=(sender_id, message_text),
                                daemon=True
                            ).start()

        return "OK", 200


def process_facebook_question(sender_id: str, question: str):
    """‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏à‡∏≤‡∏Å Facebook Messenger"""
    global current_model, current_provider
    try:
        logging.info(f"Facebook Bot: ‡∏£‡∏±‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏à‡∏≤‡∏Å {sender_id} - {question}")

        # ‡∏™‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•
        send_facebook_message(sender_id, "‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡πÉ‡∏´‡πâ‡∏Ñ‡∏∏‡∏ì...")

        # ‡πÉ‡∏ä‡πâ model ‡πÅ‡∏•‡∏∞ provider ‡∏à‡∏≤‡∏Å chat interface (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ) ‡∏´‡∏£‡∏∑‡∏≠ fallback ‡πÄ‡∏õ‡πá‡∏ô default
        model = current_model if current_model else FB_DEFAULT_MODEL
        provider = current_provider if current_provider else "ollama"

        logging.info(f"Facebook Bot using model: {model}, provider: {provider}")

        # ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô RAG ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°
        response = query_rag(question, chat_llm=model, ai_provider=provider, show_source=False)
        answer = response.get('answer', '‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö')

        # ‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Facebook (‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î 2000 ‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£)
        if len(answer) > 1900:
            answer = answer[:1900] + "\n\n... (‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏ñ‡∏π‡∏Å‡∏ï‡∏±‡∏î‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡∏à‡∏≤‡∏Å‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß)"

        # ‡∏™‡πà‡∏á‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏Å‡∏•‡∏±‡∏ö‡πÑ‡∏õ‡∏¢‡∏±‡∏á Facebook
        send_facebook_message(sender_id, answer)

        logging.info(f"Facebook Bot: ‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏£‡πâ‡∏≠‡∏¢")

    except Exception as e:
        logging.error(f"Facebook processing error: {str(e)}")
        try:
            send_facebook_message(sender_id, "‡∏Ç‡∏≠‡∏≠‡∏†‡∏±‡∏¢ ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏•‡∏≠‡∏á‡πÉ‡∏´‡∏°‡πà‡∏≠‡∏µ‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á")
        except:
            pass


def send_facebook_message(recipient_id: str, message_text: str):
    """‡∏™‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÑ‡∏õ‡∏¢‡∏±‡∏á Facebook Messenger"""
    try:
        url = f"https://graph.facebook.com/v18.0/me/messages?access_token={FB_PAGE_ACCESS_TOKEN}"

        payload = {
            "recipient": {"id": recipient_id},
            "message": {"text": message_text}
        }

        response = requests.post(url, json=payload)
        response.raise_for_status()

    except Exception as e:
        logging.error(f"Error sending Facebook message: {str(e)}")


def start_line_server():
    """‡πÄ‡∏£‡∏¥‡πà‡∏° LINE Bot Server"""
    if setup_line_bot():
        try:
            app.run(host='0.0.0.0', port=LINE_WEBHOOK_PORT, debug=False)
        except Exception as e:
            logging.error(f"LINE server error: {str(e)}")


def start_facebook_server():
    """‡πÄ‡∏£‡∏¥‡πà‡∏° Facebook Bot Server"""
    if setup_facebook_bot():
        try:
            # Facebook ‡πÉ‡∏ä‡πâ port ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ö LINE ‡∏ñ‡πâ‡∏≤ LINE ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡πÄ‡∏õ‡∏¥‡∏î‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô
            port = FB_WEBHOOK_PORT if LINE_ENABLED else LINE_WEBHOOK_PORT
            app.run(host='0.0.0.0', port=port, debug=False)
        except Exception as e:
            logging.error(f"Facebook server error: {str(e)}")


def start_line_bot_thread():
    """‡πÄ‡∏£‡∏¥‡πà‡∏° LINE Bot ‡πÉ‡∏ô thread ‡πÅ‡∏¢‡∏Å"""
    global line_thread
    if line_thread and line_thread.is_alive():
        logging.warning("LINE Bot ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß")
        return False

    if not LINE_ENABLED:
        logging.info("LINE Bot is disabled")
        return False

    try:
        line_thread = threading.Thread(target=start_line_server, daemon=True)
        line_thread.start()
        logging.info(f"LINE Bot server started on port {LINE_WEBHOOK_PORT}")
        return True
    except Exception as e:
        logging.error(f"Failed to start LINE Bot: {str(e)}")
        return False


def start_facebook_bot_thread():
    """‡πÄ‡∏£‡∏¥‡πà‡∏° Facebook Bot ‡πÉ‡∏ô thread ‡πÅ‡∏¢‡∏Å"""
    global fb_thread
    if fb_thread and fb_thread.is_alive():
        logging.warning("Facebook Bot ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß")
        return False

    if not FB_ENABLED:
        logging.info("Facebook Bot is disabled")
        return False

    try:
        fb_thread = threading.Thread(target=start_facebook_server, daemon=True)
        fb_thread.start()
        logging.info(f"Facebook Bot server started on port {FB_WEBHOOK_PORT}")
        return True
    except Exception as e:
        logging.error(f"Failed to start Facebook Bot: {str(e)}")
        return False


def determine_optimal_results(question: str) -> int:
    """
    ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°‡∏ï‡∏≤‡∏°‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°
    """
    question_lower = question.lower()

    # ‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏£‡∏ö‡∏ñ‡πâ‡∏ß‡∏ô - ‡πÉ‡∏ä‡πâ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏°‡∏≤‡∏Å‡∏Ç‡∏∂‡πâ‡∏ô
    comprehensive_keywords = ["‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î", "‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏ó‡∏±‡πâ‡∏á‡∏™‡∏¥‡πâ‡∏ô", "‡∏ó‡∏∏‡∏Å", "‡∏ó‡∏∏‡∏Å‡∏≠‡∏¢‡πà‡∏≤‡∏á", "‡∏™‡∏£‡∏∏‡∏õ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î", "‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÄ‡∏•‡∏¢"]
    if any(keyword in question_lower for keyword in comprehensive_keywords):
        return 8

    # ‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÅ‡∏ö‡∏ö‡∏ô‡∏±‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô - ‡πÉ‡∏ä‡πâ‡∏õ‡∏≤‡∏ô‡∏Å‡∏•‡∏≤‡∏á
    counting_keywords = ["‡∏Å‡∏µ‡πà", "‡∏à‡∏≥‡∏ô‡∏ß‡∏ô", "‡∏°‡∏µ‡∏Å‡∏µ‡πà", "‡∏Å‡∏µ‡πà‡∏ï‡∏±‡∏ß", "‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÄ‡∏ó‡πà‡∏≤‡πÑ‡∏£", "‡∏°‡∏µ‡∏Å‡∏µ‡πà‡∏≠‡∏¢‡πà‡∏≤‡∏á"]
    if any(keyword in question_lower for keyword in counting_keywords):
        return 5

    # ‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÅ‡∏ö‡∏ö‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏ö‡∏≤‡∏á‡∏™‡πà‡∏ß‡∏ô - ‡πÉ‡∏ä‡πâ‡∏õ‡∏≤‡∏ô‡∏Å‡∏•‡∏≤‡∏á
    selective_keywords = ["‡∏ö‡πâ‡∏≤‡∏á", "‡∏ö‡∏≤‡∏á‡∏™‡πà‡∏ß‡∏ô", "‡∏ö‡∏≤‡∏á‡∏≠‡∏¢‡πà‡∏≤‡∏á", "‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á", "‡∏Å‡∏£‡∏ì‡∏µ", "‡πÄ‡∏ä‡πà‡∏ô", "‡∏î‡∏±‡∏á‡∏ô‡∏µ‡πâ"]
    if any(keyword in question_lower for keyword in selective_keywords):
        return 4

    # ‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÄ‡∏à‡∏≤‡∏∞‡∏à‡∏á - ‡πÉ‡∏ä‡πâ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ô‡πâ‡∏≠‡∏¢
    specific_keywords = ["‡∏Ñ‡∏∑‡∏≠‡∏≠‡∏∞‡πÑ‡∏£", "‡∏≠‡∏∞‡πÑ‡∏£", "‡∏ó‡∏µ‡πà‡πÑ‡∏´‡∏ô", "‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÑ‡∏´‡∏£‡πà", "‡∏ó‡∏≥‡πÑ‡∏°", "‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£", "‡πÉ‡∏Ñ‡∏£"]
    if any(keyword in question_lower for keyword in specific_keywords):
        return 3

    # ‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ - ‡πÉ‡∏ä‡πâ‡∏Ñ‡πà‡∏≤‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô
    return 3


def calculate_relevance_score(question: str, context: str) -> float:
    """
    ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÅ‡∏•‡∏∞ context (‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢)
    """
    # ‡πÅ‡∏¢‡∏Å‡∏Ñ‡∏≥‡πÉ‡∏ô‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÅ‡∏•‡∏∞ context
    question_words = set(word_tokenize(question))
    context_words = set(word_tokenize(context))

    if len(question_words) == 0:
        return 0.0

    # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡∏ã‡πâ‡∏≥‡∏Å‡∏±‡∏ô
    common_words = question_words.intersection(context_words)

    # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì Jaccard similarity
    jaccard_similarity = len(common_words) / len(question_words.union(context_words))

    # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì keyword matching ‡πÅ‡∏ö‡∏ö case-insensitive
    question_lower = question.lower()
    context_lower = context.lower()

    # ‡πÉ‡∏´‡πâ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏û‡∏¥‡πÄ‡∏®‡∏©‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ñ‡∏≥‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏ó‡∏µ‡πà‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ô
    exact_matches = 0
    partial_matches = 0

    for word in question_words:
        word_lower = word.lower()
        if word_lower in context_lower:
            exact_matches += 1

    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£ match ‡πÅ‡∏ö‡∏ö‡∏¢‡πà‡∏≠‡∏¢ (‡πÄ‡∏ä‡πà‡∏ô "1-on-1" ‡∏Å‡∏±‡∏ö "session", "‡∏ô‡∏±‡∏î" ‡∏Å‡∏±‡∏ö "‡∏Ñ‡∏∏‡∏¢")
    for q_word in question_words:
        for c_word in context_words:
            # ‡∏ñ‡πâ‡∏≤‡∏Ñ‡∏≥‡πÉ‡∏î‡∏Ñ‡∏≥‡∏´‡∏ô‡∏∂‡πà‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏™‡πà‡∏ß‡∏ô‡∏´‡∏ô‡∏∂‡πà‡∏á‡∏Ç‡∏≠‡∏á‡∏≠‡∏µ‡∏Å‡∏Ñ‡∏≥
            if q_word.lower() in c_word.lower() or c_word.lower() in q_word.lower():
                partial_matches += 0.5

    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á semantically ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢
    semantic_matches = 0
    question_lower = question_lower.replace("1-on-1", "session ‡∏™‡∏≠‡∏ô‡πÄ‡∏™‡∏£‡∏¥‡∏° ‡∏ô‡∏±‡∏î‡∏Ñ‡∏∏‡∏¢")
    question_lower = question_lower.replace("‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á", "‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏à‡∏∏‡∏î")
    question_lower = question_lower.replace("‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢", "‡∏™‡∏≠‡∏ô‡πÄ‡∏™‡∏£‡∏¥‡∏°")
    question_lower = question_lower.replace("‡πÑ‡∏°‡πà‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à", "‡∏Ç‡πâ‡∏≠‡∏™‡∏á‡∏™‡∏±‡∏¢")

    for word in question_lower.split():
        if word in context_lower and len(word) > 2:  # ‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏°‡∏≤‡∏¢‡∏°‡∏≤‡∏Å‡∏Å‡∏ß‡πà‡∏≤ 2 ‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£
            semantic_matches += 0.3

    # ‡∏£‡∏ß‡∏°‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡πÅ‡∏ö‡∏ö‡∏ñ‡πà‡∏ß‡∏á‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å
    base_score = jaccard_similarity * 0.4
    exact_score = (exact_matches / len(question_words)) * 0.4
    partial_score = min(partial_matches / len(question_words), 0.3) * 0.2
    semantic_score = min(semantic_matches / len(question_words), 0.2) * 0.2

    final_score = base_score + exact_score + partial_score + semantic_score

    return min(final_score, 1.0)  # ‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î‡∏ó‡∏µ‡πà 1.0


def filter_relevant_contexts(question: str, documents: list, metadatas: list, min_relevance: float = 0.05) -> list:
    """
    ‡∏Å‡∏£‡∏≠‡∏á‡πÄ‡∏â‡∏û‡∏≤‡∏∞ context ‡∏ó‡∏µ‡πàÁõ∏ÂÖ≥ÊÄß‡∏™‡∏π‡∏á
    """
    if not documents:
        return []

    filtered_contexts = []

    for doc, metadata in zip(documents, metadatas):
        # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á
        relevance_score = calculate_relevance_score(question, doc)

        # ‡πÄ‡∏Å‡πá‡∏ö‡πÄ‡∏â‡∏û‡∏≤‡∏∞ context ‡∏ó‡∏µ‡πà‡∏ú‡πà‡∏≤‡∏ô threshold
        if relevance_score >= min_relevance:
            filtered_contexts.append({
                'text': doc,
                'metadata': metadata,
                'relevance_score': relevance_score
            })

    # ‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏ï‡∏≤‡∏°‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á (‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î‡∏Å‡πà‡∏≠‡∏ô)
    filtered_contexts.sort(key=lambda x: x['relevance_score'], reverse=True)

    # ‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏à‡∏≥‡∏ô‡∏ß‡∏ô context ‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î
    max_contexts = 5
    return filtered_contexts[:max_contexts]


def query_rag(question: str, chat_llm: str = "gemma3:latest", ai_provider: str = "ollama", show_source: bool = False, formal_style: bool = False):
    """
    ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÉ‡∏ô‡∏£‡∏∞‡∏ö‡∏ö Enhanced RAG ‡πÅ‡∏•‡∏∞‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡πÅ‡∏ö‡∏ö streaming ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ Ollama
    """
    global summarize, enhanced_rag

    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡∏™‡∏£‡∏∏‡∏õ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ‡∏Ñ‡πà‡∏≤‡∏ß‡πà‡∏≤‡∏á
    if 'summarize' not in globals() or summarize is None:
        summarize = "‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡∏™‡∏£‡∏∏‡∏õ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏à‡∏≤‡∏Å PDF"

    logging.info(f"#### Enhanced RAG Mode: {RAG_MODE} #### ")
    logging.info(f"#### Question: {question} #### ")

    # Get relevant memories if enhanced mode is enabled
    relevant_memories = []
    if RAG_MODE == "enhanced":
        relevant_memories = enhanced_rag.get_relevant_memory(question)
        logging.info(f"Found {len(relevant_memories)} relevant memories")

    question_embedding = embed_text(question)

    # Smart Retrieval: ‡∏õ‡∏£‡∏±‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏ï‡∏≤‡∏°‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°
    max_result = determine_optimal_results(question)
    logging.info(f"Using max_result: {max_result}")

    # ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏î‡πâ‡∏ß‡∏¢ similarity threshold ‡∏ó‡∏µ‡πà‡∏™‡∏π‡∏á‡∏Ç‡∏∂‡πâ‡∏ô
    results = collection.query(
        query_embeddings=[question_embedding.tolist()],
        n_results=max_result
    )

    # Relevance Filtering: ‡∏Å‡∏£‡∏≠‡∏á‡πÄ‡∏â‡∏û‡∏≤‡∏∞ context ‡∏ó‡∏µ‡πàÁõ∏ÂÖ≥ÊÄß‡∏™‡∏π‡∏á
    filtered_contexts = filter_relevant_contexts(question, results["documents"][0], results["metadatas"][0], min_relevance=0.05)
    logging.info(f"Filtered {len(results['documents'][0])} contexts to {len(filtered_contexts)} relevant contexts")

    # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ context ‡∏ó‡∏µ‡πà‡∏ú‡πà‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏≠‡∏á ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
    if len(filtered_contexts) == 0:
        logging.warning("No contexts passed relevance filter, using all retrieved contexts")
        filtered_contexts = [{'text': doc, 'metadata': meta} for doc, meta in zip(results["documents"][0], results["metadatas"][0])]

    context_texts = []
    image_paths = []

    # ‡πÉ‡∏ä‡πâ‡πÄ‡∏â‡∏û‡∏≤‡∏∞ context ‡∏ó‡∏µ‡πà‡∏ú‡πà‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏≠‡∏á
    for doc, metadata in zip([ctx['text'] for ctx in filtered_contexts], [ctx['metadata'] for ctx in filtered_contexts]):
        context_texts.append(doc)
        logging.info(f"Selected context: {doc}")
        logging.info(f"metadata: {metadata}")

        # Regex pattern ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤ [img: ‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå.jpeg]
        pattern = r"pic_(\d+)_(\d+)\.jpeg"
        imgs = re.findall(pattern, doc)

        if imgs:
            image_paths.append(imgs)
            logging.info(f"img: {imgs}")

        if metadata and metadata["type"] == "image":
            logging.info(f"image_path : { metadata['image_path']}")
            image_paths.append(metadata['image_path'])
    


    context = "\n".join(context_texts)

    # Build enhanced prompt using EnhancedRAG
    if RAG_MODE == "enhanced":
        prompt = enhanced_rag.build_context_prompt(question, context_texts, relevant_memories, show_source, formal_style)
        logging.info(f"Using Enhanced RAG prompt with {len(relevant_memories)} memories")
        logging.info("############## Begin Enhanced RAG Prompt #################")
        logging.info(f"prompt: {prompt}")
        logging.info("############## End Enhanced RAG Prompt #################")
    else:
        # Standard RAG prompt with stricter instructions
        if summarize is None:
            summarize = ""

        # ‡∏™‡∏£‡πâ‡∏≤‡∏á prompt ‡∏ï‡∏≤‡∏°‡∏™‡πÑ‡∏ï‡∏•‡πå‡∏ó‡∏µ‡πà‡πÄ‡∏•‡∏∑‡∏≠‡∏Å
    if formal_style:
        style_instruction = "‡∏ï‡∏≠‡∏ö‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏£ ‡πÉ‡∏ä‡πâ‡∏†‡∏≤‡∏©‡∏≤‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏†‡∏≤‡∏û ‡πÅ‡∏•‡∏∞‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô"
        source_phrase = ""
        response_prefix = "‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö:"
    else:
        style_instruction = "‡∏ï‡∏≠‡∏ö‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏Å‡∏±‡∏ô‡πÄ‡∏≠‡∏á ‡πÉ‡∏ä‡πâ‡∏†‡∏≤‡∏©‡∏≤‡∏ó‡∏µ‡πà‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏á‡πà‡∏≤‡∏¢"
        source_phrase = ""
        response_prefix = "‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö:"

    source_instruction = ""
    if show_source:
        source_instruction = f"\n- ‡∏´‡∏≤‡∏Å‡∏ï‡∏≠‡∏ö‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å‡∏ö‡∏£‡∏¥‡∏ö‡∏ó ‡πÉ‡∏´‡πâ‡∏£‡∏∞‡∏ö‡∏∏‡∏ß‡πà‡∏≤ '{source_phrase}'" if source_phrase else ""

    prompt = f"""‡∏Ñ‡∏∏‡∏ì‡πÄ‡∏õ‡πá‡∏ô‡∏ú‡∏π‡πâ‡∏ä‡πà‡∏ß‡∏¢‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏π‡πâ‡∏î‡πâ‡∏≤‡∏ô‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ó‡∏µ‡πà‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ß‡πâ ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÇ‡∏î‡∏¢‡∏≠‡∏≤‡∏®‡∏±‡∏¢‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡∏ó‡∏µ‡πà‡πÉ‡∏´‡πâ‡∏°‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏´‡∏•‡∏±‡∏Å

**‡πÅ‡∏ô‡∏ß‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡∏ï‡∏≠‡∏ö:**
- {style_instruction}
- ‡πÉ‡∏´‡πâ‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏ó‡∏µ‡πà‡∏™‡∏≠‡∏î‡∏Ñ‡∏•‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏ô‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡πÄ‡∏õ‡πá‡∏ô‡∏´‡∏•‡∏±‡∏Å
- ‡∏´‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏ô‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡πÑ‡∏°‡πà‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏û‡∏≠ ‡πÉ‡∏´‡πâ‡∏ï‡∏≠‡∏ö‡∏ï‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏°‡∏µ‡πÅ‡∏•‡∏∞‡∏£‡∏∞‡∏ö‡∏∏‡∏Ç‡πâ‡∏≠‡∏à‡∏≥‡∏Å‡∏±‡∏î
- ‡∏ï‡∏≠‡∏ö‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå‡πÅ‡∏•‡∏∞‡∏°‡∏µ‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏ä‡∏ô‡πå{source_instruction}
- ‡∏≠‡∏≤‡∏à‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡πÄ‡∏•‡πá‡∏Å‡∏ô‡πâ‡∏≠‡∏¢‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô ‡πÅ‡∏ï‡πà‡πÑ‡∏°‡πà‡∏ï‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ

**‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°:** {question}

**‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡∏à‡∏≤‡∏Å‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£:**
{summarize}

{context}

{response_prefix}"""

    logging.info("############## Begin Standard Prompt #################")
    logging.info(f"prompt: {prompt}")
    logging.info("############## End Standard Prompt #################")
    logging.info(f"Prompt length: {len(prompt)} characters")

    log_with_time("+++++++++++++ Send prompt To LLM ++++++++++++++++++")
    overall_start = time.time()

    # Debug: Check server status before API call (only for Ollama)
    if ai_provider == "ollama":
        health_start = time.time()
        try:
            import requests
            logging.info("Checking Ollama health before API call...")
            ollama_api_url = os.getenv("OLLAMA_URL", "http://localhost:11434")
            logging.info(f"Ollama API URL: {ollama_api_url}")
            health_check = requests.get(f"{ollama_api_url}/api/tags", timeout=5)
            measure_time(health_start, "Ollama health check")
            log_with_time(f"Ollama health check: Status {health_check.status_code}")
            if health_check.status_code == 200:
                models = health_check.json().get('models', [])
                model_names = [m['name'] for m in models]
                log_with_time(f"Available models: {model_names}")
                log_with_time(f"Target model '{chat_llm}' available: {chat_llm in model_names}")
            else:
                log_with_time(f"Ollama server returned status: {health_check.status_code}")
        except Exception as e:
            log_with_time(f"Ollama health check failed: {e}")
            # Return empty stream instead of None
            error_msg = f"‚ùå Ollama server error: {str(e)}"
            return ({"message": {"content": error_msg}} for _ in range(1))
    else:
        logging.info(f"Skipping health check for {ai_provider} provider")

    api_call_start = time.time()
    log_with_time(f"Starting AI provider: {ai_provider} with model: {chat_llm}")
    log_with_time(f"Prompt preview: {prompt[:100]}...")

    ## Generation  ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Å‡∏≤‡∏£‡∏ï‡∏≠‡∏ö chat
    try:
        stream = call_ai_provider(
            provider_name=ai_provider,
            model=chat_llm,
            messages=[{"role": "user", "content": prompt}],
            stream=True,
            temperature=0.3,
            top_p=0.9,
            max_tokens=2000,
            num_predict=1500
        )
        measure_time(api_call_start, f"{ai_provider}.chat() API call")
        log_with_time(f"{ai_provider} call successful, returning stream")
        measure_time(overall_start, "Total LLM response setup time")
        return stream
    except Exception as e:
        measure_time(api_call_start, f"{ai_provider} API call (failed)")
        log_with_time(f"{ai_provider} call failed: {e}")
        # Return empty stream instead of None
        error_msg = f"‚ùå LLM call failed: {str(e)}"
        return ({"message": {"content": error_msg}} for _ in range(1))

async def query_rag_with_lightrag(
    question: str,
    chat_llm: str = "gemma3:latest",
    ai_provider: str = "ollama",
    show_source: bool = False,
    formal_style: bool = False,
    use_graph_reasoning: bool = False,
    reasoning_mode: str = "hybrid"
):
    """
    Enhanced RAG query with LightRAG graph reasoning capabilities

    Args:
        question: User's question
        chat_llm: LLM model to use
        show_source: Whether to show source information
        formal_style: Whether to use formal response style
        use_graph_reasoning: Whether to use LightRAG graph reasoning
        reasoning_mode: LightRAG reasoning mode ("naive", "local", "global", "hybrid")
    """
    global summarize, enhanced_rag

    if not LIGHT_RAG_AVAILABLE or not use_graph_reasoning:
        # Fallback to standard query_rag
        logging.info("üîÑ Using standard RAG (LightRAG not available or disabled)")
        return query_rag(question, chat_llm, show_source, formal_style)

    try:
        log_with_time("üß† Starting LightRAG-enhanced query...")

        # Initialize LightRAG system if needed
        try:
            await initialize_lightrag_system()
        except Exception as e:
            logging.warning(f"‚ö†Ô∏è LightRAG initialization failed, using standard RAG: {e}")
            return query_rag(question, chat_llm, show_source, formal_style)

        # Perform graph reasoning query
        lightrag_result = await query_with_graph_reasoning(question, reasoning_mode)

        if "error" in lightrag_result:
            logging.warning(f"‚ö†Ô∏è LightRAG query failed, using standard RAG: {lightrag_result['error']}")
            return query_rag(question, chat_llm, show_source, formal_style)

        # Extract graph reasoning result
        graph_answer = lightrag_result.get("result", "")
        graph_insights = lightrag_result.get("graph_insights", {})
        processing_time = lightrag_result.get("processing_time", 0)

        log_with_time(f"‚úÖ LightRAG query completed in {processing_time:.2f}s")
        log_with_time(f"üß† Graph insights: {graph_insights}")

        # Build enhanced response combining standard RAG and graph reasoning
        standard_answer = query_rag(question, chat_llm, show_source, formal_style)

        # Extract text from standard answer stream
        standard_text = ""
        for chunk in standard_answer:
            if "message" in chunk and "content" in chunk["message"]:
                standard_text += chunk["message"]["content"]

        # Combine answers
        if graph_answer and graph_answer != "Error":
            combined_answer = f"""üß† **Graph Reasoning Analysis:**
{graph_answer}

üìö **Traditional RAG Analysis:**
{standard_text}

---
*This response combines graph-based reasoning with traditional document retrieval for comprehensive analysis.*"""
        else:
            combined_answer = standard_text

        # Return as stream for compatibility
        return ({"message": {"content": combined_answer}} for _ in range(1))

    except Exception as e:
        logging.error(f"‚ùå LightRAG-enhanced query failed: {e}")
        # Fallback to standard RAG
        return query_rag(question, chat_llm, show_source, formal_style)

def query_rag_with_multi_hop(
    question: str,
    chat_llm: str = "gemma3:latest",
    ai_provider: str = "ollama",
    show_source: bool = False,
    formal_style: bool = False,
    hop_count: int = 2
):
    """
    Multi-hop reasoning query using LightRAG

    Args:
        question: Initial question
        chat_llm: LLM model to use
        show_source: Whether to show source information
        formal_style: Whether to use formal response style
        hop_count: Number of reasoning hops
    """
    if not LIGHT_RAG_AVAILABLE:
        # Fallback to standard query_rag
        logging.info("üîÑ Using standard RAG (LightRAG not available)")
        return query_rag(question, chat_llm, show_source, formal_style)

    try:
        import asyncio

        log_with_time(f"üîÑ Starting multi-hop reasoning query (hops: {hop_count})...")

        # Run async multi-hop query
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)

        try:
            result = loop.run_until_complete(multi_hop_reasoning(question, hop_count))
        finally:
            loop.close()

        if "error" in result:
            logging.warning(f"‚ö†Ô∏è Multi-hop query failed, using standard RAG: {result['error']}")
            return query_rag(question, chat_llm, show_source, formal_style)

        # Format multi-hop result
        synthesis = result.get("final_synthesis", "No synthesis available")
        hop_results = result.get("hop_results", [])

        formatted_result = f"""üîÑ **Multi-Hop Reasoning Analysis ({hop_count} hops):**

{synthesis}

**Reasoning Steps:**"""

        for i, hop in enumerate(hop_results):
            hop_text = hop.get("result", "")[:500]  # Limit length
            formatted_result += f"""
*Hop {i+1}:* {hop_text}..."""

        formatted_result += f"""

---
*This response uses multi-hop reasoning to explore relationships and implications beyond the initial query.*"""

        # Return as stream for compatibility
        return ({"message": {"content": formatted_result}} for _ in range(1))

    except Exception as e:
        logging.error(f"‚ùå Multi-hop query failed: {e}")
        # Fallback to standard RAG
        return query_rag(question, chat_llm, show_source, formal_style)

def get_lightrag_system_status():
    """Get comprehensive LightRAG system status"""
    if not LIGHT_RAG_AVAILABLE:
        return {
            "status": "‚ùå LightRAG Not Available",
            "version": "N/A",
            "graph_built": False,
            "chroma_records": "N/A"
        }

    try:
        # Get LightRAG status using synchronous function
        status = get_lightrag_status()

        # Get ChromaDB record count
        chroma_count = collection.count() if collection else 0

        return {
            "status": "‚úÖ LightRAG Available",
            "lightrag_status": status,
            "chroma_records": chroma_count,
            "graph_available": os.path.exists("./data/lightrag")
        }

    except Exception as e:
        logging.error(f"‚ùå Failed to get LightRAG status: {e}")
        return {
            "status": "‚ö†Ô∏è LightRAG Error",
            "error": str(e),
            "chroma_records": collection.count() if collection else 0
        }

# UI Event Handlers
def handle_file_selection(files):
    """
    Handle file selection with improved file list display
    """
    if not files:
        return gr.update(value=""), gr.update(visible=False)

    # Prepare file information
    file_info_list = []
    for file in files:
        file_path = file.name if hasattr(file, 'name') else str(file)
        file_name = os.path.basename(file_path)
        file_ext = os.path.splitext(file_name)[1].lower()

        # Determine file type
        if file_ext == '.pdf':
            file_type = 'üìÑ PDF'
        elif file_ext in ['.txt', '.md']:
            file_type = 'üìù Text'
        elif file_ext in ['.docx', '.doc']:
            file_type = 'üìã Word'
        else:
            file_type = 'üìÅ File'

        # Get file size
        try:
            if hasattr(file, 'size'):
                file_size = file.size
            elif os.path.exists(file_path):
                file_size = os.path.getsize(file_path)
            else:
                file_size = 0

            # Format file size
            if file_size < 1024:
                size_str = f"{file_size} B"
            elif file_size < 1024 * 1024:
                size_str = f"{file_size / 1024:.1f} KB"
            else:
                size_str = f"{file_size / (1024 * 1024):.1f} MB"
        except:
            size_str = "Unknown"

        file_info_list.append({
            "name": file_name,
            "type": file_type,
            "size": size_str,
            "path": file_path
        })

    # Format file list display
    file_list_text = "## üìã Selected Files\n\n"
    for i, info in enumerate(file_info_list, 1):
        file_list_text += f"**{i}. {info['type']} {info['name']}**\n"
        file_list_text += f"   üìè Size: {info['size']}\n"
        file_list_text += f"   üìç Path: `{info['path']}`\n\n"

    return file_list_text, gr.update(visible=True)

def user(user_message: str, history: List[Dict]) -> Tuple[str, List[Dict]]:
    """
    ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ input ‡∏Ç‡∏≠‡∏á‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ‡πÅ‡∏•‡∏∞‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏•‡∏á‡πÉ‡∏ô‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡∏Å‡∏≤‡∏£‡πÅ‡∏ä‡∏ó
    """
    return "", history + [{"role": "user", "content": user_message}]


# ==================== FEEDBACK FUNCTIONS ====================

def save_feedback(question: str, answer: str, feedback_type: str, user_comment: str = "",
                  corrected_answer: str = "", model_used: str = "", sources: str = ""):
    """‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å feedback ‡∏•‡∏á‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•"""
    try:
        conn = sqlite3.connect(FEEDBACK_DB_PATH)
        cursor = conn.cursor()

        cursor.execute('''
            INSERT INTO feedback (question, answer, feedback_type, user_comment, corrected_answer, model_used, sources)
            VALUES (?, ?, ?, ?, ?, ?, ?)
        ''', (question, answer, feedback_type, user_comment, corrected_answer, model_used, sources))

        feedback_id = cursor.lastrowid

        # ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç ‡πÉ‡∏´‡πâ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏•‡∏á‡∏ï‡∏≤‡∏£‡∏≤‡∏á corrected_answers
        if feedback_type == "bad" and corrected_answer and corrected_answer.strip():
            try:
                # ‡∏™‡∏£‡πâ‡∏≤‡∏á embedding ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏° (‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏Ñ‡∏•‡πâ‡∏≤‡∏¢‡∏Å‡∏±‡∏ô)
                question_embedding = sentence_model.encode(question, convert_to_tensor=True).cpu().numpy()
                embedding_str = json.dumps(question_embedding.tolist())

                cursor.execute('''
                    INSERT INTO corrected_answers (original_question, original_answer, corrected_answer, feedback_id, question_embedding)
                    VALUES (?, ?, ?, ?, ?)
                ''', (question, answer, corrected_answer, feedback_id, embedding_str))

                logging.info(f"‚úÖ Saved corrected answer for learning: {question[:50]}...")

            except Exception as e:
                logging.warning(f"‚ö†Ô∏è Failed to create embedding for corrected answer: {str(e)}")

        conn.commit()
        conn.close()

        logging.info(f"‚úÖ Saved {feedback_type} feedback for question: {question[:50]}...")
        return True
    except Exception as e:
        logging.error(f"‚ùå Failed to save feedback: {str(e)}")
        return False


def find_similar_corrected_answer(question: str, threshold: float = 0.8, include_weighted: bool = True) -> dict:
    """‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏Ñ‡∏•‡πâ‡∏≤‡∏¢‡∏Å‡∏±‡∏ô (Enhanced with weighted scoring)"""
    try:
        conn = sqlite3.connect(FEEDBACK_DB_PATH)
        cursor = conn.cursor()

        cursor.execute('''
            SELECT ca.original_question, ca.original_answer, ca.corrected_answer,
                   ca.question_embedding, ca.applied_count, ca.feedback_id,
                   f.feedback_type, f.user_comment, f.timestamp
            FROM corrected_answers ca
            JOIN feedback f ON ca.feedback_id = f.id
            ORDER BY ca.created_at DESC
        ''')

        rows = cursor.fetchall()
        conn.close()

        if not rows:
            return None

        # ‡∏™‡∏£‡πâ‡∏≤‡∏á embedding ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô
        question_embedding = sentence_model.encode(question, convert_to_tensor=True).cpu().numpy()

        best_match = None
        best_score = 0

        for row in rows:
            try:
                stored_embedding = json.loads(row[3])
                stored_embedding = np.array(stored_embedding)

                # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì cosine similarity
                similarity = np.dot(question_embedding, stored_embedding) / (
                    np.linalg.norm(question_embedding) * np.linalg.norm(stored_embedding)
                )

                # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì weighted score (‡∏û‡∏¥‡∏à‡∏≤‡∏£‡∏ì‡∏≤‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏Å‡πà‡∏≤‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô)
                recency_factor = 1.0  # ‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÄ‡∏û‡∏¥‡πà‡∏° logic ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö recency ‡πÑ‡∏î‡πâ
                usage_factor = min(row[4] * 0.1, 1.0)  # ‡∏à‡∏≥‡∏Å‡∏±‡∏î usage factor ‡∏ó‡∏µ‡πà 1.0
                feedback_quality = 1.2 if row[6] == 'good' else 1.0  # feedback type weight

                weighted_score = similarity * recency_factor * usage_factor * feedback_quality

                if similarity > threshold and weighted_score > best_score:
                    best_match = {
                        'original_question': row[0],
                        'original_answer': row[1],
                        'corrected_answer': row[2],
                        'similarity': similarity,
                        'weighted_score': weighted_score,
                        'applied_count': row[4],
                        'feedback_type': row[6],
                        'user_comment': row[7],
                        'feedback_id': row[5]
                    }
                    best_score = weighted_score

            except Exception as e:
                logging.warning(f"‚ö†Ô∏è Error processing embedding: {str(e)}")
                continue

        return best_match

    except Exception as e:
        logging.error(f"‚ùå Failed to find similar corrected answer: {str(e)}")
        return None


def calculate_feedback_priority(question: str, corrected_answer: str, confidence: float) -> float:
    """‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì priority score ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö feedback (0.0 - 1.0)"""
    try:
        priority = confidence * 0.4  # 40% weight from confidence

        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ã‡∏±‡∏ö‡∏ã‡πâ‡∏≠‡∏ô‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏° (‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏ã‡∏±‡∏ö‡∏ã‡πâ‡∏≠‡∏ô‡πÑ‡∏î‡πâ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏™‡∏π‡∏á‡∏Å‡∏ß‡πà‡∏≤)
        question_complexity = len(question.split()) * 0.01
        priority += min(question_complexity, 0.2)  # 20% weight from complexity

        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç (‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏¢‡∏≤‡∏ß‡πÜ ‡∏°‡∏±‡∏Å‡∏°‡∏µ‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡πà‡∏≤‡∏™‡∏π‡∏á)
        answer_value = min(len(corrected_answer.split()) * 0.005, 0.2)  # 20% weight from answer quality
        priority += answer_value

        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏°‡∏µ‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏Ñ‡∏•‡πâ‡∏≤‡∏¢‡∏Å‡∏±‡∏ô‡πÄ‡∏Ñ‡∏¢‡∏°‡∏µ‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
        similar_issues = check_similar_issue_frequency(question)
        issue_frequency_bonus = min(similar_issues * 0.05, 0.2)  # 20% weight from frequency
        priority += issue_frequency_bonus

        return min(priority, 1.0)

    except Exception as e:
        logging.error(f"‚ùå Error calculating feedback priority: {str(e)}")
        return 0.5  # Default priority

def check_similar_issue_frequency(question: str) -> int:
    """‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏°‡∏µ‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏Ñ‡∏•‡πâ‡∏≤‡∏¢‡∏Å‡∏±‡∏ô‡πÄ‡∏Ñ‡∏¢‡∏°‡∏µ‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏ö‡πà‡∏≠‡∏¢‡πÅ‡∏Ñ‡πà‡πÑ‡∏´‡∏ô"""
    try:
        conn = sqlite3.connect(FEEDBACK_DB_PATH)
        cursor = conn.cursor()

        # ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏Ñ‡∏•‡πâ‡∏≤‡∏¢‡∏Å‡∏±‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏Ñ‡∏¢‡∏°‡∏µ‡∏õ‡∏±‡∏ç‡∏´‡∏≤
        question_start = question[:20] if len(question) > 20 else question
        question_end = question[-20:] if len(question) > 20 else ""
        cursor.execute('''
            SELECT COUNT(*) FROM feedback
            WHERE feedback_type = 'bad'
            AND (question LIKE ? OR question LIKE ?)
        ''', (f"%{question_start}%", f"%{question_end}%"))

        count = cursor.fetchone()[0]
        conn.close()
        return count

    except Exception as e:
        logging.error(f"‚ùå Error checking similar issue frequency: {str(e)}")
        return 0

def apply_feedback_to_rag(question: str, corrected_answer: str, confidence: float = 0.9) -> bool:
    """‡∏ô‡∏≥ feedback ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡πÑ‡∏õ‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á RAG ‡∏ó‡∏±‡∏ô‡∏ó‡∏µ (Real-time Learning Integration)"""
    try:
        # 1. ‡∏™‡∏£‡πâ‡∏≤‡∏á embedding ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö corrected answer
        question_embedding = sentence_model.encode(question, convert_to_tensor=True).cpu().numpy()
        answer_embedding = sentence_model.encode(corrected_answer, convert_to_tensor=True).cpu().numpy()

        # 2. ‡πÄ‡∏û‡∏¥‡πà‡∏° corrected answer ‡πÄ‡∏Ç‡πâ‡∏≤ vector database ‡∏û‡∏£‡πâ‡∏≠‡∏° high weight
        global chroma_client
        collection = chroma_client.get_or_create_collection(name="pdf_data")

        # ‡∏™‡∏£‡πâ‡∏≤‡∏á unique ID ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö corrected answer
        corrected_id = f"corrected_{abs(hash(question + corrected_answer))}_{int(time.time())}"

        # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì priority score ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö corrected answer
        priority_score = calculate_feedback_priority(question, corrected_answer, confidence)

        # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏Ç‡πâ‡∏≤ ChromaDB ‡∏û‡∏£‡πâ‡∏≠‡∏° metadata ‡πÅ‡∏•‡∏∞ priority
        collection.add(
            embeddings=[question_embedding.tolist()],
            documents=[f"Q: {question}\nA: {corrected_answer}"],
            metadatas=[{
                "source": "feedback_corrected",
                "confidence": confidence,
                "priority_score": priority_score,
                "question": question,
                "answer": corrected_answer,
                "type": "corrected_answer",
                "created_at": datetime.now().isoformat()
            }],
            ids=[corrected_id]
        )

        logging.info(f"‚úÖ Applied feedback to RAG system: {question[:50]}... -> {corrected_answer[:50]}...")
        return True

    except Exception as e:
        logging.error(f"‚ùå Failed to apply feedback to RAG: {str(e)}")
        return False


def increment_corrected_answer_usage(original_question: str) -> bool:
    """‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏ó‡∏µ‡πà‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏ñ‡∏π‡∏Å‡∏ô‡∏≥‡πÑ‡∏õ‡πÉ‡∏ä‡πâ"""
    try:
        conn = sqlite3.connect(FEEDBACK_DB_PATH)
        cursor = conn.cursor()

        cursor.execute('''
            UPDATE corrected_answers
            SET applied_count = applied_count + 1
            WHERE original_question = ?
        ''', (original_question,))

        # ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï feedback table ‡πÉ‡∏´‡πâ applied = TRUE
        cursor.execute('''
            UPDATE feedback
            SET applied = TRUE
            WHERE question = ? AND corrected_answer != '' AND corrected_answer IS NOT NULL
        ''', (original_question,))

        conn.commit()
        conn.close()

        logging.info(f"‚úÖ Incremented usage count for corrected answer: {original_question[:50]}...")
        return True

    except Exception as e:
        logging.error(f"‚ùå Failed to increment corrected answer usage: {str(e)}")
        return False


def get_learning_stats():
    """‡∏î‡∏∂‡∏á‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ"""
    try:
        conn = sqlite3.connect(FEEDBACK_DB_PATH)
        cursor = conn.cursor()

        # ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
        cursor.execute("SELECT COUNT(*) FROM corrected_answers")
        total_corrected = cursor.fetchone()[0]

        # ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ô‡∏≥‡πÑ‡∏õ‡πÉ‡∏ä‡πâ
        cursor.execute("SELECT COUNT(*) FROM corrected_answers WHERE applied_count > 0")
        used_corrected = cursor.fetchone()[0]

        # ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô feedback ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
        cursor.execute("SELECT COUNT(*) FROM feedback")
        total_feedback = cursor.fetchone()[0]

        # ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô feedback ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç
        cursor.execute("SELECT COUNT(*) FROM feedback WHERE corrected_answer != '' AND corrected_answer IS NOT NULL")
        corrected_feedback = cursor.fetchone()[0]

        # ‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡πÉ‡∏ä‡πâ‡∏ö‡πà‡∏≠‡∏¢‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î
        cursor.execute('''
            SELECT original_question, applied_count
            FROM corrected_answers
            WHERE applied_count > 0
            ORDER BY applied_count DESC
            LIMIT 5
        ''')
        most_used = cursor.fetchall()

        conn.close()

        return {
            'total_corrected': total_corrected,
            'used_corrected': used_corrected,
            'total_feedback': total_feedback,
            'corrected_feedback': corrected_feedback,
            'learning_rate': (used_corrected / total_corrected * 100) if total_corrected > 0 else 0,
            'most_used': most_used
        }

    except Exception as e:
        logging.error(f"‚ùå Failed to get learning stats: {str(e)}")
        return {
            'total_corrected': 0, 'used_corrected': 0, 'total_feedback': 0,
            'corrected_feedback': 0, 'learning_rate': 0, 'most_used': []
        }

# Tag Management Functions
def create_tag(name: str, color: str = '#007bff', description: str = '') -> bool:
    """‡∏™‡∏£‡πâ‡∏≤‡∏á tag ‡πÉ‡∏´‡∏°‡πà"""
    try:
        conn = sqlite3.connect(FEEDBACK_DB_PATH)
        cursor = conn.cursor()

        cursor.execute('''
            INSERT INTO tags (name, color, description) VALUES (?, ?, ?)
        ''', (name, color, description))

        conn.commit()
        conn.close()
        logging.info(f"‚úÖ Created tag: {name}")
        return True

    except sqlite3.IntegrityError:
        logging.warning(f"‚ö†Ô∏è Tag '{name}' already exists")
        return False
    except Exception as e:
        logging.error(f"‚ùå Failed to create tag: {str(e)}")
        return False

def analyze_feedback_patterns() -> dict:
    """‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö feedback ‡∏î‡πâ‡∏ß‡∏¢ AI ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏´‡∏≤ insights"""
    try:
        conn = sqlite3.connect(FEEDBACK_DB_PATH)
        cursor = conn.cursor()

        # ‡∏î‡∏∂‡∏á feedback ‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î 100 ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£
        cursor.execute('''
            SELECT question, answer, feedback_type, user_comment, corrected_answer, timestamp
            FROM feedback
            ORDER BY timestamp DESC
            LIMIT 100
        ''')

        feedback_data = cursor.fetchall()
        conn.close()

        if not feedback_data:
            return {"patterns": [], "recommendations": [], "quality_score": 0}

        # ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏´‡∏°‡∏ß‡∏î‡∏´‡∏°‡∏π‡πà‡∏õ‡∏±‡∏ç‡∏´‡∏≤
        categories = {}
        quality_issues = []
        improvement_suggestions = []

        for fb in feedback_data:
            question, answer, feedback_type, comment, corrected, timestamp = fb

            # ‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏à‡∏≤‡∏Å comments
            if comment:
                comment_lower = comment.lower()
                if any(word in comment_lower for word in ['‡πÑ‡∏°‡πà‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à', '‡∏™‡∏±‡∏ö‡∏™‡∏ô', '‡∏¢‡∏≤‡∏Å', '‡∏ã‡∏±‡∏ö‡∏ã‡πâ‡∏≠‡∏ô']):
                    categories.setdefault('‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à', 0)
                    categories['‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à'] += 1
                elif any(word in comment_lower for word in ['‡πÑ‡∏°‡πà‡∏Ñ‡∏£‡∏ö', '‡∏Ç‡∏≤‡∏î', '‡πÄ‡∏û‡∏¥‡πà‡∏°', '‡πÑ‡∏°‡πà‡∏û‡∏≠']):
                    categories.setdefault('‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏£‡∏ö‡∏ñ‡πâ‡∏ß‡∏ô', 0)
                    categories['‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏£‡∏ö‡∏ñ‡πâ‡∏ß‡∏ô'] += 1
                elif any(word in comment_lower for word in ['‡πÅ‡∏´‡∏•‡πà‡∏á', '‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á', 'source', 'reference']):
                    categories.setdefault('‡πÅ‡∏´‡∏•‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•', 0)
                    categories['‡πÅ‡∏´‡∏•‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•'] += 1

            # ‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏ï‡πà‡∏≥
            if feedback_type == 'bad' and corrected:
                quality_issues.append({
                    'question': question[:100],
                    'issue_type': 'incorrect_answer',
                    'has_correction': True
                })

        # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥
        if categories.get('‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à', 0) > 5:
            improvement_suggestions.append("üîç ‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏°‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡πÉ‡∏´‡πâ‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏á‡πà‡∏≤‡∏¢‡∏Ç‡∏∂‡πâ‡∏ô")
        if categories.get('‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏£‡∏ö‡∏ñ‡πâ‡∏ß‡∏ô', 0) > 5:
            improvement_suggestions.append("üìù ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡πÉ‡∏ô‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡πÉ‡∏´‡πâ‡∏Ñ‡∏£‡∏ö‡∏ñ‡πâ‡∏ß‡∏ô‡∏Ç‡∏∂‡πâ‡∏ô")
        if categories.get('‡πÅ‡∏´‡∏•‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•', 0) > 3:
            improvement_suggestions.append("üìé ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏Ç‡∏≠‡∏á‡πÅ‡∏´‡∏•‡πà‡∏á‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á")

        # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û
        total_feedback = len(feedback_data)
        good_feedback = sum(1 for fb in feedback_data if fb[2] == 'good')
        quality_score = (good_feedback / total_feedback * 100) if total_feedback > 0 else 0

        return {
            "patterns": categories,
            "quality_issues": quality_issues[:10],  # ‡∏à‡∏≥‡∏Å‡∏±‡∏î 10 ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£
            "recommendations": improvement_suggestions,
            "quality_score": quality_score,
            "total_analyzed": total_feedback
        }

    except Exception as e:
        logging.error(f"‚ùå Failed to analyze feedback patterns: {str(e)}")
        return {"patterns": [], "recommendations": [], "quality_score": 0}

def get_comprehensive_analytics() -> dict:
    """‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• analytics ‡πÅ‡∏ö‡∏ö‡∏Ñ‡∏£‡∏ö‡∏ñ‡πâ‡∏ß‡∏ô"""
    try:
        # ‡∏î‡∏∂‡∏á‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô
        basic_stats = get_feedback_stats()
        learning_stats = get_learning_stats()
        pattern_analysis = analyze_feedback_patterns()

        # ‡∏î‡∏∂‡∏á‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥‡∏ï‡∏≤‡∏°‡∏ä‡πà‡∏ß‡∏á‡πÄ‡∏ß‡∏•‡∏≤
        conn = sqlite3.connect(FEEDBACK_DB_PATH)
        cursor = conn.cursor()

        # Feedback ‡πÉ‡∏ô‡∏ä‡πà‡∏ß‡∏á 7 ‡∏ß‡∏±‡∏ô‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î
        cursor.execute('''
            SELECT DATE(timestamp) as date, COUNT(*) as count,
                   SUM(CASE WHEN feedback_type = 'good' THEN 1 ELSE 0 END) as good_count
            FROM feedback
            WHERE timestamp >= DATE('now', '-7 days')
            GROUP BY DATE(timestamp)
            ORDER BY date DESC
        ''')
        weekly_trend = cursor.fetchall()

        # Top problematic questions
        cursor.execute('''
            SELECT question, COUNT(*) as issue_count
            FROM feedback
            WHERE feedback_type = 'bad'
            GROUP BY question
            HAVING issue_count > 1
            ORDER BY issue_count DESC
            LIMIT 10
        ''')
        problematic_questions = cursor.fetchall()

        conn.close()

        return {
            "basic_stats": basic_stats,
            "learning_stats": learning_stats,
            "pattern_analysis": pattern_analysis,
            "weekly_trend": weekly_trend,
            "problematic_questions": problematic_questions,
            "generated_at": datetime.now().isoformat()
        }

    except Exception as e:
        logging.error(f"‚ùå Failed to get comprehensive analytics: {str(e)}")
        return {}

def get_all_tags() -> list:
    """‡∏î‡∏∂‡∏á tags ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î"""
    try:
        conn = sqlite3.connect(FEEDBACK_DB_PATH)
        cursor = conn.cursor()

        cursor.execute('''
            SELECT id, name, color, description, created_at
            FROM tags
            ORDER BY name
        ''')

        tags = cursor.fetchall()
        conn.close()
        return tags

    except Exception as e:
        logging.error(f"‚ùå Failed to get tags: {str(e)}")
        return []

def tag_document(document_id: str, tag_id: int) -> bool:
    """‡∏Å‡∏≥‡∏´‡∏ô‡∏î tag ‡πÉ‡∏´‡πâ‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£"""
    try:
        conn = sqlite3.connect(FEEDBACK_DB_PATH)
        cursor = conn.cursor()

        cursor.execute('''
            INSERT OR IGNORE INTO document_tags (document_id, tag_id) VALUES (?, ?)
        ''', (document_id, tag_id))

        conn.commit()
        conn.close()
        return True

    except Exception as e:
        logging.error(f"‚ùå Failed to tag document: {str(e)}")
        return False

def tag_feedback(feedback_id: int, tag_id: int) -> bool:
    """‡∏Å‡∏≥‡∏´‡∏ô‡∏î tag ‡πÉ‡∏´‡πâ feedback"""
    try:
        conn = sqlite3.connect(FEEDBACK_DB_PATH)
        cursor = conn.cursor()

        cursor.execute('''
            INSERT OR IGNORE INTO feedback_tags (feedback_id, tag_id) VALUES (?, ?)
        ''', (feedback_id, tag_id))

        conn.commit()
        conn.close()
        return True

    except Exception as e:
        logging.error(f"‚ùå Failed to tag feedback: {str(e)}")
        return False

def get_documents_by_tag(tag_id: int) -> list:
    """‡∏î‡∏∂‡∏á‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ï‡∏≤‡∏° tag"""
    try:
        conn = sqlite3.connect(FEEDBACK_DB_PATH)
        cursor = conn.cursor()

        cursor.execute('''
            SELECT DISTINCT dt.document_id
            FROM document_tags dt
            WHERE dt.tag_id = ?
        ''', (tag_id,))

        document_ids = [row[0] for row in cursor.fetchall()]
        conn.close()
        return document_ids

    except Exception as e:
        logging.error(f"‚ùå Failed to get documents by tag: {str(e)}")
        return []

def get_feedback_by_tag(tag_id: int) -> list:
    """‡∏î‡∏∂‡∏á feedback ‡∏ï‡∏≤‡∏° tag"""
    try:
        conn = sqlite3.connect(FEEDBACK_DB_PATH)
        cursor = conn.cursor()

        cursor.execute('''
            SELECT f.id, f.question, f.answer, f.feedback_type, f.timestamp, f.user_comment
            FROM feedback f
            JOIN feedback_tags ft ON f.id = ft.feedback_id
            WHERE ft.tag_id = ?
            ORDER BY f.timestamp DESC
        ''', (tag_id,))

        feedback = cursor.fetchall()
        conn.close()
        return feedback

    except Exception as e:
        logging.error(f"‚ùå Failed to get feedback by tag: {str(e)}")
        return []

def search_documents_by_tags(tag_ids: list) -> list:
    """‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ï‡∏≤‡∏°‡∏´‡∏•‡∏≤‡∏¢ tags (AND logic)"""
    try:
        if not tag_ids:
            return []

        conn = sqlite3.connect(FEEDBACK_DB_PATH)
        cursor = conn.cursor()

        # Build dynamic query for AND logic
        placeholders = ','.join(['?' for _ in tag_ids])
        query = f'''
            SELECT dt.document_id, COUNT(*) as match_count
            FROM document_tags dt
            WHERE dt.tag_id IN ({placeholders})
            GROUP BY dt.document_id
            HAVING match_count = ?
        '''

        cursor.execute(query, tag_ids + [len(tag_ids)])
        document_ids = [row[0] for row in cursor.fetchall()]
        conn.close()
        return document_ids

    except Exception as e:
        logging.error(f"‚ùå Failed to search documents by tags: {str(e)}")
        return []

def delete_tag(tag_id: int) -> bool:
    """‡∏•‡∏ö tag"""
    try:
        conn = sqlite3.connect(FEEDBACK_DB_PATH)
        cursor = conn.cursor()

        # Delete from junction tables first (foreign key constraints)
        cursor.execute('DELETE FROM document_tags WHERE tag_id = ?', (tag_id,))
        cursor.execute('DELETE FROM feedback_tags WHERE tag_id = ?', (tag_id,))

        # Delete the tag
        cursor.execute('DELETE FROM tags WHERE id = ?', (tag_id,))

        conn.commit()
        conn.close()
        logging.info(f"‚úÖ Deleted tag: {tag_id}")
        return True

    except Exception as e:
        logging.error(f"‚ùå Failed to delete tag: {str(e)}")
        return False

def get_tag_stats() -> dict:
    """‡∏î‡∏∂‡∏á‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô tags"""
    try:
        conn = sqlite3.connect(FEEDBACK_DB_PATH)
        cursor = conn.cursor()

        # Most used tags
        cursor.execute('''
            SELECT t.name, COUNT(dt.id) as usage_count
            FROM tags t
            LEFT JOIN document_tags dt ON t.id = dt.tag_id
            GROUP BY t.id, t.name
            ORDER BY usage_count DESC
            LIMIT 10
        ''')

        most_used_tags = cursor.fetchall()

        # Tags with feedback
        cursor.execute('''
            SELECT t.name, COUNT(ft.id) as feedback_count
            FROM tags t
            LEFT JOIN feedback_tags ft ON t.id = ft.tag_id
            GROUP BY t.id, t.name
            ORDER BY feedback_count DESC
            LIMIT 10
        ''')

        feedback_tags = cursor.fetchall()

        conn.close()

        return {
            'most_used_tags': most_used_tags,
            'feedback_tags': feedback_tags
        }

    except Exception as e:
        logging.error(f"‚ùå Failed to get tag stats: {str(e)}")
        return {
            'most_used_tags': [],
            'feedback_tags': []
        }

# Tag UI Helper Functions
def refresh_tags_list():
    """‡∏£‡∏µ‡πÄ‡∏ü‡∏£‡∏ä‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£ tags"""
    try:
        tags = get_all_tags()
        tag_choices = [(f"üè∑Ô∏è {tag[1]}", tag[0]) for tag in tags]
        tag_data = [[tag[0], tag[1], tag[2], tag[3] or "", tag[4]] for tag in tags]
        return tag_data, tag_choices, gr.HTML(""), ""
    except Exception as e:
        logging.error(f"‚ùå Failed to refresh tags: {str(e)}")
        return [], [], gr.HTML(f'<div style="color: red;">‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î: {str(e)}</div>'), ""

def create_new_tag(name: str, color: str, description: str):
    """‡∏™‡∏£‡πâ‡∏≤‡∏á tag ‡πÉ‡∏´‡∏°‡πà"""
    if not name.strip():
        return [], [], gr.HTML('<div style="color: orange;">‚ö†Ô∏è ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡πÉ‡∏™‡πà‡∏ä‡∏∑‡πà‡∏≠ Tag</div>'), ""

    try:
        success = create_tag(name.strip(), color, description.strip())
        if success:
            return refresh_tags_list()
        else:
            return [], [], gr.HTML('<div style="color: orange;">‚ö†Ô∏è Tag ‡∏ô‡∏µ‡πâ‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß</div>'), ""
    except Exception as e:
        logging.error(f"‚ùå Failed to create tag: {str(e)}")
        return [], [], gr.HTML(f'<div style="color: red;">‚ùå ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏™‡∏£‡πâ‡∏≤‡∏á Tag ‡πÑ‡∏î‡πâ: {str(e)}</div>'), ""

def delete_selected_tag(selected_row: dict):
    """‡∏•‡∏ö tag ‡∏ó‡∏µ‡πà‡πÄ‡∏•‡∏∑‡∏≠‡∏Å"""
    try:
        if not selected_row or not selected_row.get("ID"):
            return [], [], gr.HTML('<div style="color: orange;">‚ö†Ô∏è ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡πÄ‡∏•‡∏∑‡∏≠‡∏Å Tag ‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏•‡∏ö</div>'), ""

        tag_id = selected_row["ID"]
        tag_name = selected_row.get("‡∏ä‡∏∑‡πà‡∏≠ Tag", "")

        success = delete_tag(tag_id)
        if success:
            tag_data, tag_choices, _, _ = refresh_tags_list()
            return tag_data, tag_choices, gr.HTML(f'<div style="color: green;">‚úÖ ‡∏•‡∏ö Tag "{tag_name}" ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à</div>'), ""
        else:
            return [], [], gr.HTML('<div style="color: red;">‚ùå ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏•‡∏ö Tag ‡πÑ‡∏î‡πâ</div>'), ""
    except Exception as e:
        logging.error(f"‚ùå Failed to delete tag: {str(e)}")
        return [], [], gr.HTML(f'<div style="color: red;">‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î: {str(e)}</div>'), ""

def update_tag_statistics():
    """‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥ tags"""
    try:
        stats = get_tag_stats()
        popular_data = [[tag[0], tag[1]] for tag in stats['most_used_tags']]
        feedback_data = [[tag[0], tag[1]] for tag in stats['feedback_tags']]
        return popular_data, feedback_data
    except Exception as e:
        logging.error(f"‚ùå Failed to update tag stats: {str(e)}")
        return [], []

def search_documents_by_selected_tags(selected_tags: list):
    """‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ï‡∏≤‡∏° tags ‡∏ó‡∏µ‡πà‡πÄ‡∏•‡∏∑‡∏≠‡∏Å"""
    try:
        if not selected_tags:
            return [], gr.HTML('<div style="color: orange;">‚ö†Ô∏è ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ô‡πâ‡∏≠‡∏¢ 1 Tag</div>')

        # Extract tag IDs from selected labels
        tags = get_all_tags()
        tag_id_map = {f"üè∑Ô∏è {tag[1]}": tag[0] for tag in tags}
        selected_tag_ids = [tag_id_map[tag] for tag in selected_tags if tag in tag_id_map]

        if not selected_tag_ids:
            return [], gr.HTML('<div style="color: orange;">‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏û‡∏ö Tags ‡∏ó‡∏µ‡πà‡πÄ‡∏•‡∏∑‡∏≠‡∏Å</div>')

        document_ids = search_documents_by_tags(selected_tag_ids)

        if not document_ids:
            return [], gr.HTML('<div style="color: blue;">‚ÑπÔ∏è ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ó‡∏µ‡πà‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö Tags ‡∏ó‡∏µ‡πà‡πÄ‡∏•‡∏∑‡∏≠‡∏Å</div>')

        # Get content preview from ChromaDB
        search_data = []
        for doc_id in document_ids[:20]:  # Limit to 20 results
            try:
                result = collection.get(ids=[doc_id])
                if result['documents']:
                    content = result['documents'][0][:100] + "..." if len(result['documents'][0]) > 100 else result['documents'][0]
                    search_data.append([doc_id, content])
            except:
                search_data.append([doc_id, "‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÇ‡∏´‡∏•‡∏î‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡πÑ‡∏î‡πâ"])

        status = gr.HTML(f'<div style="color: green;">‚úÖ ‡∏û‡∏ö {len(search_data)} ‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£</div>')
        return search_data, status
    except Exception as e:
        logging.error(f"‚ùå Failed to search by tags: {str(e)}")
        return [], gr.HTML(f'<div style="color: red;">‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î: {str(e)}</div>')

def load_feedback_by_selected_tag(tag_label: str):
    """‡πÇ‡∏´‡∏•‡∏î feedback ‡∏ï‡∏≤‡∏° tag ‡∏ó‡∏µ‡πà‡πÄ‡∏•‡∏∑‡∏≠‡∏Å"""
    try:
        if not tag_label:
            return [], gr.HTML('<div style="color: orange;">‚ö†Ô∏è ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡πÄ‡∏•‡∏∑‡∏≠‡∏Å Tag</div>')

        # Get tag ID from label
        tags = get_all_tags()
        tag_id_map = {f"üè∑Ô∏è {tag[1]}": tag[0] for tag in tags}
        tag_id = tag_id_map.get(tag_label)

        if not tag_id:
            return [], gr.HTML('<div style="color: orange;">‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏û‡∏ö Tag ‡∏ó‡∏µ‡πà‡πÄ‡∏•‡∏∑‡∏≠‡∏Å</div>')

        feedback_list = get_feedback_by_tag(tag_id)

        if not feedback_list:
            return [], gr.HTML('<div style="color: blue;">‚ÑπÔ∏è ‡πÑ‡∏°‡πà‡∏°‡∏µ Feedback ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Tag ‡∏ô‡∏µ‡πâ</div>')

        # Format feedback data
        feedback_data = []
        for fb in feedback_list:
            question = fb[1][:50] + "..." if len(fb[1]) > 50 else fb[1]
            answer = fb[2][:100] + "..." if len(fb[2]) > 100 else fb[2]
            feedback_data.append([fb[0], question, answer, fb[3], fb[4], fb[5] or ""])

        status = gr.HTML(f'<div style="color: green;">‚úÖ ‡∏û‡∏ö {len(feedback_data)} Feedback</div>')
        return feedback_data, status
    except Exception as e:
        logging.error(f"‚ùå Failed to load feedback by tag: {str(e)}")
        return [], gr.HTML(f'<div style="color: red;">‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î: {str(e)}</div>')

# Enhanced RAG System Classes
import time
import json
import pickle

class PerformanceMonitor:
    """Monitor RAG performance and log metrics"""

    @staticmethod
    def log_performance(session_id: str, rag_mode: str, question: str, response_time: float,
                       context_count: int, memory_hit: bool, success: bool, error_message: str = None):
        """Log performance metrics"""
        try:
            conn = sqlite3.connect(FEEDBACK_DB_PATH)
            cursor = conn.cursor()

            cursor.execute('''
                INSERT INTO rag_performance_log
                (session_id, rag_mode, question, response_time, context_count, memory_hit, success, error_message)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?)
            ''', (session_id, rag_mode, question, response_time, context_count, memory_hit, success, error_message))

            conn.commit()
            conn.close()
        except Exception as e:
            logging.error(f"‚ùå Failed to log performance: {str(e)}")

    @staticmethod
    def get_performance_stats(rag_mode: str = None, limit: int = 100):
        """Get performance statistics"""
        try:
            conn = sqlite3.connect(FEEDBACK_DB_PATH)
            cursor = conn.cursor()

            query = "SELECT * FROM rag_performance_log"
            params = []

            if rag_mode:
                query += " WHERE rag_mode = ?"
                params.append(rag_mode)

            query += " ORDER BY timestamp DESC LIMIT ?"
            params.append(limit)

            cursor.execute(query, params)
            results = cursor.fetchall()
            conn.close()

            return results
        except Exception as e:
            logging.error(f"‚ùå Failed to get performance stats: {str(e)}")
            return []

class ContextCache:
    """Cache contexts for improved performance"""

    @staticmethod
    def _get_question_hash(question: str) -> str:
        """Generate hash for question"""
        import hashlib
        return hashlib.md5(question.encode()).hexdigest()

    @staticmethod
    def get_cached_contexts(question: str) -> list:
        """Get cached contexts for question"""
        try:
            question_hash = ContextCache._get_question_hash(question)
            conn = sqlite3.connect(FEEDBACK_DB_PATH)
            cursor = conn.cursor()

            cursor.execute('''
                SELECT contexts, access_count FROM context_cache
                WHERE question_hash = ?
            ''', (question_hash,))

            result = cursor.fetchone()

            if result:
                # Update access count
                cursor.execute('''
                    UPDATE context_cache
                    SET access_count = access_count + 1
                    WHERE question_hash = ?
                ''', (question_hash,))
                conn.commit()

                conn.close()
                return json.loads(result[0]) if result[0] else []

            conn.close()
            return None
        except Exception as e:
            logging.error(f"‚ùå Failed to get cached contexts: {str(e)}")
            return None

    @staticmethod
    def cache_contexts(question: str, contexts: list, question_embedding):
        """Cache contexts for question"""
        try:
            question_hash = ContextCache._get_question_hash(question)
            conn = sqlite3.connect(FEEDBACK_DB_PATH)
            cursor = conn.cursor()

            embedding_bytes = pickle.dumps(question_embedding)
            contexts_json = json.dumps(contexts)

            cursor.execute('''
                INSERT OR REPLACE INTO context_cache
                (question_hash, question, contexts, embedding, access_count)
                VALUES (?, ?, ?, ?, 1)
            ''', (question_hash, question, contexts_json, embedding_bytes))

            conn.commit()
            conn.close()
            logging.info(f"‚úÖ Cached contexts for question: {question[:50]}...")
        except Exception as e:
            logging.error(f"‚ùå Failed to cache contexts: {str(e)}")

class ImprovedStandardRAG:
    """Improved Standard RAG with memory and fallback"""

    def __init__(self, cache_size: int = 50):
        self.cache_size = cache_size
        self.question_cache = {}  # Simple in-memory cache
        self.fallback_responses = [
            "‡∏Ç‡∏≠‡πÇ‡∏ó‡∏©‡∏Ñ‡∏£‡∏±‡∏ö ‡∏â‡∏±‡∏ô‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡πÉ‡∏ô‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ó‡∏µ‡πà‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ß‡πâ",
            "‡∏ï‡∏≤‡∏°‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏ô‡∏µ‡πâ‡∏Ñ‡∏£‡∏±‡∏ö",
            "‡∏â‡∏±‡∏ô‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏ô‡∏µ‡πâ‡∏à‡∏≤‡∏Å‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡πÑ‡∏î‡πâ‡∏Ñ‡∏£‡∏±‡∏ö"
        ]

    def get_similar_questions(self, question: str, limit: int = 3) -> list:
        """Get similar questions from database (improved memory)"""
        try:
            question_embedding = embed_text(question)
            conn = sqlite3.connect(FEEDBACK_DB_PATH)
            cursor = conn.cursor()

            # Get recent questions with embeddings
            cursor.execute('''
                SELECT question, answer, question_embedding, relevance_score
                FROM session_memory
                WHERE question_embedding IS NOT NULL
                ORDER BY timestamp DESC
                LIMIT 50
            ''')

            results = cursor.fetchall()
            similar_questions = []

            for row in results:
                stored_embedding = pickle.loads(row[2])

                # Proper cosine similarity calculation
                similarity = self._cosine_similarity(question_embedding, stored_embedding)

                if similarity > 0.7:  # Threshold for similarity
                    similar_questions.append({
                        'question': row[0],
                        'answer': row[1],
                        'similarity': similarity,
                        'relevance_score': row[3]
                    })

            # Sort by similarity and return top results
            similar_questions.sort(key=lambda x: x['similarity'], reverse=True)
            conn.close()

            return similar_questions[:limit]
        except Exception as e:
            logging.error(f"‚ùå Failed to get similar questions: {str(e)}")
            return []

    def _cosine_similarity(self, a, b):
        """Calculate proper cosine similarity"""
        import numpy as np
        dot_product = np.dot(a, b)
        norm_a = np.linalg.norm(a)
        norm_b = np.linalg.norm(b)

        if norm_a == 0 or norm_b == 0:
            return 0

        return dot_product / (norm_a * norm_b)

    def get_fallback_answer(self, question: str) -> str:
        """Get fallback answer when no context found"""
        try:
            # Try to find similar questions first
            similar = self.get_similar_questions(question)
            if similar:
                return f"‡∏û‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏Ñ‡∏•‡πâ‡∏≤‡∏¢‡∏Å‡∏±‡∏ô: {similar[0]['question']}\n‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö: {similar[0]['answer']}"

            # Return generic fallback
            import random
            return random.choice(self.fallback_responses)
        except Exception as e:
            logging.error(f"‚ùå Failed to get fallback answer: {str(e)}")
            return "‡∏Ç‡∏≠‡πÇ‡∏ó‡∏©‡∏Ñ‡∏£‡∏±‡∏ö ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•"

    def save_to_memory(self, session_id: str, question: str, answer: str, contexts: list):
        """Save conversation to memory database"""
        try:
            question_embedding = embed_text(question)
            embedding_bytes = pickle.dumps(question_embedding)
            contexts_json = json.dumps(contexts[:3])  # Save top 3 contexts

            conn = sqlite3.connect(FEEDBACK_DB_PATH)
            cursor = conn.cursor()

            cursor.execute('''
                INSERT INTO session_memory
                (session_id, question, answer, question_embedding, contexts, relevance_score)
                VALUES (?, ?, ?, ?, ?, ?)
            ''', (session_id, question, answer, embedding_bytes, contexts_json, 1.0))

            conn.commit()
            conn.close()
            logging.info(f"‚úÖ Saved to memory: {question[:50]}...")
        except Exception as e:
            logging.error(f"‚ùå Failed to save to memory: {str(e)}")

class ImprovedEnhancedRAG:
    """Improved Enhanced RAG with proper similarity and persistence"""

    def __init__(self):
        self.session_memory = deque(maxlen=MEMORY_WINDOW_SIZE)
        self.conversation_history = []
        self.current_session_id = self._generate_session_id()
        self.performance_monitor = PerformanceMonitor()

    def _generate_session_id(self) -> str:
        """Generate unique session ID with better collision resistance"""
        timestamp = datetime.now().isoformat()
        random_str = str(time.time())[-6:]  # Add random component
        combined = f"{timestamp}_{random_str}"
        session_hash = hashlib.sha256(combined.encode()).hexdigest()[:12]
        return f"enhanced_session_{session_hash}"

    def get_relevant_memory(self, current_question: str, threshold: float = 0.75) -> list:
        """Get relevant memory with proper cosine similarity"""
        if not ENABLE_SESSION_MEMORY:
            return []

        try:
            start_time = time.time()
            current_embedding = embed_text(current_question)
            relevant_memories = []

            # Try database first
            db_memories = self._get_database_memories(current_embedding, threshold)
            if db_memories:
                relevant_memories.extend(db_memories)

            # Add in-memory results if needed
            if len(relevant_memories) < 3:
                memory_memories = self._get_in_memory_memories(current_embedding, threshold)
                relevant_memories.extend(memory_memories)

            # Sort by similarity and return top results
            relevant_memories.sort(key=lambda x: x['similarity'], reverse=True)
            result = relevant_memories[:5]  # Return top 5 instead of 3

            response_time = (time.time() - start_time) * 1000
            self.performance_monitor.log_performance(
                self.current_session_id, "enhanced", current_question,
                response_time, 0, True, True
            )

            return result
        except Exception as e:
            logging.error(f"‚ùå Failed to get relevant memory: {str(e)}")
            return []

    def _get_database_memories(self, current_embedding, threshold: float) -> list:
        """Get memories from database with proper similarity"""
        try:
            conn = sqlite3.connect(FEEDBACK_DB_PATH)
            cursor = conn.cursor()

            # Get recent memories from database
            cursor.execute('''
                SELECT question, answer, question_embedding, contexts
                FROM session_memory
                WHERE question_embedding IS NOT NULL
                ORDER BY timestamp DESC
                LIMIT 100
            ''')

            results = cursor.fetchall()
            db_memories = []

            for row in results:
                stored_embedding = pickle.loads(row[2])

                # Proper cosine similarity
                similarity = self._cosine_similarity(current_embedding, stored_embedding)

                if similarity > threshold:
                    contexts = json.loads(row[3]) if row[3] else []
                    db_memories.append({
                        'question': row[0],
                        'answer': row[1],
                        'similarity': similarity,
                        'contexts': contexts,
                        'source': 'database'
                    })

            conn.close()
            return db_memories
        except Exception as e:
            logging.error(f"‚ùå Failed to get database memories: {str(e)}")
            return []

    def _get_in_memory_memories(self, current_embedding, threshold: float) -> list:
        """Get memories from in-memory deque"""
        memories = []

        for memory_entry in self.session_memory:
            memory_question = memory_entry["question"]
            memory_embedding = embed_text(memory_question)

            # Proper cosine similarity
            similarity = self._cosine_similarity(current_embedding, memory_embedding)

            if similarity > threshold:
                memories.append({
                    'question': memory_question,
                    'answer': memory_entry["answer"],
                    'similarity': similarity,
                    'contexts': memory_entry["contexts"],
                    'source': 'memory'
                })

        return memories

    def _cosine_similarity(self, a, b):
        """Calculate proper cosine similarity"""
        import numpy as np
        dot_product = np.dot(a, b)
        norm_a = np.linalg.norm(a)
        norm_b = np.linalg.norm(b)

        if norm_a == 0 or norm_b == 0:
            return 0

        return dot_product / (norm_a * norm_b)

    def add_to_memory(self, session_id: str, question: str, answer: str, contexts: list):
        """Add conversation to both memory and database"""
        memory_entry = {
            "session_id": session_id,
            "timestamp": datetime.now().isoformat(),
            "question": question,
            "answer": answer,
            "contexts": contexts[:3]
        }

        self.session_memory.append(memory_entry)

        # Also save to database for persistence
        self._save_to_database(session_id, question, answer, contexts)

        # Add to conversation history
        self.conversation_history.extend([
            {"role": "user", "content": question, "timestamp": memory_entry["timestamp"]},
            {"role": "assistant", "content": answer, "timestamp": memory_entry["timestamp"]}
        ])

    def _save_to_database(self, session_id: str, question: str, answer: str, contexts: list):
        """Save to database for persistence"""
        try:
            question_embedding = embed_text(question)
            embedding_bytes = pickle.dumps(question_embedding)
            contexts_json = json.dumps(contexts[:3])

            conn = sqlite3.connect(FEEDBACK_DB_PATH)
            cursor = conn.cursor()

            cursor.execute('''
                INSERT INTO session_memory
                (session_id, question, answer, question_embedding, contexts, relevance_score)
                VALUES (?, ?, ?, ?, ?, ?)
            ''', (session_id, question, answer, embedding_bytes, contexts_json, 1.0))

            conn.commit()
            conn.close()
            logging.info(f"‚úÖ Enhanced RAG: Saved to database - {question[:50]}...")
        except Exception as e:
            logging.error(f"‚ùå Enhanced RAG: Failed to save to database - {str(e)}")

class RAGManager:
    """Main RAG Manager that handles both standard and enhanced modes"""

    def __init__(self):
        self.standard_rag = ImprovedStandardRAG()
        self.enhanced_rag = ImprovedEnhancedRAG()
        self.context_cache = ContextCache()
        self.performance_monitor = PerformanceMonitor()
        self.tag_retrieval = TagBasedRetrieval()
        self.current_session_id = self._generate_session_id()

    def _generate_session_id(self) -> str:
        """Generate session ID"""
        timestamp = datetime.now().isoformat()
        session_hash = hashlib.md5(timestamp.encode()).hexdigest()[:8]
        return f"rag_session_{session_hash}"

    def query(self, question: str, rag_mode: str = "enhanced", chat_llm: str = "gemma3:latest",
              show_source: bool = False, formal_style: bool = False):
        """Main query function that handles both modes"""
        start_time = time.time()

        try:
            # Check cache first
            cached_contexts = self.context_cache.get_cached_contexts(question)
            if cached_contexts:
                logging.info(f"‚úÖ Cache hit for question: {question[:50]}...")
                # Use cached contexts but still process with LLM
                contexts = cached_contexts
                memory_hit = True
            else:
                memory_hit = False
                contexts = self._retrieve_contexts(question)

                # Cache the contexts for future use
                if contexts:
                    question_embedding = embed_text(question)
                    self.context_cache.cache_contexts(question, contexts, question_embedding)

            # Get relevant memories for enhanced mode
            relevant_memories = []
            if rag_mode == "enhanced":
                relevant_memories = self.enhanced_rag.get_relevant_memory(question)
                logging.info(f"Found {len(relevant_memories)} relevant memories")

            # Generate response
            response_generator = self._generate_response(
                question, contexts, relevant_memories, rag_mode,
                chat_llm, show_source, formal_style
            )

            # Save to memory
            if rag_mode == "enhanced":
                self.enhanced_rag.add_to_memory(
                    self.current_session_id, question, "", contexts
                )
            else:
                self.standard_rag.save_to_memory(
                    self.current_session_id, question, "", contexts
                )

            # Log performance
            response_time = (time.time() - start_time) * 1000
            self.performance_monitor.log_performance(
                self.current_session_id, rag_mode, question,
                response_time, len(contexts), memory_hit, True
            )

            return response_generator

        except Exception as e:
            error_msg = f"‚ùå RAG Query failed: {str(e)}"
            logging.error(error_msg)

            # Log performance error
            response_time = (time.time() - start_time) * 1000
            self.performance_monitor.log_performance(
                self.current_session_id, rag_mode, question,
                response_time, 0, memory_hit, False, str(e)
            )

            # Return fallback response
            if rag_mode == "standard":
                fallback = self.standard_rag.get_fallback_answer(question)
            else:
                fallback = "‡∏Ç‡∏≠‡πÇ‡∏ó‡∏©‡∏Ñ‡∏£‡∏±‡∏ö ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏£‡∏∞‡∏ö‡∏ö Enhanced RAG ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏•‡∏≠‡∏á‡πÉ‡∏´‡∏°‡πà"

            def fallback_generator():
                yield fallback
                return

            return fallback_generator()

    def _retrieve_contexts(self, question: str) -> list:
        """Retrieve contexts from ChromaDB with tag-based enhancement"""
        try:
            # Step 1: Extract tags from question
            question_tags, tag_analysis = self.tag_retrieval.tag_question_and_enhance_search(question)

            # Step 2: Get base contexts from ChromaDB
            question_embedding = embed_text(question)
            max_result = determine_optimal_results(question)

            results = collection.query(
                query_embeddings=[question_embedding.tolist()],
                n_results=max_result
            )

            # Step 3: Filter relevant contexts
            base_contexts = []
            for doc, meta in zip(results["documents"][0], results["metadatas"][0]):
                base_contexts.append({'text': doc, 'metadata': meta})

            filtered_contexts = filter_relevant_contexts(
                question, results["documents"][0], results["metadatas"][0], min_relevance=0.05
            )

            if len(filtered_contexts) == 0:
                logging.warning("No contexts passed relevance filter, using all retrieved contexts")
                filtered_contexts = base_contexts

            # Step 4: Apply tag-based weighting if tags found
            if question_tags:
                final_contexts = self.tag_retrieval.get_tag_weighted_contexts(
                    question, filtered_contexts, question_tags
                )

                # Add tag info to context metadata
                for ctx in final_contexts:
                    if 'tag_relevance_score' in ctx:
                        ctx['metadata']['tag_relevance_score'] = ctx['tag_relevance_score']
                        ctx['metadata']['matching_tags'] = ctx.get('matching_tags', [])

                logging.info(f"üéØ Applied tag-based ranking: {len(final_contexts)} contexts with tags {question_tags}")
                return final_contexts
            else:
                logging.info("üìù No tags found, using standard retrieval")
                return filtered_contexts

        except Exception as e:
            logging.error(f"‚ùå Failed to retrieve contexts: {str(e)}")
            return []

    def _generate_response(self, question: str, contexts: list, relevant_memories: list,
                          rag_mode: str, chat_llm: str, show_source: bool, formal_style: bool):
        """Generate response using appropriate RAG mode"""
        try:
            if rag_mode == "enhanced":
                # Use Enhanced RAG logic
                return self._enhanced_response_generator(
                    question, contexts, relevant_memories, chat_llm, show_source, formal_style
                )
            else:
                # Use Standard RAG logic
                return self._standard_response_generator(
                    question, contexts, chat_llm, show_source, formal_style
                )
        except Exception as e:
            logging.error(f"‚ùå Failed to generate response: {str(e)}")
            def error_generator():
                yield "‡∏Ç‡∏≠‡πÇ‡∏ó‡∏©‡∏Ñ‡∏£‡∏±‡∏ö ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö"
                return
            return error_generator()

    def _standard_response_generator(self, question: str, contexts: list, chat_llm: str,
                                   show_source: bool, formal_style: bool):
        """Standard RAG response generator"""
        if not contexts:
            # Use fallback mechanism
            fallback = self.standard_rag.get_fallback_answer(question)
            def fallback_gen():
                yield fallback
                return
            return fallback_gen()

        # Build standard prompt
        prompt = self._build_standard_prompt(question, contexts, show_source, formal_style)

        # Generate streaming response
        return chat_with_model_streaming(chat_llm, prompt, [])

    def _enhanced_response_generator(self, question: str, contexts: list, relevant_memories: list,
                                   chat_llm: str, show_source: bool, formal_style: bool):
        """Enhanced RAG response generator with memory"""
        # Use existing enhanced RAG logic but with improvements
        return query_rag(question, chat_llm, show_source, formal_style)

    def _build_standard_prompt(self, question: str, contexts: list, show_source: bool, formal_style: bool) -> str:
        """Build standard RAG prompt"""
        style_instruction = "‡∏ï‡∏≠‡∏ö‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏£" if formal_style else "‡∏ï‡∏≠‡∏ö‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏Å‡∏±‡∏ô‡πÄ‡∏≠‡∏á"

        context_text = "\n\n".join([f"‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• {i+1}: {ctx['text']}" for i, ctx in enumerate(contexts[:5])])

        return f"""‡∏Ñ‡∏∏‡∏ì‡πÄ‡∏õ‡πá‡∏ô‡∏ú‡∏π‡πâ‡∏ä‡πà‡∏ß‡∏¢‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏π‡πâ‡∏î‡πâ‡∏≤‡∏ô‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ó‡∏µ‡πà‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ß‡πâ

**‡πÅ‡∏ô‡∏ß‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡∏ï‡∏≠‡∏ö:**
- {style_instruction}
- ‡πÉ‡∏´‡πâ‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏ó‡∏µ‡πà‡∏™‡∏≠‡∏î‡∏Ñ‡∏•‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏ô‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡πÄ‡∏õ‡πá‡∏ô‡∏´‡∏•‡∏±‡∏Å
- ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á ‡πÉ‡∏´‡πâ‡∏ö‡∏≠‡∏Å‡∏ß‡πà‡∏≤‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•

**‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á:**
{context_text}

**‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°:** {question}

**‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö:**"""

# Advanced Tag System with LLM Integration
import re
from typing import List, Dict, Tuple

class LLMTagger:
    """LLM-powered tag suggestion and analysis"""

    def __init__(self):
        # Predefined tag patterns for automatic detection
        self.tag_patterns = {
            '‡∏ä‡∏≥‡∏£‡∏∞': [r'‡∏ä‡∏≥‡∏£‡∏∞', r'‡∏à‡πà‡∏≤‡∏¢‡πÄ‡∏á‡∏¥‡∏ô', r'‡∏Å‡∏≤‡∏£‡∏à‡πà‡∏≤‡∏¢', r'‡πÄ‡∏á‡∏¥‡∏ô', r'‡∏ö‡∏¥‡∏•', r'‡∏Ñ‡πà‡∏≤‡πÉ‡∏ä‡πâ‡∏à‡πà‡∏≤‡∏¢', r'‡∏Ñ‡πà‡∏≤‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£'],
            '‡∏õ‡∏±‡∏ç‡∏´‡∏≤': [r'‡∏õ‡∏±‡∏ç‡∏´‡∏≤', r'‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î', r'error', r'‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ', r'‡∏•‡πâ‡∏°‡πÄ‡∏´‡∏•‡∏ß', r'‡∏ö‡∏±‡∏Å', r'‡∏Ç‡∏±‡∏î‡∏Ç‡πâ‡∏≠‡∏á'],
            '‡∏™‡∏≠‡∏ö‡∏ñ‡∏≤‡∏°': [r'‡∏™‡∏≠‡∏ö‡∏ñ‡∏≤‡∏°', r'‡∏ñ‡∏≤‡∏°', r'‡∏≠‡∏¢‡∏≤‡∏Å‡∏£‡∏π‡πâ', r'‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏ó‡∏£‡∏≤‡∏ö', r'‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•', r'‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î'],
            '‡πÄ‡∏ó‡∏Ñ‡∏ô‡∏¥‡∏Ñ': [r'‡∏ß‡∏¥‡∏ò‡∏µ', r'‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô', r'‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤', r'configure', r'setup', r'‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô'],
            '‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç': [r'‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç', r'‡πÄ‡∏£‡πà‡∏á‡∏î‡πà‡∏ß‡∏ô', r'‡∏â‡∏∏‡∏Å‡πÄ‡∏â‡∏¥‡∏ô', r'‡∏î‡πà‡∏ß‡∏ô', r'‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô', r'‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏°‡∏≤‡∏Å'],
            '‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£': [r'‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£', r'‡πÑ‡∏ü‡∏•‡πå', r'PDF', r'doc', r'‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•', r'‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤'],
            '‡∏£‡∏∞‡∏ö‡∏ö': [r'‡∏£‡∏∞‡∏ö‡∏ö', r'system', r'‡πÇ‡∏õ‡∏£‡πÅ‡∏Å‡∏£‡∏°', r'application', r'‡πÅ‡∏≠‡∏õ', r'‡∏ã‡∏≠‡∏ü‡∏ï‡πå‡πÅ‡∏ß‡∏£‡πå'],
            '‡∏ö‡∏±‡∏ç‡∏ä‡∏µ': [r'‡∏ö‡∏±‡∏ç‡∏ä‡∏µ', r'account', r'user', r'‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ', r'login', r'‡∏£‡∏´‡∏±‡∏™‡∏ú‡πà‡∏≤‡∏ô']
        }

    def extract_tags_from_text(self, text: str) -> List[str]:
        """Extract tags from text using pattern matching"""
        found_tags = []
        text_lower = text.lower()

        for tag_name, patterns in self.tag_patterns.items():
            for pattern in patterns:
                if re.search(pattern, text_lower, re.IGNORECASE):
                    found_tags.append(tag_name)
                    break

        return list(set(found_tags))  # Remove duplicates

    def suggest_tags_with_llm(self, text: str, context: str = "") -> List[str]:
        """Use LLM to suggest relevant tags"""
        try:
            # Create a simple prompt for tag suggestion
            prompt = f"""‡∏Ñ‡∏∏‡∏ì‡πÄ‡∏õ‡πá‡∏ô‡∏ú‡∏π‡πâ‡πÄ‡∏ä‡∏µ‡πà‡∏¢‡∏ß‡∏ä‡∏≤‡∏ç‡∏î‡πâ‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏´‡∏°‡∏ß‡∏î‡∏´‡∏°‡∏π‡πà‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤

‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ï‡πà‡∏≠‡πÑ‡∏õ‡∏ô‡∏µ‡πâ:
"{text}"

‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥ tags ‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏° (‡πÑ‡∏°‡πà‡πÄ‡∏Å‡∏¥‡∏ô 5 tags):
- ‡πÉ‡∏ä‡πâ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢
- ‡πÉ‡∏ä‡πâ‡∏Ñ‡∏≥‡∏™‡∏±‡πâ‡∏ô‡πÜ ‡∏ó‡∏µ‡πà‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏á‡πà‡∏≤‡∏¢
- ‡∏û‡∏¥‡∏à‡∏≤‡∏£‡∏ì‡∏≤‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏°‡∏≤‡∏¢‡πÇ‡∏î‡∏¢‡∏£‡∏ß‡∏°

‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏à‡∏≤‡∏Å tags ‡πÄ‡∏´‡∏•‡πà‡∏≤‡∏ô‡∏µ‡πâ: {', '.join(self.tag_patterns.keys())}

‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ tags ‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏° ‡∏ï‡∏≠‡∏ö‡∏ß‡πà‡∏≤ "‡πÑ‡∏°‡πà‡∏°‡∏µ"

Tags ‡∏ó‡∏µ‡πà‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥:"""

            # Use existing model for tag suggestion
            response = list(chat_with_model_streaming("gemma3:latest", prompt, []))

            if response:
                suggested_text = "".join(response).strip()

                # Extract tags from response
                suggested_tags = []
                for tag_name in self.tag_patterns.keys():
                    if tag_name in suggested_text:
                        suggested_tags.append(tag_name)

                return suggested_tags

            return []
        except Exception as e:
            logging.error(f"‚ùå Failed to suggest tags with LLM: {str(e)}")
            return []

class TagBasedRetrieval:
    """Enhanced retrieval system using tags"""

    def __init__(self):
        self.llm_tagger = LLMTagger()

    def tag_question_and_enhance_search(self, question: str) -> Tuple[List[str], Dict]:
        """Tag question and enhance search with tag-based filtering"""
        try:
            # Extract tags using both methods
            pattern_tags = self.llm_tagger.extract_tags_from_text(question)
            llm_tags = self.llm_tagger.suggest_tags_with_llm(question)

            # Combine and prioritize
            all_tags = list(set(pattern_tags + llm_tags))

            tag_analysis = {
                'pattern_tags': pattern_tags,
                'llm_tags': llm_tags,
                'all_tags': all_tags,
                'tag_count': len(all_tags)
            }

            logging.info(f"üè∑Ô∏è Question tags: {all_tags}")
            return all_tags, tag_analysis

        except Exception as e:
            logging.error(f"‚ùå Failed to tag question: {str(e)}")
            return [], {}

    def get_tag_weighted_contexts(self, question: str, base_contexts: List[Dict], question_tags: List[str]) -> List[Dict]:
        """Weight contexts based on tag relevance"""
        try:
            if not question_tags or not base_contexts:
                return base_contexts

            weighted_contexts = []

            for ctx in base_contexts:
                context_text = ctx.get('text', '')
                context_metadata = ctx.get('metadata', {})

                # Calculate tag relevance score
                tag_score = self._calculate_tag_relevance(context_text, question_tags)

                # Get document ID for additional tag lookup
                doc_id = context_metadata.get('id', '')
                document_tags = self._get_document_tags(doc_id)

                # Additional score if document has matching tags
                document_tag_score = len(set(question_tags) & set(document_tags))

                # Combined score
                total_score = tag_score + (document_tag_score * 0.5)

                # Create weighted context
                weighted_ctx = ctx.copy()
                weighted_ctx['tag_relevance_score'] = total_score
                weighted_ctx['matching_tags'] = list(set(question_tags) & set(document_tags))
                weighted_ctx['document_tags'] = document_tags

                weighted_contexts.append(weighted_ctx)

            # Sort by tag relevance
            weighted_contexts.sort(key=lambda x: x.get('tag_relevance_score', 0), reverse=True)

            logging.info(f"üéØ Tag-weighted contexts: {len(weighted_contexts)} with relevance scores")
            return weighted_contexts

        except Exception as e:
            logging.error(f"‚ùå Failed to weight contexts by tags: {str(e)}")
            return base_contexts

    def _calculate_tag_relevance(self, text: str, tags: List[str]) -> float:
        """Calculate how relevant text is to given tags"""
        score = 0.0
        text_lower = text.lower()

        for tag in tags:
            if tag in self.llm_tagger.tag_patterns:
                patterns = self.llm_tagger.tag_patterns[tag]
                tag_matches = sum(1 for pattern in patterns
                               if re.search(pattern, text_lower, re.IGNORECASE))
                score += tag_matches

        return score

    def _get_document_tags(self, document_id: str) -> List[str]:
        """Get tags associated with a document"""
        try:
            if not document_id:
                return []

            conn = sqlite3.connect(FEEDBACK_DB_PATH)
            cursor = conn.cursor()

            cursor.execute('''
                SELECT t.name FROM document_tags dt
                JOIN tags t ON dt.tag_id = t.id
                WHERE dt.document_id = ?
            ''', (document_id,))

            tags = [row[0] for row in cursor.fetchall()]
            conn.close()

            return tags
        except Exception as e:
            logging.error(f"‚ùå Failed to get document tags: {str(e)}")
            return []

    def auto_tag_document(self, document_id: str, content: str) -> List[str]:
        """Automatically tag a document based on its content"""
        try:
            suggested_tags = self.llm_tagger.suggest_tags_with_llm(content)
            pattern_tags = self.llm_tagger.extract_tags_from_text(content)

            all_tags = list(set(suggested_tags + pattern_tags))

            # Save tags to database
            for tag_name in all_tags:
                self._tag_document(document_id, tag_name)

            logging.info(f"üè∑Ô∏è Auto-tagged document {document_id} with tags: {all_tags}")
            return all_tags

        except Exception as e:
            logging.error(f"‚ùå Failed to auto-tag document: {str(e)}")
            return []

    def _tag_document(self, document_id: str, tag_name: str):
        """Tag a document (create tag if needed)"""
        try:
            conn = sqlite3.connect(FEEDBACK_DB_PATH)
            cursor = conn.cursor()

            # Get or create tag
            cursor.execute('SELECT id FROM tags WHERE name = ?', (tag_name,))
            result = cursor.fetchone()

            if result:
                tag_id = result[0]
            else:
                # Create new tag with default color
                cursor.execute('''
                    INSERT INTO tags (name, color, description) VALUES (?, ?, ?)
                ''', (tag_name, '#007bff', f'Auto-generated tag for {tag_name}'))
                tag_id = cursor.lastrowid

            # Tag the document
            cursor.execute('''
                INSERT OR IGNORE INTO document_tags (document_id, tag_id) VALUES (?, ?)
            ''', (document_id, tag_id))

            conn.commit()
            conn.close()

        except Exception as e:
            logging.error(f"‚ùå Failed to tag document: {str(e)}")

def get_feedback_stats():
    """‡∏î‡∏∂‡∏á‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥ feedback"""
    try:
        conn = sqlite3.connect(FEEDBACK_DB_PATH)
        cursor = conn.cursor()

        # ‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ
        cursor.execute("SELECT COUNT(*) FROM feedback")
        total = cursor.fetchone()[0]

        cursor.execute("SELECT COUNT(*) FROM feedback WHERE feedback_type = 'good'")
        good = cursor.fetchone()[0]

        cursor.execute("SELECT COUNT(*) FROM feedback WHERE feedback_type = 'bad'")
        bad = cursor.fetchone()[0]

        # Feedback ‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î
        cursor.execute('''
            SELECT question, answer, feedback_type, timestamp, user_comment
            FROM feedback
            ORDER BY timestamp DESC
            LIMIT 10
        ''')
        recent_feedback = cursor.fetchall()

        conn.close()

        return {
            "total": total,
            "good": good,
            "bad": bad,
            "accuracy": (good / total * 100) if total > 0 else 0,
            "recent": recent_feedback
        }
    except Exception as e:
        logging.error(f"‚ùå Failed to get feedback stats: {str(e)}")
        return {"total": 0, "good": 0, "bad": 0, "accuracy": 0, "recent": []}


def delete_feedback(feedback_id: int):
    """‡∏•‡∏ö feedback ‡∏ï‡∏≤‡∏° ID"""
    try:
        conn = sqlite3.connect(FEEDBACK_DB_PATH)
        cursor = conn.cursor()

        cursor.execute("DELETE FROM feedback WHERE id = ?", (feedback_id,))
        affected = cursor.rowcount

        conn.commit()
        conn.close()

        if affected > 0:
            logging.info(f"‚úÖ Deleted feedback ID: {feedback_id}")
            return True
        else:
            logging.warning(f"‚ö†Ô∏è Feedback ID {feedback_id} not found")
            return False
    except Exception as e:
        logging.error(f"‚ùå Failed to delete feedback: {str(e)}")
        return False


def export_feedback():
    """‡∏™‡πà‡∏á‡∏≠‡∏≠‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• feedback ‡πÄ‡∏õ‡πá‡∏ô CSV"""
    try:
        conn = sqlite3.connect(FEEDBACK_DB_PATH)
        cursor = conn.cursor()

        cursor.execute('''
            SELECT id, question, answer, feedback_type, user_comment, corrected_answer,
                   timestamp, model_used, sources
            FROM feedback
            ORDER BY timestamp DESC
        ''')

        rows = cursor.fetchall()
        conn.close()

        # ‡∏™‡∏£‡πâ‡∏≤‡∏á CSV string
        csv_data = []
        csv_data.append("ID,Question,Answer,Feedback Type,User Comment,Corrected Answer,Timestamp,Model,Sources")

        for row in rows:
            # Escape quotes in text fields
            q1 = str(row[1]).replace('"', '""') if row[1] else ""
            q2 = str(row[2]).replace('"', '""') if row[2] else ""
            q4 = str(row[4]).replace('"', '""') if row[4] else ""
            q5 = str(row[5]).replace('"', '""') if row[5] else ""
            q8 = str(row[8]).replace('"', '""') if row[8] else ""

            csv_row = [
                str(row[0]),
                f'"{q1}"',  # Question
                f'"{q2}"',  # Answer
                row[3],      # Feedback type
                f'"{q4}"',  # User comment
                f'"{q5}"',  # Corrected answer
                row[6],      # Timestamp
                row[7],      # Model
                f'"{q8}"'   # Sources
            ]
            csv_data.append(",".join(csv_row))

        return "\n".join(csv_data)
    except Exception as e:
        logging.error(f"‚ùå Failed to export feedback: {str(e)}")
        return None


# ==================== END FEEDBACK FUNCTIONS ====================


def chatbot_interface(history: List[Dict], llm_model: str, ai_provider: str = "ollama", show_source: bool = False, formal_style: bool = False,
                       send_to_discord: bool = False, send_to_line: bool = False, send_to_facebook: bool = False,
                       line_user_id: str = "", fb_user_id: str = "", use_graph_reasoning: bool = False,
                       reasoning_mode: str = "hybrid", multi_hop_enabled: bool = False, hop_count: int = 2):
    """
    ‡∏≠‡∏¥‡∏ô‡πÄ‡∏ó‡∏≠‡∏£‡πå‡πÄ‡∏ü‡∏ã‡πÅ‡∏ä‡∏ó‡∏ö‡∏≠‡∏ó‡πÅ‡∏ö‡∏ö streaming with LightRAG support
    """
    global current_model, current_provider

    # Update global state so LINE/Discord/FB bots use the same model
    current_model = llm_model
    current_provider = ai_provider

    print(f"DEBUG: chatbot_interface received - Provider: {ai_provider}, Model: {llm_model}")  # Debug
    user_message = history[-1]["content"]

    # ‡∏™‡πà‡∏á‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö feedback
    current_q = user_message

    # Choose query method based on LightRAG settings
    if use_graph_reasoning:
        if multi_hop_enabled:
            # Use multi-hop reasoning
            logging.info(f"üîÑ Using Multi-Hop LightRAG (hops: {hop_count}) for query: {user_message}")
            stream = query_rag_with_multi_hop(
                user_message,
                chat_llm=llm_model,
                ai_provider=ai_provider,
                show_source=show_source,
                formal_style=formal_style,
                hop_count=hop_count
            )
        else:
            # Use standard graph reasoning
            logging.info(f"üß† Using LightRAG Graph Reasoning (mode: {reasoning_mode}) for query: {user_message}")
            import asyncio

            # Run async query in sync context
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            try:
                result = loop.run_until_complete(
                    query_rag_with_lightrag(
                        user_message,
                        chat_llm=llm_model,
                        ai_provider=ai_provider,
                        show_source=show_source,
                        formal_style=formal_style,
                        use_graph_reasoning=True,
                        reasoning_mode=reasoning_mode
                    )
                )
                # Convert result to stream format
                if isinstance(result, str):
                    # If result is already a string, create a simple stream
                    stream = ({"message": {"content": result}} for _ in range(1))
                else:
                    # If result is a stream generator, use it directly
                    stream = result
            finally:
                loop.close()
    else:
        # Use standard RAG
        stream = query_rag(user_message, chat_llm=llm_model, ai_provider=ai_provider, show_source=show_source, formal_style=formal_style)

    history.append({"role": "assistant", "content": ""})
    full_answer=""
    """
    ‡∏™‡πà‡∏ß‡∏ô‡∏Ç‡∏≠‡∏á‡∏Å‡∏≤‡∏£ ‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°
    """
    for chunk in stream:
        content = chunk["message"]["content"]
        full_answer += content
        history[-1]["content"] += content
        #logging.info(f"content: {content}")
        yield history, current_q, full_answer, json.dumps([]) if show_source else ""

    """
    ‡∏™‡πà‡∏ß‡∏ô‡∏Ç‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏î‡∏∂‡∏á‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û ‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏°‡∏≤‡πÅ‡∏™‡∏î‡∏á ‡πÇ‡∏î‡∏¢‡∏î‡∏∂‡∏á‡∏à‡∏≤‡∏Å ‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏î‡πâ‡∏≤‡∏ô‡∏ö‡∏ô
    """

    # ‡πÉ‡∏ä‡πâ regex ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏î‡∏∂‡∏á‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô [‡∏†‡∏≤‡∏û: ...]
    print(full_answer)
    pattern1 = r"\[(?:‡∏†‡∏≤‡∏û:\s*)?(pic_\w+[-_]?\w*\.(?:jpe?g|png))\]"
    pattern2 = r"(pic_\w+[-_]?\w*\.(?:jpe?g|png))"
    # ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏ó‡∏∏‡∏Å‡∏£‡∏π‡∏õ ‡πÅ‡∏ö‡∏ö‡∏ó‡∏µ‡πà‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö ‡∏™‡πà‡∏á‡πÄ‡∏Ç‡πâ‡∏≤‡∏°‡∏≤

    print("----------PPPP------------")
    image_list = re.findall(pattern1, full_answer)
    print(image_list)
    if (len(image_list)==0):
        image_list = re.findall(pattern2, full_answer)
    print("----------xxxx------------")
    # ‡∏î‡∏∂‡∏á‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏£‡∏π‡∏õ‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏ã‡πâ‡∏≥‡∏Å‡∏±‡∏ô
    image_list_uniq = list(dict.fromkeys(image_list))
    if image_list_uniq:
        history[-1]["content"] += "\n\n‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á:"
        yield history, current_q, full_answer, json.dumps([]) if show_source else ""

        # ‡∏î‡∏∂‡∏á‡∏£‡∏π‡∏õ‡∏°‡∏≤‡πÅ‡∏™‡∏î‡∏á
        for img in image_list_uniq:
            img_path = f"{TEMP_IMG}/{img}"
            logger.info(f"img_path: {img_path}")
            if os.path.exists(img_path):
                    image = Image.open(img_path)
                    buffered = io.BytesIO()
                    image.save(buffered, format="PNG")
                    image_response = f"{img} ![{img}](data:image/png;base64,{base64.b64encode(buffered.getvalue()).decode()})"
                    #‡∏™‡πà‡∏á‡∏£‡∏π‡∏õ‡πÑ‡∏õ‡∏ó‡∏µ‡πà Chat
                    history.append({"role": "assistant", "content": image_response })
                    yield history, current_q, full_answer, json.dumps([]) if show_source else ""

    # Learning from corrected answers
    try:
        similar_corrected = find_similar_corrected_answer(user_message, threshold=0.85)
        if similar_corrected:
            # ‡πÉ‡∏ä‡πâ‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡πÅ‡∏ó‡∏ô
            full_answer = similar_corrected['corrected_answer']
            logging.info(f"üéì Applied learned correction (similarity: {similar_corrected['similarity']:.2f}): {user_message[:50]}...")

            # ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô
            increment_corrected_answer_usage(similar_corrected['original_question'])

            # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏à‡πâ‡∏á‡∏ß‡πà‡∏≤‡πÉ‡∏ä‡πâ‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ
            full_answer += f"\n\nüí° *‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏ô‡∏µ‡πâ‡πÑ‡∏î‡πâ‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏à‡∏≤‡∏Å‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏à‡∏≤‡∏Å‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏°‡∏≤‡∏Å‡πà‡∏≠‡∏ô (‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏•‡πâ‡∏≤‡∏¢‡∏Å‡∏±‡∏ô: {similar_corrected['similarity']:.1%})*"
    except Exception as e:
        logging.warning(f"‚ö†Ô∏è Failed to apply learning from corrected answers: {str(e)}")

    # Store conversation in memory for Enhanced RAG
    if RAG_MODE == "enhanced":
        try:
            enhanced_rag.add_to_memory(user_message, full_answer, [])
            logging.info("Stored conversation in Enhanced RAG memory")
        except Exception as e:
            logging.error(f"Failed to store in Enhanced RAG memory: {str(e)}")

    # ‡∏™‡πà‡∏á‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡πÑ‡∏õ‡∏¢‡∏±‡∏á‡πÅ‡∏û‡∏•‡∏ï‡∏ü‡∏≠‡∏£‡πå‡∏°‡∏ó‡∏µ‡πà‡πÄ‡∏•‡∏∑‡∏≠‡∏Å
    try:
        # ‡∏™‡πà‡∏á‡πÑ‡∏õ‡∏¢‡∏±‡∏á Discord (‡∏ñ‡πâ‡∏≤‡πÄ‡∏•‡∏∑‡∏≠‡∏Å)
        if send_to_discord and DISCORD_ENABLED:
            send_to_discord_sync(user_message, full_answer)
            logging.info("‚úÖ ‡∏™‡πà‡∏á‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡πÑ‡∏õ‡∏¢‡∏±‡∏á Discord ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à")

        # ‡∏™‡πà‡∏á‡πÑ‡∏õ‡∏¢‡∏±‡∏á LINE OA (‡∏ñ‡πâ‡∏≤‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÅ‡∏•‡∏∞‡∏°‡∏µ user_id)
        if send_to_line and LINE_ENABLED and line_user_id and line_bot_api:
            try:
                # ‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö LINE
                line_answer = full_answer
                if len(line_answer) > 4900:
                    line_answer = line_answer[:4900] + "\n\n... (‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏ñ‡∏π‡∏Å‡∏ï‡∏±‡∏î‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡∏à‡∏≤‡∏Å‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß)"

                line_bot_api.push_message(
                    line_user_id,
                    TextSendMessage(text=f"‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°: {user_message}\n\n‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö:\n{line_answer}")
                )
                logging.info("‚úÖ ‡∏™‡πà‡∏á‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡πÑ‡∏õ‡∏¢‡∏±‡∏á LINE OA ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à")
            except Exception as e:
                logging.error(f"‚ùå ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏™‡πà‡∏á‡πÑ‡∏õ‡∏¢‡∏±‡∏á LINE OA: {str(e)}")

        # ‡∏™‡πà‡∏á‡πÑ‡∏õ‡∏¢‡∏±‡∏á Facebook Messenger (‡∏ñ‡πâ‡∏≤‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÅ‡∏•‡∏∞‡∏°‡∏µ user_id)
        if send_to_facebook and FB_ENABLED and fb_user_id:
            try:
                # ‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Facebook
                fb_answer = full_answer
                if len(fb_answer) > 1900:
                    fb_answer = fb_answer[:1900] + "\n\n... (‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏ñ‡∏π‡∏Å‡∏ï‡∏±‡∏î‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡∏à‡∏≤‡∏Å‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß)"

                send_facebook_message(
                    fb_user_id,
                    f"‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°: {user_message}\n\n‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö:\n{fb_answer}"
                )
                logging.info("‚úÖ ‡∏™‡πà‡∏á‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡πÑ‡∏õ‡∏¢‡∏±‡∏á Facebook Messenger ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à")
            except Exception as e:
                logging.error(f"‚ùå ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏™‡πà‡∏á‡πÑ‡∏õ‡∏¢‡∏±‡∏á Facebook Messenger: {str(e)}")

    except Exception as e:
        logging.error(f"‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡πà‡∏á‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡πÑ‡∏õ‡∏¢‡∏±‡∏á‡πÅ‡∏û‡∏•‡∏ï‡∏ü‡∏≠‡∏£‡πå‡∏°: {str(e)}")

    # Final yield with complete data
    yield history, current_q, full_answer, json.dumps([]) if show_source else ""

# Global LightRAG functions for UI access
def update_lightrag_status():
    """Get LightRAG system status for UI display"""
    try:
        if LIGHT_RAG_AVAILABLE:
            status = get_lightrag_system_status()
            if status.get("status") == "‚úÖ LightRAG Available":
                return f"""‚úÖ LightRAG ‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô
‚Ä¢ ChromaDB Records: {status.get('chroma_records', 'N/A')}
‚Ä¢ Graph Available: {'‚úÖ' if status.get('graph_available') else '‚ùå'}
‚Ä¢ Mode: Mock Implementation (‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏≠‡∏±‡∏õ‡πÄ‡∏Å‡∏£‡∏î‡πÄ‡∏°‡∏∑‡πà‡∏≠ API ‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå)"""
            else:
                return f"‚ö†Ô∏è LightRAG ‡∏°‡∏µ‡∏õ‡∏±‡∏ç‡∏´‡∏≤: {status.get('error', 'Unknown error')}"
        else:
            return "‚ùå LightRAG ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡πÑ‡∏î‡πâ (Package ‡πÑ‡∏°‡πà‡∏û‡∏£‡πâ‡∏≠‡∏°)"
    except Exception as e:
        return f"‚ùå ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ: {str(e)}"

def test_graph_reasoning_interface():
    """Test LightRAG functionality for UI"""
    try:
        import asyncio

        async def run_test():
            test_query = "‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏±‡∏°‡∏û‡∏±‡∏ô‡∏ò‡πå‡πÉ‡∏ô‡∏£‡∏∞‡∏ö‡∏ö"
            result = await query_with_graph_reasoning(test_query, mode="hybrid")

            if result.get("error"):
                return f"‚ùå ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏•‡πâ‡∏°‡πÄ‡∏´‡∏•‡∏ß: {result['error']}"

            return f"""‚úÖ ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à!
‚Ä¢ Query: {test_query}
‚Ä¢ Processing Time: {result.get('processing_time', 0):.2f}s
‚Ä¢ Response Length: {len(result.get('result', ''))} chars
‚Ä¢ Mock Mode: {'‡πÉ‡∏ä‡πà' if result.get('mock') else '‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà'}
‚Ä¢ Insights: {result.get('graph_insights', {})}"""

        # Run async function
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        try:
            result = loop.run_until_complete(run_test())
            return result, gr.update(visible=True)
        finally:
            loop.close()

    except Exception as e:
        return f"‚ùå ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏•‡πâ‡∏°‡πÄ‡∏´‡∏•‡∏ß: {str(e)}", gr.update(visible=True)

# Use main interface directly for now
# Authentication will be handled in a future update
def create_authenticated_interface():
    """Create interface with authentication check"""
    return demo

# Gradio interface

with gr.Blocks(
    css="""
    .upload-container {
        border: 2px dashed #e0e0e0;
        border-radius: 12px;
        padding: 20px;
        background: #fafafa;
        transition: all 0.3s ease;
    }
    .upload-container:hover {
        border-color: #4CAF50;
        background: #f5f5f5;
    }
    .drop-zone {
        min-height: 150px;
        display: flex;
        flex-direction: column;
        align-items: center;
        justify-content: center;
        background: white;
        border: 2px dashed #ccc;
        border-radius: 8px;
        cursor: pointer;
        transition: all 0.3s ease;
    }
    .drop-zone:hover {
        border-color: #4CAF50;
        background: #f9f9f9;
    }
    .options-container {
        background: #f8f9fa;
        border-radius: 8px;
        padding: 15px;
        border: 1px solid #e9ecef;
    }
    .checkbox-primary label {
        color: #495057;
        font-weight: 500;
    }
    .checkbox-secondary label {
        color: #6c757d;
        font-size: 0.9em;
    }
    .upload-button {
        background: linear-gradient(45deg, #4CAF50, #45a049);
        border: none;
        color: white;
        padding: 12px 24px;
        font-weight: 600;
        border-radius: 8px;
        transition: all 0.3s ease;
    }
    .upload-button:hover {
        background: linear-gradient(45deg, #45a049, #3d8b40);
        transform: translateY(-2px);
        box-shadow: 0 4px 8px rgba(0,0,0,0.1);
    }
    .status-output {
        background: #f8f9fa;
        border: 1px solid #dee2e6;
        border-radius: 6px;
        font-family: 'Courier New', monospace;
    }
    """
) as demo:
    logo="https://camo.githubusercontent.com/9433204b08afdc976c2e4f5a4ba0d81f8877b585cc11206e2969326d25c41657/68747470733a2f2f63646e2e6a7364656c6976722e6e65742f67682f6e61726f6e67736b6d6c2f68746d6c352d6c6561726e406c61746573742f6173736574732f696d67732f546c697665636f64654c6f676f2d3435302e77656270"
    gr.Markdown(f"""<h3 style='display: flex; align-items: center; gap: 15px; padding: 10px; margin: 0;'>
        <img alt='T-LIVE-CODE' src='{logo}' style='height: 100px;' >
        <span style='font-size: 1.5em;'>‡πÅ‡∏ä‡∏ó‡∏ö‡∏≠‡∏ó PDF: RAG</span></h3>""")

    with gr.Tab("üìö ‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£"):
        gr.Markdown("""
        ### üìÅ ‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡πÅ‡∏•‡∏∞‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏£‡∏∞‡∏ö‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
        ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡πÑ‡∏ü‡∏•‡πå **PDF, DOCX, TXT, MD** ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏£‡∏∞‡∏ö‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥
        """)

        # Create a more professional upload section with drag-and-drop
        with gr.Column():
            # Upload area with drag-and-drop support
            with gr.Group(elem_classes="upload-container"):
                files_input = gr.File(
                    label="‡∏•‡∏≤‡∏Å‡πÑ‡∏ü‡∏•‡πå‡∏°‡∏≤‡∏ß‡∏≤‡∏á‡∏ó‡∏µ‡πà‡∏ô‡∏µ‡πà‡∏´‡∏£‡∏∑‡∏≠‡∏Ñ‡∏•‡∏¥‡∏Å‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÑ‡∏ü‡∏•‡πå",
                    file_count="multiple",
                    file_types=[".pdf", ".txt", ".md", ".docx"],
                    height=150,
                    elem_classes="drop-zone",
                    show_label=True,
                    container=True,
                    scale=1
                )

            # File display area
            with gr.Row():
                with gr.Column(scale=3):
                    gr.Markdown("""
                    **üìã ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡πÄ‡∏•‡∏∑‡∏≠‡∏Å:**
                    - ‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÑ‡∏ü‡∏•‡πå
                    - ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö PDF, TXT, MD, DOCX
                    - ‡∏Ç‡∏ô‡∏≤‡∏î‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î: 100MB ‡∏ï‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå
                    """)

                    selected_files_info = gr.Textbox(
                        label="‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡πÄ‡∏•‡∏∑‡∏≠‡∏Å",
                        value="‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÑ‡∏ü‡∏•‡πå",
                        interactive=False,
                        lines=3,
                        max_lines=5
                    )

                with gr.Column(scale=2):
                    gr.Markdown("""
                    **üí° ‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥:**
                    ‚Ä¢ ‡∏•‡∏≤‡∏Å‡πÑ‡∏ü‡∏•‡πå‡∏°‡∏≤‡∏ß‡∏≤‡∏á‡∏ó‡∏µ‡πà‡πÄ‡∏•‡∏∑‡∏≠‡∏Å
                    ‚Ä¢ ‡∏´‡∏£‡∏∑‡∏≠‡∏Ñ‡∏•‡∏¥‡∏Å‡∏õ‡∏∏‡πà‡∏° "Browse Files" ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏•‡∏∑‡∏≠‡∏Å
                    ‚Ä¢ ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡πÑ‡∏ü‡∏•‡πå‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏Å‡∏±‡∏ô‡πÑ‡∏î‡πâ‡∏´‡∏•‡∏≤‡∏¢‡πÑ‡∏ü‡∏•‡πå
                    """)

        # Processing options with better styling
        with gr.Group(elem_classes="options-container"):
            with gr.Row():
                with gr.Column(scale=2):
                    clear_before_upload = gr.Checkbox(
                        label="üóëÔ∏è ‡∏•‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏Å‡πà‡∏≤‡∏Å‡πà‡∏≠‡∏ô‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î",
                        value=False,
                        info="‡∏à‡∏∞‡∏•‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏Å‡πà‡∏≠‡∏ô‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡πÉ‡∏´‡∏°‡πà",
                        elem_classes="checkbox-primary"
                    )

                with gr.Column(scale=2):
                    include_memory_checkbox = gr.Checkbox(
                        label="üß† ‡∏£‡∏ß‡∏° Enhanced RAG Memory",
                        value=(RAG_MODE == "enhanced"),
                        info="‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏£‡∏á‡∏à‡∏≥‡∏Å‡∏≤‡∏£‡∏™‡∏ô‡∏ó‡∏ô‡∏≤‡∏û‡∏£‡πâ‡∏≠‡∏°",
                        elem_classes="checkbox-secondary"
                    )

            # Action buttons with better styling
            with gr.Row():
                upload_button = gr.Button(
                    "üì§ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î",
                    variant="primary",
                    size="lg",
                    elem_classes="upload-button"
                )
                clear_button = gr.Button(
                    "üóëÔ∏è ‡∏•‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î",
                    variant="secondary",
                    size="lg"
                )

        # Status display
        with gr.Accordion("üìä ‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•", open=True):
            upload_output = gr.Textbox(
                label="‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå",
                lines=5,
                interactive=False,
                elem_classes="status-output"
            )

        # Connect event handlers
        files_input.upload(
            fn=handle_file_selection,
            inputs=[files_input],
            outputs=[selected_files_info, upload_output]
        )

        files_input.clear(
            fn=lambda: ([], "‡∏•‡πâ‡∏≤‡∏á‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡πÑ‡∏ü‡∏•‡πå‡πÅ‡∏•‡πâ‡∏ß"),
            inputs=[],
            outputs=[selected_files_info]
        )

        upload_button.click(
            fn=process_multiple_files,
            inputs=[files_input, clear_before_upload],
            outputs=upload_output
        )

        clear_button.click(
            fn=clear_vector_db_and_images,
            inputs=None,
            outputs=upload_output,
            queue=False
        )
        upload_output = gr.Textbox(label="‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•", lines=3)
        upload_button.click(
            fn=process_multiple_files,
            inputs=[files_input, clear_before_upload],
            outputs=upload_output
        )
        clear_button.click(
            fn=clear_vector_db_and_images,
            inputs=None,
            outputs=upload_output,
            queue=False
        )

        # Google Sheets Import Section
        gr.Markdown("---")
        with gr.Row():
            gr.Markdown("### üìä ‡∏ô‡∏≥‡πÄ‡∏Ç‡πâ‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å Google Sheets")

        with gr.Group(elem_classes="upload-container"):
            gr.Markdown("""
            **üîó ‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô:**
            1. ‡πÄ‡∏õ‡∏¥‡∏î Google Sheets ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏ô‡∏≥‡πÄ‡∏Ç‡πâ‡∏≤
            2. ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô **"‡πÄ‡∏ú‡∏¢‡πÅ‡∏û‡∏£‡πà‡∏ï‡πà‡∏≠‡∏ó‡∏∏‡∏Å‡∏Ñ‡∏ô‡∏ö‡∏ô‡πÄ‡∏ß‡πá‡∏ö"** (Share > General access > Anyone with the link)
            3. ‡∏Ñ‡∏±‡∏î‡∏•‡∏≠‡∏Å URL ‡∏°‡∏≤‡∏ß‡∏≤‡∏á‡πÉ‡∏ô‡∏ä‡πà‡∏≠‡∏á‡∏î‡πâ‡∏≤‡∏ô‡∏•‡πà‡∏≤‡∏á
            """)

            with gr.Row():
                sheets_url_input = gr.Textbox(
                    label="üîó Google Sheets URL",
                    placeholder="https://docs.google.com/spreadsheets/d/...",
                    info="‡∏ß‡∏≤‡∏á URL ‡∏Ç‡∏≠‡∏á Google Sheets ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏ô‡∏≥‡πÄ‡∏Ç‡πâ‡∏≤",
                    scale=3
                )
                sheets_clear_checkbox = gr.Checkbox(
                    label="‡∏•‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏Å‡πà‡∏≤‡∏Å‡πà‡∏≠‡∏ô‡∏ô‡∏≥‡πÄ‡∏Ç‡πâ‡∏≤",
                    value=False,
                    info="‡∏ï‡∏¥‡πä‡∏Å‡∏ñ‡πâ‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏•‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏Å‡πà‡∏≤‡∏Å‡πà‡∏≠‡∏ô‡∏ô‡∏≥‡πÄ‡∏Ç‡πâ‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏´‡∏°‡πà",
                    scale=1
                )

            with gr.Row():
                sheets_import_button = gr.Button(
                    "üìä ‡∏ô‡∏≥‡πÄ‡∏Ç‡πâ‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Google Sheets",
                    variant="primary",
                    size="lg",
                    elem_classes="upload-button"
                )

            sheets_output = gr.Textbox(
                label="‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏Å‡∏≤‡∏£‡∏ô‡∏≥‡πÄ‡∏Ç‡πâ‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•",
                lines=6,
                interactive=False,
                elem_classes="status-output"
            )

        # Connect Google Sheets event handlers
        sheets_import_button.click(
            fn=process_google_sheets_url,
            inputs=[sheets_url_input, sheets_clear_checkbox],
            outputs=sheets_output
        )

        # ‡∏™‡πà‡∏ß‡∏ô‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ Database
        with gr.Row():
            gr.Markdown("### üóÑÔ∏è ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ Database")

        # Enhanced Backup & Restore Section
        with gr.Accordion("üíæ Enhanced Backup & Restore", open=True):
            with gr.Row():
                # Backup Controls
                with gr.Column(scale=1):
                    gr.Markdown("#### üì¶ ‡∏™‡∏≥‡∏£‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•")

                    backup_name_input = gr.Textbox(
                        label="‡∏ä‡∏∑‡πà‡∏≠ Backup (‡∏ñ‡πâ‡∏≤‡∏ß‡πà‡∏≤‡∏á‡∏à‡∏∞‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥)",
                        placeholder="‡πÄ‡∏ä‡πà‡∏ô: my_backup_20241030",
                        interactive=True
                    )

                    include_memory_checkbox = gr.Checkbox(
                        label="‡∏£‡∏ß‡∏° Enhanced RAG Memory",
                        value=(RAG_MODE == "enhanced"),
                        info="‡∏™‡∏≥‡∏£‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏£‡∏á‡∏à‡∏≥‡∏Å‡∏≤‡∏£‡∏™‡∏ô‡∏ó‡∏ô‡∏≤‡∏î‡πâ‡∏ß‡∏¢"
                    )

                    with gr.Row():
                        enhanced_backup_button = gr.Button("‡∏™‡∏≥‡∏£‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ç‡∏±‡πâ‡∏ô‡∏™‡∏π‡∏á", variant="primary")
                        quick_backup_button = gr.Button("‡∏™‡∏≥‡∏£‡∏≠‡∏á‡∏î‡πà‡∏ß‡∏ô", variant="secondary")

                # Restore Controls
                with gr.Column(scale=1):
                    gr.Markdown("#### üîÑ ‡∏Å‡∏π‡πâ‡∏Ñ‡∏∑‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•")

                    backup_selector = gr.Dropdown(
                        label="‡πÄ‡∏•‡∏∑‡∏≠‡∏Å Backup ‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏Å‡∏π‡πâ‡∏Ñ‡∏∑‡∏ô",
                        choices=[],
                        interactive=True,
                        info="‡∏£‡∏µ‡πÄ‡∏ü‡∏£‡∏ä‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏î‡∏π backup ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î"
                    )

                    with gr.Row():
                        restore_button = gr.Button("‡∏Å‡∏π‡πâ‡∏Ñ‡∏∑‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•", variant="primary")
                        refresh_backups_button = gr.Button("üîÑ ‡∏£‡∏µ‡πÄ‡∏ü‡∏£‡∏ä", size="sm")

            # Backup Status and Results
            backup_status_output = gr.Textbox(
                label="‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡∏Å‡∏≤‡∏£‡∏™‡∏≥‡∏£‡∏≠‡∏á/‡∏Å‡∏π‡πâ‡∏Ñ‡∏∑‡∏ô",
                lines=3,
                interactive=False
            )

            # Backup List Section
            with gr.Accordion("üìã ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£ Backup ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î", open=False):
                backup_list_output = gr.Textbox(
                    label="‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£ Backup",
                    lines=8,
                    interactive=False
                )

                with gr.Row():
                    delete_backup_button = gr.Button("‡∏•‡∏ö Backup ‡∏ó‡∏µ‡πà‡πÄ‡∏•‡∏∑‡∏≠‡∏Å", variant="stop", size="sm")
                    validate_backup_button = gr.Button("‚úÖ ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå", variant="secondary", size="sm")
                    clean_invalid_button = gr.Button("üßπ ‡∏•‡∏ö‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á", variant="secondary", size="sm")
                    refresh_list_button = gr.Button("‡∏£‡∏µ‡πÄ‡∏ü‡∏£‡∏ä‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£", size="sm")

        # Quick Database Operations
        with gr.Accordion("‚ö° ‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏Å‡∏≤‡∏£‡∏î‡πà‡∏ß‡∏ô", open=False):
            with gr.Row():
                db_info_button = gr.Button("‡πÅ‡∏™‡∏î‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Database", variant="secondary")
                inspect_button = gr.Button("‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î", variant="secondary")
                auto_backup_button = gr.Button("‡∏™‡∏£‡πâ‡∏≤‡∏á Auto Backup", variant="secondary")

            db_info_output = gr.Textbox(label="‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Database", lines=5, interactive=False)
            inspect_output = gr.Textbox(label="‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏†‡∏≤‡∏¢‡πÉ‡∏ô Database", lines=10, interactive=False)

        # Event Handlers for Enhanced Backup & Restore
        def enhanced_backup_handler(backup_name, include_memory):
            if not backup_name.strip():
                backup_name = None
            result = backup_database_enhanced(backup_name, include_memory)
            if result["success"]:
                return f"""‚úÖ ‡∏™‡∏≥‡∏£‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à!
‚Ä¢ ‡∏ä‡∏∑‡πà‡∏≠ Backup: {result['backup_name']}
‚Ä¢ ‡∏£‡∏ß‡∏° Memory: {'‚úÖ' if include_memory else '‚ùå'}
‚Ä¢ ‡∏Ç‡∏ô‡∏≤‡∏î: {result['metadata']}

‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏Å‡∏π‡πâ‡∏Ñ‡∏∑‡∏ô‡πÑ‡∏î‡πâ‡∏à‡∏≤‡∏Å‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£ backup"""
            else:
                return f"‚ùå ‡∏™‡∏≥‡∏£‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏°‡πà‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à: {result.get('error', 'Unknown error')}"

        def quick_backup_handler():
            result = backup_database_enhanced()
            if result["success"]:
                return f"‚úÖ ‡∏™‡∏≥‡∏£‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏î‡πà‡∏ß‡∏ô‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à: {result['backup_name']}"
            else:
                return f"‚ùå ‡∏™‡∏≥‡∏£‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏°‡πà‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à: {result.get('error', 'Unknown error')}"

        def restore_handler(backup_name):
            if not backup_name:
                return "‚ùå ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡πÄ‡∏•‡∏∑‡∏≠‡∏Å backup ‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏Å‡∏π‡πâ‡∏Ñ‡∏∑‡∏ô"

            result = restore_database_enhanced(backup_name)
            if result["success"]:
                emergency_name = result.get('emergency_backup', {}).get('backup_name', 'N/A')
                return f"""‚úÖ ‡∏Å‡∏π‡πâ‡∏Ñ‡∏∑‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à!
‚Ä¢ ‡∏à‡∏≤‡∏Å Backup: {backup_name}
‚Ä¢ ‡πÄ‡∏ß‡∏•‡∏≤‡∏Å‡∏π‡πâ‡∏Ñ‡∏∑‡∏ô: {result['restored_at']}
‚Ä¢ ‡∏™‡∏£‡πâ‡∏≤‡∏á Emergency Backup: {emergency_name}

‡πÇ‡∏õ‡∏£‡∏î‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏´‡∏•‡∏±‡∏á‡∏Å‡∏π‡πâ‡∏Ñ‡∏∑‡∏ô"""
            else:
                return f"‚ùå ‡∏Å‡∏π‡πâ‡∏Ñ‡∏∑‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏°‡πà‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à: {result.get('error', 'Unknown error')}"

        def refresh_backups_handler():
            backups = list_available_backups()
            if backups:
                choices = [backup["name"] for backup in backups]
                return gr.Dropdown(choices=choices, value=choices[0] if choices else None)
            else:
                return gr.Dropdown(choices=[], value=None)

        def get_backup_list_handler():
            backups = list_available_backups()
            if backups:
                backup_info = []
                for backup in backups:
                    info = f"""üìÅ {backup['name']}
‚Ä¢ ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÄ‡∏°‡∏∑‡πà‡∏≠: {backup['created_at']}
‚Ä¢ ‡∏Ç‡∏ô‡∏≤‡∏î: {backup['size_mb']} MB
‚Ä¢ ‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó: {backup['type']}
‚Ä¢ Memory: {'‚úÖ' if backup['includes_memory'] else '‚ùå'}
‚Ä¢ RAG Mode: {backup['rag_mode']}
‚Ä¢ Records: {backup['database_info'].get('total_records', 'N/A') if backup['database_info'] else 'N/A'}
---"""
                    backup_info.append(info)
                return "\n".join(backup_info)
            else:
                return "‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• backup ‡πÉ‡∏ô‡∏£‡∏∞‡∏ö‡∏ö"

        def delete_backup_handler(backup_name):
            if not backup_name:
                return "‚ùå ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡πÄ‡∏•‡∏∑‡∏≠‡∏Å backup ‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏•‡∏ö"

            result = delete_backup(backup_name)
            if result["success"]:
                return f"‚úÖ {result['message']}"
            else:
                return f"‚ùå ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏•‡∏ö backup: {result.get('error', 'Unknown error')}"

        def auto_backup_handler():
            result = auto_backup_before_operation()
            if result["success"]:
                return f"‚úÖ ‡∏™‡∏£‡πâ‡∏≤‡∏á Auto Backup ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à: {result['backup_name']}"
            else:
                return f"‚ùå ‡∏™‡∏£‡πâ‡∏≤‡∏á Auto Backup ‡πÑ‡∏°‡πà‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à: {result.get('error', 'Unknown error')}"

        def clean_invalid_handler():
            try:
                cleanup_invalid_backups()
                return "‚úÖ ‡∏•‡∏ö‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏£‡πâ‡∏≠‡∏¢‡πÅ‡∏•‡πâ‡∏ß"
            except Exception as e:
                return f"‚ùå ‡∏•‡∏ö‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡πÑ‡∏°‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡πÑ‡∏°‡πà‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à: {str(e)}"

        def validate_backup_handler(backup_name):
            if not backup_name:
                return "‚ùå ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡πÄ‡∏•‡∏∑‡∏≠‡∏Å backup ‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö"

            backup_path = os.path.join(TEMP_VECTOR_BACKUP, backup_name)
            if not os.path.exists(backup_path):
                return f"‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö backup: {backup_name}"

            is_valid, message = validate_backup_integrity(backup_path)
            if is_valid:
                return f"""‚úÖ Backup ‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á!
‚Ä¢ ‡∏ä‡∏∑‡πà‡∏≠ Backup: {backup_name}
‚Ä¢ ‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞: {message}
‚Ä¢ ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏Å‡∏π‡πâ‡∏Ñ‡∏∑‡∏ô"""
            else:
                return f"""‚ùå Backup ‡πÑ‡∏°‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á!
‚Ä¢ ‡∏ä‡∏∑‡πà‡∏≠ Backup: {backup_name}
‚Ä¢ ‡∏õ‡∏±‡∏ç‡∏´‡∏≤: {message}
‚Ä¢ ‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥: ‡∏™‡∏£‡πâ‡∏≤‡∏á backup ‡πÉ‡∏´‡∏°‡πà‡πÅ‡∏ó‡∏ô"""

        # Connect event handlers
        enhanced_backup_button.click(
            fn=enhanced_backup_handler,
            inputs=[backup_name_input, include_memory_checkbox],
            outputs=backup_status_output,
            queue=False
        )

        quick_backup_button.click(
            fn=quick_backup_handler,
            inputs=None,
            outputs=backup_status_output,
            queue=False
        )

        restore_button.click(
            fn=restore_handler,
            inputs=backup_selector,
            outputs=backup_status_output,
            queue=False
        )

        refresh_backups_button.click(
            fn=refresh_backups_handler,
            inputs=None,
            outputs=backup_selector,
            queue=False
        )

        refresh_list_button.click(
            fn=get_backup_list_handler,
            inputs=None,
            outputs=backup_list_output,
            queue=False
        )

        delete_backup_button.click(
            fn=delete_backup_handler,
            inputs=backup_selector,
            outputs=backup_list_output,
            queue=False
        )

        auto_backup_button.click(
            fn=auto_backup_handler,
            inputs=None,
            outputs=db_info_output,
            queue=False
        )

        clean_invalid_button.click(
            fn=clean_invalid_handler,
            inputs=None,
            outputs=backup_status_output,
            queue=False
        )

        validate_backup_button.click(
            fn=validate_backup_handler,
            inputs=backup_selector,
            outputs=backup_status_output,
            queue=False
        )

        # Initialize backup list on page load
        demo.load(
            fn=refresh_backups_handler,
            inputs=None,
            outputs=backup_selector,
            queue=False
        )

        # Quick database operations handlers
        db_info_button.click(
            fn=lambda: str(get_database_info()),
            inputs=None,
            outputs=db_info_output,
            queue=False
        )

        inspect_button.click(
            fn=inspect_database,
            inputs=None,
            outputs=inspect_output,
            queue=False
        )

        # ‡∏™‡πà‡∏ß‡∏ô‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ Discord Bot
        with gr.Row():
            gr.Markdown("### ü§ñ ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ Discord Bot")

        with gr.Row():
            start_bot_button = gr.Button("‡πÄ‡∏£‡∏¥‡πà‡∏° Discord Bot", variant="primary")
            stop_bot_button = gr.Button("‡∏´‡∏¢‡∏∏‡∏î Discord Bot", variant="stop")

        bot_status_output = gr.Textbox(label="‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞ Discord Bot", lines=3)

        with gr.Row():
            bot_model_display = gr.Textbox(
                label="‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô (‡∏ã‡∏¥‡∏á‡∏Ñ‡πå‡∏à‡∏≤‡∏Å‡∏´‡∏ô‡πâ‡∏≤‡πÅ‡∏ä‡∏ó)",
                value=f"‡∏£‡∏≠‡∏Å‡∏≤‡∏£‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏à‡∏≤‡∏Å‡∏´‡∏ô‡πâ‡∏≤‡πÅ‡∏ä‡∏ó... (Fallback: {DISCORD_DEFAULT_MODEL})",
                interactive=False,
                lines=1
            )

            bot_reply_mode = gr.Dropdown(
                choices=[
                    ("‡∏ï‡∏≠‡∏ö‡πÉ‡∏ô Channel", "channel"),
                    ("‡∏ï‡∏≠‡∏ö‡πÉ‡∏ô DM", "dm"),
                    ("‡∏ï‡∏≠‡∏ö‡∏ó‡∏±‡πâ‡∏á Channel ‡πÅ‡∏•‡∏∞ DM", "both")
                ],
                value=DISCORD_REPLY_MODE,
                label="‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£‡∏ï‡∏≠‡∏ö‡∏Å‡∏•‡∏±‡∏ö"
            )

        def update_discord_reply_mode(mode):
            global DISCORD_REPLY_MODE
            DISCORD_REPLY_MODE = mode
            mode_name = {"channel": "‡πÉ‡∏ô Channel", "dm": "‡πÉ‡∏ô DM", "both": "‡∏ó‡∏±‡πâ‡∏á Channel ‡πÅ‡∏•‡∏∞ DM"}
            return f"‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£‡∏ï‡∏≠‡∏ö‡∏Å‡∏•‡∏±‡∏ö‡πÄ‡∏õ‡πá‡∏ô: {mode_name.get(mode, mode)}"

        def start_bot_ui():
            if start_discord_bot_thread():
                return "Discord Bot ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡πÅ‡∏•‡πâ‡∏ß! ‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÉ‡∏ä‡πâ‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á‡πÑ‡∏î‡πâ‡πÉ‡∏ô Discord"
            else:
                return "‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÄ‡∏£‡∏¥‡πà‡∏° Discord Bot ‡πÑ‡∏î‡πâ ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡πÉ‡∏ô .env"

        def stop_bot_ui():
            if stop_discord_bot():
                return "Discord Bot ‡∏´‡∏¢‡∏∏‡∏î‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡πÅ‡∏•‡πâ‡∏ß"
            else:
                return "‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏´‡∏¢‡∏∏‡∏î Discord Bot ‡πÑ‡∏î‡πâ"

        start_bot_button.click(
            fn=start_bot_ui,
            inputs=None,
            outputs=bot_status_output,
            queue=False
        )

        stop_bot_button.click(
            fn=stop_bot_ui,
            inputs=None,
            outputs=bot_status_output,
            queue=False
        )

        bot_reply_mode.change(
            fn=update_discord_reply_mode,
            inputs=bot_reply_mode,
            outputs=bot_status_output,
            queue=False
        )

        # ‡∏™‡πà‡∏ß‡∏ô‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ LINE OA Bot
        with gr.Row():
            gr.Markdown("### üì± ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ LINE OA Bot")

        with gr.Row():
            start_line_button = gr.Button("‡πÄ‡∏£‡∏¥‡πà‡∏° LINE OA Bot", variant="primary")
            stop_line_button = gr.Button("‡∏´‡∏¢‡∏∏‡∏î LINE OA Bot", variant="stop")

        line_status_output = gr.Textbox(label="‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞ LINE OA Bot", lines=3)
        line_model_display = gr.Textbox(
            label="‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô (‡∏ã‡∏¥‡∏á‡∏Ñ‡πå‡∏à‡∏≤‡∏Å‡∏´‡∏ô‡πâ‡∏≤‡πÅ‡∏ä‡∏ó)",
            value=f"‡∏£‡∏≠‡∏Å‡∏≤‡∏£‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏à‡∏≤‡∏Å‡∏´‡∏ô‡πâ‡∏≤‡πÅ‡∏ä‡∏ó... (Fallback: {LINE_DEFAULT_MODEL})",
            interactive=False,
            lines=1
        )

        def start_line_ui():
            if start_line_bot_thread():
                return f"LINE OA Bot ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡πÅ‡∏•‡πâ‡∏ß! Webhook URL: http://localhost:{LINE_WEBHOOK_PORT}/callback"
            else:
                return "‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÄ‡∏£‡∏¥‡πà‡∏° LINE OA Bot ‡πÑ‡∏î‡πâ ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡πÉ‡∏ô .env"

        def stop_line_ui():
            if line_thread and line_thread.is_alive():
                return "LINE OA Bot ‡∏´‡∏¢‡∏∏‡∏î‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡πÅ‡∏•‡πâ‡∏ß (‡∏£‡∏µ‡∏™‡∏ï‡∏≤‡∏£‡πå‡∏ó‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ restart ‡πÇ‡∏õ‡∏£‡πÅ‡∏Å‡∏£‡∏°)"
            else:
                return "LINE OA Bot ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß"

        start_line_button.click(
            fn=start_line_ui,
            inputs=None,
            outputs=line_status_output,
            queue=False
        )

        stop_line_button.click(
            fn=stop_line_ui,
            inputs=None,
            outputs=line_status_output,
            queue=False
        )

        # ‡∏™‡πà‡∏ß‡∏ô‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ Facebook Messenger Bot
        with gr.Row():
            gr.Markdown("### üí¨ ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ Facebook Messenger Bot")

        with gr.Row():
            start_fb_button = gr.Button("‡πÄ‡∏£‡∏¥‡πà‡∏° Facebook Bot", variant="primary")
            stop_fb_button = gr.Button("‡∏´‡∏¢‡∏∏‡∏î Facebook Bot", variant="stop")

        fb_status_output = gr.Textbox(label="‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞ Facebook Bot", lines=3)
        fb_model_display = gr.Textbox(
            label="‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô (‡∏ã‡∏¥‡∏á‡∏Ñ‡πå‡∏à‡∏≤‡∏Å‡∏´‡∏ô‡πâ‡∏≤‡πÅ‡∏ä‡∏ó)",
            value=f"‡∏£‡∏≠‡∏Å‡∏≤‡∏£‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏à‡∏≤‡∏Å‡∏´‡∏ô‡πâ‡∏≤‡πÅ‡∏ä‡∏ó... (Fallback: {FB_DEFAULT_MODEL})",
            interactive=False,
            lines=1
        )

        def start_fb_ui():
            if start_facebook_bot_thread():
                return f"Facebook Bot ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡πÅ‡∏•‡πâ‡∏ß! Webhook URL: {FB_WEBHOOK}"
            else:
                return "‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÄ‡∏£‡∏¥‡πà‡∏° Facebook Bot ‡πÑ‡∏î‡πâ ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡πÉ‡∏ô .env"

        def stop_fb_ui():
            if fb_thread and fb_thread.is_alive():
                return "Facebook Bot ‡∏´‡∏¢‡∏∏‡∏î‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡πÅ‡∏•‡πâ‡∏ß (‡∏£‡∏µ‡∏™‡∏ï‡∏≤‡∏£‡πå‡∏ó‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ restart ‡πÇ‡∏õ‡∏£‡πÅ‡∏Å‡∏£‡∏°)"
            else:
                return "Facebook Bot ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß"

        start_fb_button.click(
            fn=start_fb_ui,
            inputs=None,
            outputs=fb_status_output,
            queue=False
        )

        stop_fb_button.click(
            fn=stop_fb_ui,
            inputs=None,
            outputs=fb_status_output,
            queue=False
        )

    with gr.Tab("üìä Feedback ‡πÅ‡∏•‡∏∞‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥"):
        gr.Markdown("## üìä ‡∏£‡∏∞‡∏ö‡∏ö Feedback ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö")

        # ‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥‡∏´‡∏•‡∏±‡∏Å
        with gr.Row():
            with gr.Column(scale=1):
                gr.Markdown("### üìà ‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥‡∏Å‡∏≤‡∏£‡∏ï‡∏≠‡∏ö‡∏Å‡∏•‡∏±‡∏ö")
                stats_display = gr.HTML()

                refresh_stats_btn = gr.Button("üîÑ ‡∏£‡∏µ‡πÄ‡∏ü‡∏£‡∏ä‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥", variant="secondary")

                # ‡∏õ‡∏∏‡πà‡∏°‡∏™‡πà‡∏á‡∏≠‡∏≠‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
                export_btn = gr.Button("üì• ‡∏™‡πà‡∏á‡∏≠‡∏≠‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Feedback", variant="primary")
                download_file = gr.File(visible=False)

            with gr.Column(scale=2):
                gr.Markdown("### üìù Feedback ‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î")
                feedback_display = gr.Dataframe(
                    headers=["‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°", "‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö", "‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó", "‡πÄ‡∏ß‡∏•‡∏≤", "‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏¥‡∏î‡πÄ‡∏´‡πá‡∏ô"],
                    datatype=["str", "str", "str", "str", "str"],
                    interactive=False,
                    wrap=True,
                    value=[]  # ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏î‡πâ‡∏ß‡∏¢‡∏Ñ‡πà‡∏≤‡∏ß‡πà‡∏≤‡∏á
                )

                # ‡∏™‡πà‡∏ß‡∏ô‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ feedback
                with gr.Row():
                    feedback_id_input = gr.Number(
                        label="Feedback ID ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏•‡∏ö",
                        minimum=1,
                        step=1,
                        info="‡πÉ‡∏™‡πà ID ‡∏à‡∏≤‡∏Å‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡∏î‡πâ‡∏≤‡∏ô‡∏ö‡∏ô"
                    )
                    delete_feedback_btn = gr.Button("üóëÔ∏è ‡∏•‡∏ö Feedback", variant="stop")

                delete_status = gr.Textbox(label="‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡∏Å‡∏≤‡∏£‡∏•‡∏ö", interactive=False)

        # ‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ
        with gr.Row():
            with gr.Column():
                gr.Markdown("### üéì ‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ (Learning Analytics)")
                learning_stats_display = gr.HTML()

                refresh_learning_btn = gr.Button("üîÑ ‡∏£‡∏µ‡πÄ‡∏ü‡∏£‡∏ä‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ", variant="secondary")

        # Enhanced Analytics Dashboard
        with gr.Row():
            with gr.Column():
                gr.Markdown("### üìä Analytics Dashboard (Advanced Insights)")

                with gr.Row():
                    with gr.Column(scale=1):
                        # Quality Score Overview
                        quality_score_display = gr.HTML()

                        # Pattern Analysis Results
                        pattern_display = gr.HTML()

                    with gr.Column(scale=1):
                        # Weekly Trend Chart
                        weekly_trend_display = gr.HTML()

                        # Improvement Recommendations
                        recommendations_display = gr.HTML()

                with gr.Row():
                    refresh_analytics_btn = gr.Button("üìà ‡∏£‡∏µ‡πÄ‡∏ü‡∏£‡∏ä Analytics ‡∏Ç‡∏±‡πâ‡∏ô‡∏™‡∏π‡∏á", variant="primary")
                    export_analytics_btn = gr.Button("üì• ‡∏™‡πà‡∏á‡∏≠‡∏≠‡∏Å‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô", variant="secondary")

                analytics_export_file = gr.File(
                    label="üìä Analytics Report",
                    visible=False,
                    file_types=[".json"]
                )

                # ‡πÅ‡∏™‡∏î‡∏á‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ô‡∏≥‡πÑ‡∏õ‡πÉ‡∏ä‡πâ‡∏ö‡πà‡∏≠‡∏¢‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î
                most_used_display = gr.Dataframe(
                    headers=["‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç", "‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏ó‡∏µ‡πà‡∏ô‡∏≥‡πÑ‡∏õ‡πÉ‡∏ä‡πâ"],
                    datatype=["str", "int"],
                    interactive=False,
                    wrap=True
                )

        # Enhanced Analytics Functions
        def update_analytics_dashboard():
            """‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï analytics dashboard ‡∏Ç‡∏±‡πâ‡∏ô‡∏™‡∏π‡∏á"""
            try:
                analytics = get_comprehensive_analytics()
                if not analytics:
                    return "<div>‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• analytics</div>", "<div>‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•</div>", "<div>‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•</div>", "<div>‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•</div>"

                # Quality Score Display
                pattern_analysis = analytics.get('pattern_analysis', {})
                quality_score = pattern_analysis.get('quality_score', 0)
                quality_color = '#4caf50' if quality_score >= 80 else '#ff9800' if quality_score >= 60 else '#f44336'

                quality_html = f"""
                <div style="background: white; padding: 20px; border-radius: 10px; border: 1px solid #e0e0e0;">
                    <h4 style="margin: 0 0 15px 0; color: #333;">üéØ ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏£‡∏∞‡∏ö‡∏ö</h4>
                    <div style="text-align: center;">
                        <div style="font-size: 3em; font-weight: bold; color: {quality_color}; margin: 10px 0;">
                            {quality_score:.1f}%
                        </div>
                        <div style="color: #666; font-size: 0.9em;">
                            ‡∏à‡∏≤‡∏Å {pattern_analysis.get('total_analyzed', 0)} ‡∏Å‡∏≤‡∏£‡∏ï‡∏≠‡∏ö‡∏Å‡∏•‡∏±‡∏ö‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î
                        </div>
                    </div>
                </div>
                """

                # Pattern Analysis Display
                patterns = pattern_analysis.get('patterns', {})
                pattern_html = "<div style='background: white; padding: 20px; border-radius: 10px; border: 1px solid #e0e0e0;'><h4 style='margin: 0 0 15px 0; color: #333;'>üîç ‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏õ‡∏±‡∏ç‡∏´‡∏≤</h4>"
                if patterns:
                    for category, count in patterns.items():
                        pattern_html += f"<div style='margin: 8px 0; padding: 8px; background: #f5f5f5; border-radius: 5px;'>{category}: {count} ‡∏Ñ‡∏£‡∏±‡πâ‡∏á</div>"
                else:
                    pattern_html += "<div style='color: #666;'>‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏ó‡∏µ‡πà‡∏ô‡πà‡∏≤‡∏™‡∏ô‡πÉ‡∏à</div>"
                pattern_html += "</div>"

                # Weekly Trend Display
                weekly_trend = analytics.get('weekly_trend', [])
                trend_html = "<div style='background: white; padding: 20px; border-radius: 10px; border: 1px solid #e0e0e0;'><h4 style='margin: 0 0 15px 0; color: #333;'>üìà ‡πÅ‡∏ô‡∏ß‡πÇ‡∏ô‡πâ‡∏° 7 ‡∏ß‡∏±‡∏ô</h4>"
                if weekly_trend:
                    for date, count, good_count in weekly_trend:
                        accuracy = (good_count / count * 100) if count > 0 else 0
                        trend_html += f"<div style='margin: 8px 0; padding: 8px; background: #f5f5f5; border-radius: 5px; display: flex; justify-content: space-between;'><span>{date}</span><span>{count} ‡∏Ñ‡∏£‡∏±‡πâ‡∏á ({accuracy:.0f}% ‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á)</span></div>"
                else:
                    trend_html += "<div style='color: #666;'>‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏ô‡∏ä‡πà‡∏ß‡∏á 7 ‡∏ß‡∏±‡∏ô</div>"
                trend_html += "</div>"

                # Recommendations Display
                recommendations = pattern_analysis.get('recommendations', [])
                rec_html = "<div style='background: white; padding: 20px; border-radius: 10px; border: 1px solid #e0e0e0;'><h4 style='margin: 0 0 15px 0; color: #333;'>üí° ‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á</h4>"
                if recommendations:
                    for rec in recommendations:
                        rec_html += f"<div style='margin: 8px 0; padding: 10px; background: #e3f2fd; border-radius: 5px; border-left: 4px solid #2196f3;'>{rec}</div>"
                else:
                    rec_html += "<div style='color: #666;'>‡∏£‡∏∞‡∏ö‡∏ö‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡πÑ‡∏î‡πâ‡∏î‡∏µ‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß</div>"
                rec_html += "</div>"

                return quality_html, pattern_html, trend_html, rec_html

            except Exception as e:
                error_html = f"<div style='color: red;'>‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î: {str(e)}</div>"
                return error_html, error_html, error_html, error_html

        def export_analytics_report():
            """‡∏™‡πà‡∏á‡∏≠‡∏≠‡∏Å‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô analytics"""
            try:
                analytics = get_comprehensive_analytics()
                report = {
                    "report_type": "comprehensive_analytics",
                    "generated_at": datetime.now().isoformat(),
                    "data": analytics
                }

                # ‡∏™‡∏£‡πâ‡∏≤‡∏á JSON file ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö download
                import json
                report_json = json.dumps(report, ensure_ascii=False, indent=2)

                return gr.File(value=report_json, visible=True, label="üìä Analytics Report.json")
            except Exception as e:
                return gr.HTML(f"<div style='color: red;'>‚ùå ‡∏™‡πà‡∏á‡∏≠‡∏≠‡∏Å‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡πÑ‡∏°‡πà‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à: {str(e)}</div>")

        # ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥
        def update_stats_display():
            try:
                stats = get_feedback_stats()

                stats_html = f"""
                <div style="display: flex; gap: 20px; margin-bottom: 20px;">
                    <div style="background: #e8f5e8; padding: 15px; border-radius: 8px; text-align: center; flex: 1;">
                        <h3 style="margin: 0; color: #2e7d32;">{stats['total']}</h3>
                        <p style="margin: 5px 0 0 0; color: #555;">‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î</p>
                    </div>
                    <div style="background: #e3f2fd; padding: 15px; border-radius: 8px; text-align: center; flex: 1;">
                        <h3 style="margin: 0; color: #1976d2;">{stats['good']}</h3>
                        <p style="margin: 5px 0 0 0; color: #555;">‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á üëç</p>
                    </div>
                    <div style="background: #ffebee; padding: 15px; border-radius: 8px; text-align: center; flex: 1;">
                        <h3 style="margin: 0; color: #d32f2f;">{stats['bad']}</h3>
                        <p style="margin: 5px 0 0 0; color: #555;">‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î üëé</p>
                    </div>
                    <div style="background: #fff3e0; padding: 15px; border-radius: 8px; text-align: center; flex: 1;">
                        <h3 style="margin: 0; color: #f57c00;">{stats['accuracy']:.1f}%</h3>
                        <p style="margin: 5px 0 0 0; color: #555;">‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥</p>
                    </div>
                </div>
                """

                return stats_html, stats['recent']
            except Exception as e:
                error_html = f"""
                <div style="background: #ffebee; padding: 15px; border-radius: 8px; text-align: center;">
                    <h3 style="margin: 0; color: #d32f2f;">‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î</h3>
                    <p style="margin: 5px 0 0 0; color: #555;">‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏î‡πâ: {str(e)}</p>
                </div>
                """
                return error_html, []

        # ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏•‡∏ö feedback
        def delete_feedback_handler(feedback_id):
            if feedback_id is None or feedback_id <= 0:
                return "‚ùå ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏£‡∏∞‡∏ö‡∏∏ Feedback ID ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á"

            if delete_feedback(int(feedback_id)):
                # ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥‡πÅ‡∏•‡∏∞‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡πÉ‡∏´‡∏°‡πà
                stats_html, recent_data = update_stats_display()
                return "‚úÖ ‡∏•‡∏ö Feedback ‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏£‡πâ‡∏≠‡∏¢‡πÅ‡∏•‡πâ‡∏ß", stats_html, recent_data
            else:
                return "‚ùå ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏•‡∏ö Feedback ‡πÑ‡∏î‡πâ (ID ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏´‡∏£‡∏∑‡∏≠‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î)", None, None

        # ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏™‡πà‡∏á‡∏≠‡∏≠‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
        def export_feedback_handler():
            csv_data = export_feedback()
            if csv_data:
                import io
                from datetime import datetime

                filename = f"feedback_export_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv"

                # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå‡∏ä‡∏±‡πà‡∏ß‡∏Ñ‡∏£‡∏≤‡∏ß
                filepath = f"./data/{filename}"
                with open(filepath, 'w', encoding='utf-8-sig') as f:  # utf-8-sig ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Excel
                    f.write(csv_data)

                return filepath
            return None

        # ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ
        def update_learning_display():
            try:
                learning_stats = get_learning_stats()

                learning_html = f"""
                <div style="display: flex; gap: 20px; margin-bottom: 20px;">
                    <div style="background: #e8f5e8; padding: 15px; border-radius: 8px; text-align: center; flex: 1;">
                        <h3 style="margin: 0; color: #2e7d32;">{learning_stats['total_corrected']}</h3>
                        <p style="margin: 5px 0 0 0; color: #555;">‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç</p>
                    </div>
                    <div style="background: #e3f2fd; padding: 15px; border-radius: 8px; text-align: center; flex: 1;">
                        <h3 style="margin: 0; color: #1976d2;">{learning_stats['used_corrected']}</h3>
                        <p style="margin: 5px 0 0 0; color: #555;">‡∏ñ‡∏π‡∏Å‡∏ô‡∏≥‡πÑ‡∏õ‡πÉ‡∏ä‡πâ</p>
                    </div>
                    <div style="background: #fff3e0; padding: 15px; border-radius: 8px; text-align: center; flex: 1;">
                        <h3 style="margin: 0; color: #f57c00;">{learning_stats['learning_rate']:.1f}%</h3>
                        <p style="margin: 5px 0 0 0; color: #555;">‡∏≠‡∏±‡∏ï‡∏£‡∏≤‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ</p>
                    </div>
                    <div style="background: #f3e5f5; padding: 15px; border-radius: 8px; text-align: center; flex: 1;">
                        <h3 style="margin: 0; color: #7b1fa2;">{learning_stats['corrected_feedback']}</h3>
                        <p style="margin: 5px 0 0 0; color: #555;">Feedback ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç</p>
                    </div>
                </div>
                """

                # ‡∏à‡∏±‡∏î‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏™‡∏î‡∏á‡πÉ‡∏ô‡∏ï‡∏≤‡∏£‡∏≤‡∏á
                most_used_data = []
                for item in learning_stats['most_used']:
                    # ‡∏ï‡∏±‡∏î‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏¢‡∏≤‡∏ß‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ
                    question = item[0]
                    if len(question) > 100:
                        question = question[:97] + "..."
                    most_used_data.append([question, item[1]])

                return learning_html, most_used_data
            except Exception as e:
                error_html = f"""
                <div style="background: #ffebee; padding: 15px; border-radius: 8px; text-align: center;">
                    <h3 style="margin: 0; color: #d32f2f;">‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î</h3>
                    <p style="margin: 5px 0 0 0; color: #555;">‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡πÑ‡∏î‡πâ: {str(e)}</p>
                </div>
                """
                return error_html, []

        # ‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠ events
        refresh_stats_btn.click(
            fn=update_stats_display,
            inputs=[],
            outputs=[stats_display, feedback_display]
        )

        delete_feedback_btn.click(
            fn=delete_feedback_handler,
            inputs=[feedback_id_input],
            outputs=[delete_status, stats_display, feedback_display]
        )

        export_btn.click(
            fn=export_feedback_handler,
            inputs=[],
            outputs=[download_file]
        )

        refresh_learning_btn.click(
            fn=update_learning_display,
            inputs=[],
            outputs=[learning_stats_display, most_used_display]
        )

        # Analytics Dashboard Event Handlers
        refresh_analytics_btn.click(
            fn=update_analytics_dashboard,
            inputs=[],
            outputs=[quality_score_display, pattern_display, weekly_trend_display, recommendations_display]
        )

        export_analytics_btn.click(
            fn=export_analytics_report,
            inputs=[],
            outputs=[analytics_export_file]
        )

        # ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÅ‡∏£‡∏Å (delayed load with error handling)
        demo.load(
            fn=lambda: [update_stats_display(), update_learning_display()],
            inputs=[],
            outputs=[stats_display, feedback_display, learning_stats_display, most_used_display],
            show_progress=True
        )

    # ==================== TAG MANAGEMENT TAB ====================
    with gr.Tab("üè∑Ô∏è ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ Tag"):
        gr.Markdown("## üè∑Ô∏è ‡∏£‡∏∞‡∏ö‡∏ö‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ Tag")
        gr.Markdown("‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ tags ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏à‡∏±‡∏î‡∏Å‡∏•‡∏∏‡πà‡∏°‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡πÅ‡∏•‡∏∞ feedback ‡πÉ‡∏´‡πâ‡πÄ‡∏Ç‡πâ‡∏≤‡∏ñ‡∏∂‡∏á‡πÑ‡∏î‡πâ‡∏£‡∏ß‡∏î‡πÄ‡∏£‡πá‡∏ß‡πÅ‡∏•‡∏∞‡∏ï‡∏£‡∏á‡∏õ‡∏£‡∏∞‡πÄ‡∏î‡πá‡∏ô")

        with gr.Row():
            with gr.Column(scale=2):
                gr.Markdown("### üìù ‡∏™‡∏£‡πâ‡∏≤‡∏á Tag ‡πÉ‡∏´‡∏°‡πà")
                with gr.Row():
                    tag_name_input = gr.Textbox(
                        label="‡∏ä‡∏∑‡πà‡∏≠ Tag",
                        placeholder="‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á: ‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏ó‡∏µ‡πà‡∏û‡∏ö‡∏ö‡πà‡∏≠‡∏¢, ‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç, ‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ",
                        scale=3
                    )
                    tag_color_input = gr.ColorPicker(
                        label="‡∏™‡∏µ Tag",
                        value="#007bff",
                        scale=1
                    )
                tag_desc_input = gr.Textbox(
                    label="‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î Tag",
                    placeholder="‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö tag ‡∏ô‡∏µ‡πâ"
                )
                create_tag_btn = gr.Button("üè∑Ô∏è ‡∏™‡∏£‡πâ‡∏≤‡∏á Tag", variant="primary")

            with gr.Column(scale=3):
                gr.Markdown("### üìã Tags ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î")
                tags_list = gr.Dataframe(
                    headers=["ID", "‡∏ä‡∏∑‡πà‡∏≠ Tag", "‡∏™‡∏µ", "‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î", "‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡∏™‡∏£‡πâ‡∏≤‡∏á"],
                    datatype=["number", "str", "str", "str", "str"],
                    interactive=False,
                    wrap=True
                )
                refresh_tags_btn = gr.Button("üîÑ ‡∏£‡∏µ‡πÄ‡∏ü‡∏£‡∏ä‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£ Tag")
                delete_tag_btn = gr.Button("üóëÔ∏è ‡∏•‡∏ö Tag ‡∏ó‡∏µ‡πà‡πÄ‡∏•‡∏∑‡∏≠‡∏Å", variant="stop")

        with gr.Row():
            with gr.Column():
                gr.Markdown("### üèÜ Tags ‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡∏ö‡πà‡∏≠‡∏¢")
                popular_tags = gr.Dataframe(
                    headers=["‡∏ä‡∏∑‡πà‡∏≠ Tag", "‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ"],
                    datatype=["str", "number"],
                    interactive=False,
                    wrap=True
                )

            with gr.Column():
                gr.Markdown("### üí¨ Tags ‡πÉ‡∏ô Feedback")
                feedback_tags = gr.Dataframe(
                    headers=["‡∏ä‡∏∑‡πà‡∏≠ Tag", "‡∏à‡∏≥‡∏ô‡∏ß‡∏ô Feedback"],
                    datatype=["str", "number"],
                    interactive=False,
                    wrap=True
                )

        with gr.Row():
            with gr.Column():
                gr.Markdown("### üîç ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ï‡∏≤‡∏° Tag")
                with gr.Row():
                    selected_tags_search = gr.CheckboxGroup(
                        label="‡πÄ‡∏•‡∏∑‡∏≠‡∏Å Tags (‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏´‡∏•‡∏≤‡∏¢‡∏≠‡∏±‡∏ô‡πÑ‡∏î‡πâ)",
                        choices=[]
                    )
                    search_by_tags_btn = gr.Button("üîç ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤", variant="primary")

                search_results = gr.Dataframe(
                    headers=["Document ID", "Content Preview"],
                    datatype=["str", "str"],
                    interactive=False,
                    wrap=True
                )

            with gr.Column():
                gr.Markdown("### üí¨ Feedback ‡∏ï‡∏≤‡∏° Tag")
                tag_feedback_selector = gr.Dropdown(
                    label="‡πÄ‡∏•‡∏∑‡∏≠‡∏Å Tag",
                    choices=[]
                )
                load_feedback_by_tag_btn = gr.Button("üìã ‡πÇ‡∏´‡∏•‡∏î Feedback", variant="primary")

                tag_feedback_display = gr.Dataframe(
                    headers=["ID", "‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°", "‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö", "‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó", "‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà", "‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏¥‡∏î‡πÄ‡∏´‡πá‡∏ô"],
                    datatype=["number", "str", "str", "str", "str", "str"],
                    interactive=False,
                    wrap=True
                )

        # Status display
        tag_status = gr.HTML("")
        tag_status_display = gr.HTML("")  # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏™‡∏î‡∏á‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡∏ï‡πà‡∏≤‡∏á‡πÜ

    # ==================== END TAG MANAGEMENT TAB ====================

    with gr.Tab("üîë API Key Configuration"):
        gr.Markdown("## ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ API Key ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö AI Providers")
        gr.Markdown("‡πÉ‡∏™‡πà API Key ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏ú‡∏π‡πâ‡πÉ‡∏´‡πâ‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£ AI ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô")

        with gr.Row():
            with gr.Column():
                # Minimax API Key
                minimax_api_key = gr.Textbox(
                    value=os.getenv("MINIMAX_API_KEY", ""),
                    label="Minimax API Key",
                    type="password",
                    placeholder="‡πÉ‡∏™‡πà API Key ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Minimax",
                    info="‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏• Minimax (abab6.5, abab6.5s, abab5.5)"
                )
                minimax_status = gr.HTML("")

            with gr.Column():
                # Manus API Key
                manus_api_key = gr.Textbox(
                    value=os.getenv("MANUS_API_KEY", ""),
                    label="Manus API Key",
                    type="password",
                    placeholder="‡πÉ‡∏™‡πà API Key ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Manus",
                    info="‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏• Manus (manus-code, manus-reasoning, manus-vision)"
                )
                manus_status = gr.HTML("")

        with gr.Row():
            with gr.Column():
                # Gemini API Key
                gemini_api_key = gr.Textbox(
                    value=os.getenv("GEMINI_API_KEY", ""),
                    label="Google Gemini API Key",
                    type="password",
                    placeholder="‡πÉ‡∏™‡πà API Key ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Google Gemini",
                    info="‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏• Gemini (gemini-2.0-flash-exp, gemini-1.5-pro, gemini-1.5-flash)"
                )
                gemini_status = gr.HTML("")

            with gr.Column():
                # OpenAI API Key
                openai_api_key = gr.Textbox(
                    value=os.getenv("OPENAI_API_KEY", ""),
                    label="OpenAI API Key (ChatGPT)",
                    type="password",
                    placeholder="‡πÉ‡∏™‡πà API Key ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö OpenAI",
                    info="‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏• ChatGPT (gpt-4o, gpt-4o-mini, gpt-4-turbo, gpt-3.5-turbo)"
                )
                openai_status = gr.HTML("")

        with gr.Row():
            with gr.Column():
                # Zhipu AI API Key
                zhipu_api_key = gr.Textbox(
                    value=os.getenv("ZHIPU_API_KEY", ""),
                    label="Zhipu AI API Key (GLM)",
                    type="password",
                    placeholder="‡πÉ‡∏™‡πà API Key ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Zhipu AI (z.ai)",
                    info="‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏• GLM-4.6 (GLM-4.6, glm-4, glm-4v, glm-3-turbo) - ‡∏Å‡∏î '‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏• Zhipu' ‡∏Å‡πà‡∏≠‡∏ô‡πÉ‡∏ä‡πâ"
                )
                zhipu_status = gr.HTML("")

        # Test connection buttons
        with gr.Row():
            test_minimax_btn = gr.Button("‡∏ó‡∏î‡∏™‡∏≠‡∏ö Minimax", variant="secondary")
            test_manus_btn = gr.Button("‡∏ó‡∏î‡∏™‡∏≠‡∏ö Manus", variant="secondary")
            test_gemini_btn = gr.Button("‡∏ó‡∏î‡∏™‡∏≠‡∏ö Gemini", variant="secondary")
            test_openai_btn = gr.Button("‡∏ó‡∏î‡∏™‡∏≠‡∏ö OpenAI", variant="secondary")
            test_zhipu_btn = gr.Button("‡∏ó‡∏î‡∏™‡∏≠‡∏ö Zhipu AI", variant="secondary")

        # Save configuration button
        save_config_btn = gr.Button("‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤", variant="primary")
        config_status = gr.HTML("")

        def test_api_connection(provider_name, api_key):
            """Test API connection for a provider"""
            if not api_key or not api_key.strip():
                return f"‚ùå ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡πÉ‡∏™‡πà API Key ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö {provider_name}"

            try:
                if provider_name == "minimax":
                    client = openai.OpenAI(api_key=api_key, base_url="https://api.minimax.chat/v1")
                    response = client.chat.completions.create(
                        model="abab6.5s",
                        messages=[{"role": "user", "content": "Hello"}],
                        max_tokens=10
                    )
                    return f"‚úÖ {provider_name} API Key ‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡πÅ‡∏•‡∏∞‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô"

                elif provider_name == "manus":
                    client = openai.OpenAI(api_key=api_key, base_url="https://api.manus.ai/v1")
                    response = client.chat.completions.create(
                        model="manus-code",
                        messages=[{"role": "user", "content": "Hello"}],
                        max_tokens=10
                    )
                    return f"‚úÖ {provider_name} API Key ‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡πÅ‡∏•‡∏∞‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô"

                elif provider_name == "gemini":
                    if not GEMINI_AVAILABLE:
                        return f"‚ùå {provider_name} library ‡πÑ‡∏°‡πà‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á: pip install google-generativeai"

                    try:
                        genai.configure(api_key=api_key)
                        model = genai.GenerativeModel('gemini-2.5-flash')
                        response = model.generate_content("Hello")
                        return f"‚úÖ {provider_name} API Key ‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡πÅ‡∏•‡∏∞‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô"
                    except Exception as api_error:
                        return f"‚ùå {provider_name} API Key ‡πÑ‡∏°‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏´‡∏£‡∏∑‡∏≠ library ‡∏°‡∏µ‡∏õ‡∏±‡∏ç‡∏´‡∏≤: {str(api_error)}"

                elif provider_name == "zhipu":
                    client = openai.OpenAI(api_key=api_key, base_url="https://api.z.ai/api/paas/v4")
                    response = client.chat.completions.create(
                        model="GLM-4.6",
                        messages=[{"role": "user", "content": "Hello"}],
                        max_tokens=10
                    )
                    return f"‚úÖ {provider_name} API Key ‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡πÅ‡∏•‡∏∞‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô"

                elif provider_name == "openai":
                    client = openai.OpenAI(api_key=api_key)
                    response = client.chat.completions.create(
                        model="gpt-3.5-turbo",
                        messages=[{"role": "user", "content": "Hello"}],
                        max_tokens=10
                    )
                    return f"‚úÖ {provider_name} API Key ‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡πÅ‡∏•‡∏∞‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô"

            except Exception as e:
                return f"‚ùå {provider_name} API Key ‡πÑ‡∏°‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á: {str(e)}"

        def save_api_config(minimax_key, manus_key, gemini_key, openai_key, zhipu_key):
            """Save API keys to environment variables"""
            env_file = ".env"
            lines = []

            # Read existing .env file
            if os.path.exists(env_file):
                with open(env_file, 'r', encoding='utf-8') as f:
                    lines = f.readlines()

            # Update or add API keys
            updates = {
                "MINIMAX_API_KEY": minimax_key,
                "MANUS_API_KEY": manus_key,
                "GEMINI_API_KEY": gemini_key,
                "OPENAI_API_KEY": openai_key,
                "ZHIPU_API_KEY": zhipu_key
            }

            # Remove existing API key lines
            lines = [line for line in lines if not any(key in line for key in updates.keys())]

            # Add new API key lines
            for key, value in updates.items():
                if value and value.strip():
                    lines.append(f"{key}={value}\n")

            # Write back to .env file
            with open(env_file, 'w', encoding='utf-8') as f:
                f.writelines(lines)

            # Update environment variables
            os.environ["MINIMAX_API_KEY"] = minimax_key
            os.environ["MANUS_API_KEY"] = manus_key
            os.environ["GEMINI_API_KEY"] = gemini_key
            os.environ["OPENAI_API_KEY"] = openai_key
            os.environ["ZHIPU_API_KEY"] = zhipu_key

            return "‚úÖ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ API Key ‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏£‡πâ‡∏≠‡∏¢‡πÅ‡∏•‡πâ‡∏ß ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏£‡∏µ‡∏™‡∏ï‡∏≤‡∏£‡πå‡∏ó‡πÅ‡∏≠‡∏õ‡∏û‡∏•‡∏¥‡πÄ‡∏Ñ‡∏ä‡∏±‡∏ô‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡∏°‡∏µ‡∏ú‡∏•"

        # Connect test buttons
        test_minimax_btn.click(
            fn=lambda k: test_api_connection("minimax", k),
            inputs=[minimax_api_key],
            outputs=[minimax_status]
        )

        test_manus_btn.click(
            fn=lambda k: test_api_connection("manus", k),
            inputs=[manus_api_key],
            outputs=[manus_status]
        )

        test_gemini_btn.click(
            fn=lambda k: test_api_connection("gemini", k),
            inputs=[gemini_api_key],
            outputs=[gemini_status]
        )

        test_openai_btn.click(
            fn=lambda k: test_api_connection("openai", k),
            inputs=[openai_api_key],
            outputs=[openai_status]
        )

        test_zhipu_btn.click(
            fn=lambda k: test_api_connection("zhipu", k),
            inputs=[zhipu_api_key],
            outputs=[zhipu_status]
        )

        # Add button to check available models
        with gr.Row():
            check_zhipu_models_btn = gr.Button("‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏• Zhipu", variant="secondary", size="sm")

        def check_zhipu_models(api_key):
            if not api_key or not api_key.strip():
                return "‚ùå ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡πÉ‡∏™‡πà API Key ‡∏Å‡πà‡∏≠‡∏ô"

            try:
                import requests
                headers = {
                    "Authorization": f"Bearer {api_key}",
                    "Content-Type": "application/json"
                }

                # Try to get available models
                response = requests.get("https://api.z.ai/api/paas/v4/models", headers=headers, timeout=10)
                if response.status_code == 200:
                    models_data = response.json()
                    models = [model.get('id', 'unknown') for model in models_data.get('data', [])]
                    return f"‚úÖ ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡∏°‡∏µ: {', '.join(models[:10])}"
                else:
                    # If models endpoint doesn't work, try common model names
                    common_models = ["GLM-4.6", "glm-4.6", "glm-4", "glm-4v", "glm-3-turbo"]
                    results = []

                    for model in common_models:
                        try:
                            test_response = requests.post(
                                "https://api.z.ai/api/paas/v4/chat/completions",
                                headers=headers,
                                json={
                                    "model": model,
                                    "messages": [{"role": "user", "content": "test"}],
                                    "max_tokens": 1
                                },
                                timeout=5
                            )
                            if test_response.status_code == 200:
                                results.append(model)
                        except:
                            continue

                    if results:
                        return f"‚úÖ ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡πÑ‡∏î‡πâ: {', '.join(results)}"
                    else:
                        return f"‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡πÑ‡∏î‡πâ ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö API Key ‡∏´‡∏£‡∏∑‡∏≠ endpoint"

            except Exception as e:
                return f"‚ùå ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÑ‡∏î‡πâ: {str(e)}"

        check_zhipu_models_btn.click(
            fn=check_zhipu_models,
            inputs=[zhipu_api_key],
            outputs=[zhipu_status]
        )

        # Connect save button
        save_config_btn.click(
            fn=save_api_config,
            inputs=[minimax_api_key, manus_api_key, gemini_api_key, openai_api_key, zhipu_api_key],
            outputs=[config_status]
        )

    with gr.Tab("üéõÔ∏è ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ Models"):
        gr.Markdown("## ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ Models ‡∏Ç‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞ AI Provider")
        gr.Markdown("‡πÄ‡∏û‡∏¥‡πà‡∏°/‡∏•‡∏ö/‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç models ‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡πÑ‡∏î‡πâ‡πÉ‡∏ô‡πÅ‡∏ï‡πà‡∏•‡∏∞ provider")

        with gr.Row():
            with gr.Column(scale=1):
                # Provider selector
                manage_provider_selector = gr.Dropdown(
                    choices=[(AI_PROVIDERS[p]["name"], p) for p in AI_PROVIDERS.keys()],
                    value="gemini",
                    label="‡πÄ‡∏•‡∏∑‡∏≠‡∏Å AI Provider ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£",
                    info="‡πÄ‡∏•‡∏∑‡∏≠‡∏Å provider ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ models"
                )

                # Current models display - initialize with Gemini models
                initial_provider = "gemini"
                initial_provider_models = AI_PROVIDERS[initial_provider]["models"]
                initial_default = AI_PROVIDERS[initial_provider]["default_model"]
                initial_models_text = "\n".join([f"‚Ä¢ {m}" + (" ‚≠ê" if m == initial_default else "") for m in initial_provider_models])

                current_models_display = gr.Textbox(
                    label="Models ‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô",
                    lines=8,
                    interactive=False,
                    value=initial_models_text,
                    placeholder="‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£ models..."
                )

                # Default model display
                current_default_display = gr.Textbox(
                    label="Model ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô",
                    interactive=False,
                    value=initial_default,
                    placeholder="model ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô..."
                )

            with gr.Column(scale=1):
                gr.Markdown("### ‡πÄ‡∏û‡∏¥‡πà‡∏° Model ‡πÉ‡∏´‡∏°‡πà")
                new_model_name = gr.Textbox(
                    label="‡∏ä‡∏∑‡πà‡∏≠ Model",
                    placeholder="‡πÄ‡∏ä‡πà‡∏ô gemini-3.0-pro, gpt-5-turbo",
                    info="‡πÉ‡∏™‡πà‡∏ä‡∏∑‡πà‡∏≠ model ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÄ‡∏û‡∏¥‡πà‡∏°"
                )
                add_model_btn = gr.Button("‚ûï ‡πÄ‡∏û‡∏¥‡πà‡∏° Model", variant="primary")
                add_model_status = gr.HTML("")

                gr.Markdown("### ‡∏•‡∏ö Model")
                remove_model_selector = gr.Dropdown(
                    choices=initial_provider_models,
                    label="‡πÄ‡∏•‡∏∑‡∏≠‡∏Å Model ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏•‡∏ö",
                    info="‡πÄ‡∏•‡∏∑‡∏≠‡∏Å model ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏•‡∏ö‡∏≠‡∏≠‡∏Å"
                )
                remove_model_btn = gr.Button("üóëÔ∏è ‡∏•‡∏ö Model", variant="stop")
                remove_model_status = gr.HTML("")

                gr.Markdown("### ‡∏ï‡∏±‡πâ‡∏á Model ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô")
                set_default_selector = gr.Dropdown(
                    choices=initial_provider_models,
                    label="‡πÄ‡∏•‡∏∑‡∏≠‡∏Å Model ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô",
                    info="‡πÄ‡∏•‡∏∑‡∏≠‡∏Å model ‡∏ó‡∏µ‡πà‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡πà‡∏≤‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô"
                )
                set_default_btn = gr.Button("‚≠ê ‡∏ï‡∏±‡πâ‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡πà‡∏≤‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô", variant="secondary")
                set_default_status = gr.HTML("")

        # Functions for model management
        def display_provider_models(provider):
            """Display current models and default for selected provider"""
            if provider not in AI_PROVIDERS:
                return "Provider ‡πÑ‡∏°‡πà‡∏û‡∏ö", "", [], []

            config = AI_PROVIDERS[provider]
            models = config.get("models", [])
            default = config.get("default_model", "")

            models_text = "\n".join([f"‚Ä¢ {m}" + (" ‚≠ê" if m == default else "") for m in models])

            return models_text, default, gr.update(choices=models), gr.update(choices=models)

        def add_model_to_provider(provider, model_name):
            """Add new model to provider"""
            if not model_name or not model_name.strip():
                return "‚ùå ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡πÉ‡∏™‡πà‡∏ä‡∏∑‡πà‡∏≠ model", gr.update(), gr.update(), gr.update(), gr.update()

            model_name = model_name.strip()

            if provider not in AI_PROVIDERS:
                return "‚ùå Provider ‡πÑ‡∏°‡πà‡∏û‡∏ö", gr.update(), gr.update(), gr.update(), gr.update()

            if model_name in AI_PROVIDERS[provider]["models"]:
                return f"‚ö†Ô∏è Model '{model_name}' ‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß", gr.update(), gr.update(), gr.update(), gr.update()

            AI_PROVIDERS[provider]["models"].append(model_name)
            models = AI_PROVIDERS[provider]["models"]
            default = AI_PROVIDERS[provider]["default_model"]
            models_text = "\n".join([f"‚Ä¢ {m}" + (" ‚≠ê" if m == default else "") for m in models])

            return (
                f"‚úÖ ‡πÄ‡∏û‡∏¥‡πà‡∏° model '{model_name}' ‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏£‡πâ‡∏≠‡∏¢",
                models_text,
                default,
                gr.update(choices=models),
                gr.update(choices=models)
            )

        def remove_model_from_provider(provider, model_name):
            """Remove model from provider"""
            if not model_name:
                return "‚ùå ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡πÄ‡∏•‡∏∑‡∏≠‡∏Å model ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏•‡∏ö", gr.update(), gr.update(), gr.update(), gr.update()

            if provider not in AI_PROVIDERS:
                return "‚ùå Provider ‡πÑ‡∏°‡πà‡∏û‡∏ö", gr.update(), gr.update(), gr.update(), gr.update()

            if model_name == AI_PROVIDERS[provider]["default_model"]:
                return "‚ùå ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏•‡∏ö model ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡πÑ‡∏î‡πâ ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏ï‡∏±‡πâ‡∏á model ‡∏≠‡∏∑‡πà‡∏ô‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡πà‡∏≤‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏Å‡πà‡∏≠‡∏ô", gr.update(), gr.update(), gr.update(), gr.update()

            if model_name not in AI_PROVIDERS[provider]["models"]:
                return f"‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö model '{model_name}'", gr.update(), gr.update(), gr.update(), gr.update()

            AI_PROVIDERS[provider]["models"].remove(model_name)
            models = AI_PROVIDERS[provider]["models"]
            default = AI_PROVIDERS[provider]["default_model"]
            models_text = "\n".join([f"‚Ä¢ {m}" + (" ‚≠ê" if m == default else "") for m in models])

            return (
                f"‚úÖ ‡∏•‡∏ö model '{model_name}' ‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏£‡πâ‡∏≠‡∏¢",
                models_text,
                default,
                gr.update(choices=models),
                gr.update(choices=models)
            )

        def set_default_model(provider, model_name):
            """Set default model for provider"""
            if not model_name:
                return "‚ùå ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡πÄ‡∏•‡∏∑‡∏≠‡∏Å model", gr.update(), gr.update()

            if provider not in AI_PROVIDERS:
                return "‚ùå Provider ‡πÑ‡∏°‡πà‡∏û‡∏ö", gr.update(), gr.update()

            if model_name not in AI_PROVIDERS[provider]["models"]:
                return f"‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö model '{model_name}'", gr.update(), gr.update()

            AI_PROVIDERS[provider]["default_model"] = model_name
            models = AI_PROVIDERS[provider]["models"]
            models_text = "\n".join([f"‚Ä¢ {m}" + (" ‚≠ê" if m == model_name else "") for m in models])

            return f"‚úÖ ‡∏ï‡∏±‡πâ‡∏á '{model_name}' ‡πÄ‡∏õ‡πá‡∏ô model ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏£‡πâ‡∏≠‡∏¢", models_text, model_name

        # Event handlers
        manage_provider_selector.change(
            fn=display_provider_models,
            inputs=manage_provider_selector,
            outputs=[current_models_display, current_default_display, remove_model_selector, set_default_selector]
        )

        add_model_btn.click(
            fn=add_model_to_provider,
            inputs=[manage_provider_selector, new_model_name],
            outputs=[add_model_status, current_models_display, current_default_display, remove_model_selector, set_default_selector]
        )

        remove_model_btn.click(
            fn=remove_model_from_provider,
            inputs=[manage_provider_selector, remove_model_selector],
            outputs=[remove_model_status, current_models_display, current_default_display, remove_model_selector, set_default_selector]
        )

        set_default_btn.click(
            fn=set_default_model,
            inputs=[manage_provider_selector, set_default_selector],
            outputs=[set_default_status, current_models_display, current_default_display]
        )

    with gr.Tab("‡πÅ‡∏ä‡∏ó"):
        # Simple provider initialization - start fast
        basic_providers = ["ollama"]

        # Check for external providers quickly (no blocking)
        external_providers = []
        if os.getenv("GEMINI_API_KEY") and GEMINI_AVAILABLE:
            external_providers.append("gemini")
        if os.getenv("OPENAI_API_KEY"):
            external_providers.append("chatgpt")
        if os.getenv("MINIMAX_API_KEY"):
            external_providers.append("minimax")
        if os.getenv("MANUS_API_KEY"):
            external_providers.append("manus")
        if os.getenv("ZHIPU_API_KEY"):
            external_providers.append("zhipu")

        all_providers = basic_providers + external_providers
        provider_choices = [(AI_PROVIDERS[p]["name"], p) for p in all_providers if p in AI_PROVIDERS]

        logging.info(f"Quick available providers: {all_providers}")

        # Start with default provider (Gemini)
        provider_selector = gr.Dropdown(
            choices=provider_choices,
            value=DEFAULT_AI_PROVIDER,
            label="‡πÄ‡∏•‡∏∑‡∏≠‡∏Å AI Provider",
            info="‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏ú‡∏π‡πâ‡πÉ‡∏´‡πâ‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£ AI ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ"
        )
        selected_provider = gr.State(value=DEFAULT_AI_PROVIDER)

        # Combined update function
        def update_provider_and_models(provider):
            models = get_provider_models(provider)
            default_model = AI_PROVIDERS[provider]["default_model"] if models else None
            if models and default_model not in models:
                default_model = models[0] if models else None
            print(f"Provider: {provider}, Models: {models}, Default: {default_model}")  # Debug
            return gr.update(choices=models, value=default_model), provider, default_model

        # Start with Gemini models as default
        initial_models = get_provider_models(DEFAULT_AI_PROVIDER)
        initial_default_model = AI_PROVIDERS[DEFAULT_AI_PROVIDER]["default_model"] if initial_models else None

        model_selector = gr.Dropdown(
            choices=initial_models,
            value=initial_default_model,
            label="‡πÄ‡∏•‡∏∑‡∏≠‡∏Å LLM Model",
            allow_custom_value=True
        )
        selected_model = gr.State(value=initial_default_model)

        # Update both provider state and models when provider changes
        provider_selector.change(
            fn=update_provider_and_models,
            inputs=provider_selector,
            outputs=[model_selector, selected_provider, selected_model]
        )

        model_selector.change(fn=lambda x: x, inputs=model_selector, outputs=selected_model)

        # Function to update bot model displays
        def update_bot_model_displays(provider, model):
            """Update all bot model display textboxes with current model/provider"""
            if model and provider:
                display_text = f"‚úÖ ‡πÉ‡∏ä‡πâ‡∏à‡∏≤‡∏Å‡∏´‡∏ô‡πâ‡∏≤‡πÅ‡∏ä‡∏ó: {model} ({provider.upper()})"
            else:
                display_text = "‡∏£‡∏≠‡∏Å‡∏≤‡∏£‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏à‡∏≤‡∏Å‡∏´‡∏ô‡πâ‡∏≤‡πÅ‡∏ä‡∏ó..."
            return display_text, display_text, display_text

        # Update bot displays when model or provider changes
        provider_selector.change(
            fn=update_bot_model_displays,
            inputs=[provider_selector, model_selector],
            outputs=[bot_model_display, line_model_display, fb_model_display]
        )

        model_selector.change(
            fn=update_bot_model_displays,
            inputs=[provider_selector, model_selector],
            outputs=[bot_model_display, line_model_display, fb_model_display]
        )

        # Choice ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å RAG Mode
        rag_mode_selector = gr.Radio(
            choices=[
                ("üìñ Standard RAG - ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏à‡∏≤‡∏Å‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô", "standard"),
                ("üß† Enhanced RAG - ‡∏à‡∏î‡∏à‡∏≥‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡∏Å‡∏≤‡∏£‡∏™‡∏ô‡∏ó‡∏ô‡∏≤", "enhanced")
            ],
            value="standard",
            label="‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÇ‡∏´‡∏°‡∏î RAG",
            info="Enhanced RAG ‡∏à‡∏∞‡∏à‡∏î‡∏à‡∏≥‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡∏Å‡∏≤‡∏£‡∏™‡∏ô‡∏ó‡∏ô‡∏≤‡πÅ‡∏•‡∏∞‡πÉ‡∏ä‡πâ‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°"
        )
        selected_rag_mode = gr.State(value="standard")  # ‡πÄ‡∏Å‡πá‡∏ö‡πÑ‡∏ß‡πâ‡πÉ‡∏ô state
        rag_mode_status = gr.Textbox(label="‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞ RAG Mode", value="üìñ Standard RAG Mode", interactive=False)

        # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡πÅ‡∏™‡∏î‡∏á‡πÅ‡∏´‡∏•‡πà‡∏á‡∏ó‡∏µ‡πà‡∏°‡∏≤‡∏Ç‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
        with gr.Row():
            show_source_checkbox = gr.Checkbox(
                label="üîç ‡πÅ‡∏™‡∏î‡∏á‡πÅ‡∏´‡∏•‡πà‡∏á‡∏ó‡∏µ‡πà‡∏°‡∏≤‡∏Ç‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•",
                value=False,
                info="‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Å‡∏≤‡∏£‡∏£‡∏∞‡∏ö‡∏∏‡πÅ‡∏´‡∏•‡πà‡∏á‡∏ó‡∏µ‡πà‡∏°‡∏≤‡∏Ç‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏ô‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö"
            )

            formal_style_checkbox = gr.Checkbox(
                label="üìù ‡∏™‡πÑ‡∏ï‡∏•‡πå‡∏Å‡∏≤‡∏£‡∏ï‡∏≠‡∏ö‡πÄ‡∏õ‡πá‡∏ô‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏£",
                value=False,
                info="‡πÉ‡∏ä‡πâ‡∏†‡∏≤‡∏©‡∏≤‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡πÅ‡∏•‡∏∞‡∏™‡∏∏‡∏†‡∏≤‡∏û‡∏°‡∏≤‡∏Å‡∏Ç‡∏∂‡πâ‡∏ô"
            )

        # LightRAG Graph Reasoning Options
        with gr.Accordion("üß† LightRAG Graph Reasoning (Advanced)", open=False):
            gr.Markdown("""
            **üî¨ ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡∏â‡∏•‡∏≤‡∏î‡∏î‡πâ‡∏ß‡∏¢ Graph Reasoning:**
            ‚Ä¢ ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏±‡∏°‡∏û‡∏±‡∏ô‡∏ò‡πå‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á concepts
            ‚Ä¢ Multi-hop reasoning ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏ã‡∏±‡∏ö‡∏ã‡πâ‡∏≠‡∏ô
            ‚Ä¢ ‡πÅ‡∏™‡∏î‡∏á‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡πÇ‡∏¢‡∏á‡∏ó‡∏µ‡πà‡∏ã‡πà‡∏≠‡∏ô‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
            """)

            with gr.Row():
                use_graph_reasoning = gr.Checkbox(
                    label="üß† ‡πÄ‡∏õ‡∏¥‡∏î‡πÉ‡∏ä‡πâ Graph Reasoning",
                    value=False,
                    info="‡πÉ‡∏ä‡πâ LightRAG ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏±‡∏°‡∏û‡∏±‡∏ô‡∏ò‡πå‡πÅ‡∏•‡∏∞‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡πÇ‡∏¢‡∏á concepts"
                )

            with gr.Row():
                reasoning_mode = gr.Dropdown(
                    choices=[
                        ("üîÑ Hybrid (‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥)", "hybrid"),
                        ("üéØ Local (‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÄ‡∏à‡∏≤‡∏∞‡∏à‡∏á)", "local"),
                        ("üåê Global (‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏•‡∏∏‡∏°)", "global"),
                        ("‚ö° Naive (‡πÄ‡∏£‡πá‡∏ß)", "naive")
                    ],
                    value="hybrid",
                    label="üìä ‡πÇ‡∏´‡∏°‡∏î‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå",
                    info="‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå Graph Reasoning"
                )

            with gr.Row():
                multi_hop_enabled = gr.Checkbox(
                    label="üîÑ Multi-Hop Reasoning",
                    value=False,
                    info="‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏±‡∏°‡∏û‡∏±‡∏ô‡∏ò‡πå‡∏Ç‡∏±‡πâ‡∏ô‡∏™‡∏π‡∏á‡∏´‡∏•‡∏≤‡∏¢ step"
                )

                hop_count = gr.Slider(
                    minimum=2,
                    maximum=5,
                    value=2,
                    step=1,
                    label="üî¢ ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô Hop",
                    visible=False,
                    info="‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå Multi-Hop"
                )

            # Toggle hop count visibility
            def toggle_hop_count(enabled):
                return gr.update(visible=enabled)

            multi_hop_enabled.change(
                fn=toggle_hop_count,
                inputs=multi_hop_enabled,
                outputs=hop_count
            )

            # LightRAG Status Display
            with gr.Row():
                lightrag_status_display = gr.Textbox(
                    label="üìä ‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞ LightRAG",
                    value="‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö...",
                    interactive=False,
                    lines=2
                )

            # Update status on interface load
            demo.load(
                fn=update_lightrag_status,
                inputs=[],
                outputs=[lightrag_status_display]
            )

            # Test Graph Reasoning Button
            with gr.Row():
                test_lightrag_btn = gr.Button("üß™ ‡∏ó‡∏î‡∏™‡∏≠‡∏ö Graph Reasoning", variant="secondary", size="sm")
                lightrag_test_output = gr.Textbox(
                    label="‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö",
                    interactive=False,
                    visible=False,
                    lines=3
                )

        
            # Update status on load and test
            test_lightrag_btn.click(
                fn=test_graph_reasoning_interface,
                inputs=[],
                outputs=[lightrag_test_output, lightrag_test_output]
            )

        # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏™‡πà‡∏á‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡πÑ‡∏õ‡∏¢‡∏±‡∏á‡πÅ‡∏û‡∏•‡∏ï‡∏ü‡∏≠‡∏£‡πå‡∏°‡∏≠‡∏∑‡πà‡∏ô
        with gr.Row():
            gr.Markdown("### üì§ ‡∏™‡πà‡∏á‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡πÑ‡∏õ‡∏¢‡∏±‡∏á‡πÅ‡∏û‡∏•‡∏ï‡∏ü‡∏≠‡∏£‡πå‡∏°:")

        with gr.Row():
            send_to_discord_checkbox = gr.Checkbox(
                label="ü§ñ Discord",
                value=False,
                info="‡∏™‡πà‡∏á‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡πÑ‡∏õ‡∏¢‡∏±‡∏á Discord channel"
            )

            send_to_line_checkbox = gr.Checkbox(
                label="üì± LINE OA",
                value=False,
                info="‡∏™‡πà‡∏á‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡πÑ‡∏õ‡∏¢‡∏±‡∏á LINE OA (‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ LINE_USER_ID)"
            )

            send_to_facebook_checkbox = gr.Checkbox(
                label="üí¨ Facebook Messenger",
                value=False,
                info="‡∏™‡πà‡∏á‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡πÑ‡∏õ‡∏¢‡∏±‡∏á Facebook Messenger (‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ FB_USER_ID)"
            )

        # ‡πÄ‡∏û‡∏¥‡πà‡∏° text input ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏£‡∏∞‡∏ö‡∏∏ user ID
        with gr.Row():
            line_user_id_input = gr.Textbox(
                label="LINE User ID (‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏™‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°)",
                placeholder="‡πÉ‡∏™‡πà LINE User ID ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏™‡πà‡∏á‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö",
                visible=False,
                info="‡∏£‡∏±‡∏ö User ID ‡∏à‡∏≤‡∏Å LINE Debug console ‡∏´‡∏£‡∏∑‡∏≠‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö"
            )

            fb_user_id_input = gr.Textbox(
                label="Facebook User ID (‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏™‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°)",
                placeholder="‡πÉ‡∏™‡πà Facebook User ID ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏™‡πà‡∏á‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö",
                visible=False,
                info="‡∏£‡∏±‡∏ö User ID ‡∏à‡∏≤‡∏Å Facebook Graph API"
            )

        # ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏™‡∏î‡∏á/‡∏ã‡πà‡∏≠‡∏ô user ID input
        def toggle_line_input(checked):
            return gr.update(visible=checked)

        def toggle_fb_input(checked):
            return gr.update(visible=checked)

        send_to_line_checkbox.change(
            fn=toggle_line_input,
            inputs=send_to_line_checkbox,
            outputs=line_user_id_input
        )

        send_to_facebook_checkbox.change(
            fn=toggle_fb_input,
            inputs=send_to_facebook_checkbox,
            outputs=fb_user_id_input
        )

        def update_rag_mode(mode):
            global RAG_MODE
            RAG_MODE = mode
            if mode == "enhanced":
                return "üß† Enhanced RAG Mode - ‡∏à‡∏∞‡∏à‡∏î‡∏à‡∏≥‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡∏Å‡∏≤‡∏£‡∏™‡∏ô‡∏ó‡∏ô‡∏≤"
            else:
                return "üìñ Standard RAG Mode - ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏à‡∏≤‡∏Å‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô"

        rag_mode_selector.change(
            fn=update_rag_mode,
            inputs=rag_mode_selector,
            outputs=rag_mode_status,
            queue=False
        )

        # Enhanced RAG Memory Status
        with gr.Accordion("üß† Enhanced RAG Memory Status", open=False):
            memory_status_output = gr.Textbox(label="‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Memory", lines=4, interactive=False)
            refresh_memory_button = gr.Button("‡∏£‡∏µ‡πÄ‡∏ü‡∏£‡∏ä‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Memory", size="sm")

        def get_memory_status():
            if RAG_MODE == "enhanced":
                try:
                    memory_info = enhanced_rag.get_memory_info()
                    return f"""üìä Memory Status:
‚Ä¢ Total memories: {memory_info['total_memories']}
‚Ä¢ Session memories: {memory_info['session_memories']}
‚Ä¢ Long-term memories: {memory_info['longterm_memories']}
‚Ä¢ Memory window: {memory_info['memory_window']}
"""
                except Exception as e:
                    return f"Error getting memory status: {str(e)}"
            else:
                return "Enhanced RAG is not active. Switch to Enhanced RAG mode to use memory features."

        refresh_memory_button.click(
            fn=get_memory_status,
            inputs=None,
            outputs=memory_status_output,
            queue=False
        )

        # Chat Bot
        chatbot = gr.Chatbot(type="messages")
        msg = gr.Textbox(label="‡∏ñ‡∏≤‡∏°‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö PDF")

        # Feedback Section
        with gr.Row():
            gr.Markdown("### üí° ‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏ô‡∏µ‡πâ‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà? ‡∏ä‡πà‡∏ß‡∏¢‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡πÉ‡∏´‡πâ‡∏î‡∏µ‡∏Ç‡∏∂‡πâ‡∏ô")

        with gr.Row():
            with gr.Column(scale=3):
                # ‡πÄ‡∏Å‡πá‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Å‡∏≤‡∏£‡∏™‡∏ô‡∏ó‡∏ô‡∏≤‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö feedback
                current_question = gr.State("")
                current_answer = gr.State("")
                current_sources = gr.State("")
                feedback_type_state = gr.State("")

                # Enhanced Feedback UI
                with gr.Row():
                    with gr.Column(scale=2):
                        # Rating scale (1-5 stars)
                        rating_slider = gr.Slider(
                            minimum=1,
                            maximum=5,
                            value=3,
                            step=1,
                            label="‚≠ê ‡πÉ‡∏´‡πâ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡∏û‡∏∂‡∏á‡∏û‡∏≠‡πÉ‡∏à (1-5)",
                            info="1=‡πÅ‡∏¢‡πà‡∏°‡∏≤‡∏Å, 5=‡∏î‡∏µ‡πÄ‡∏¢‡∏µ‡πà‡∏¢‡∏°"
                        )

                        # Quick feedback buttons
                        with gr.Row():
                            good_feedback_btn = gr.Button("üëç ‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á", variant="primary", size="sm")
                            bad_feedback_btn = gr.Button("üëé ‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î", variant="secondary", size="sm")

                    with gr.Column(scale=3):
                        # Feedback categories
                        feedback_category = gr.Radio(
                            choices=[
                                ("‚úÖ ‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á", "correct"),
                                ("‚ùå ‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î", "incorrect"),
                                ("ü§î ‡πÑ‡∏°‡πà‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°", "misunderstood"),
                                ("üìÑ ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏°‡πà‡∏Ñ‡∏£‡∏ö", "incomplete"),
                                ("üîó ‡πÅ‡∏´‡∏•‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ú‡∏¥‡∏î", "wrong_source"),
                                ("üîÑ ‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ context ‡πÄ‡∏û‡∏¥‡πà‡∏°", "need_context")
                            ],
                            value="correct",
                            label="üìã ‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó Feedback",
                            info="‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏ó‡∏µ‡πà‡∏ï‡∏£‡∏á‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î"
                        )

            with gr.Column(scale=4):
                # ‡∏ä‡πà‡∏≠‡∏á‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÉ‡∏™‡πà‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏¥‡∏î‡πÄ‡∏´‡πá‡∏ô‡πÅ‡∏•‡∏∞‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á
                with gr.Row():
                    user_comment = gr.Textbox(
                        label="üí¨ ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏¥‡∏î‡πÄ‡∏´‡πá‡∏ô‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏° (‡πÑ‡∏°‡πà‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö)",
                        placeholder="‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°‡∏ß‡πà‡∏≤‡∏ó‡∏≥‡πÑ‡∏°‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏ñ‡∏∂‡∏á‡∏ñ‡∏π‡∏Å‡∏´‡∏£‡∏∑‡∏≠‡∏ú‡∏¥‡∏î...",
                        lines=2
                    )

                with gr.Row():
                    corrected_answer = gr.Textbox(
                        label="‚úÖ ‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á (‡∏ñ‡πâ‡∏≤‡∏ú‡∏¥‡∏î)",
                        placeholder="‡πÉ‡∏™‡πà‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏ó‡∏µ‡πà‡∏ô‡∏µ‡πà...",
                        lines=3,
                        visible=False
                    )

                # Source relevance rating
                with gr.Row():
                    source_relevance = gr.Radio(
                        choices=[
                            ("üéØ ‡πÅ‡∏´‡∏•‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏™‡∏π‡∏á", "high"),
                            ("üìä ‡πÅ‡∏´‡∏•‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏õ‡∏≤‡∏ô‡∏Å‡∏•‡∏≤‡∏á", "medium"),
                            ("‚ùå ‡πÅ‡∏´‡∏•‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏°‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á", "low")
                        ],
                        value="high",
                        label="üìé ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏Ç‡∏≠‡∏á‡πÅ‡∏´‡∏•‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•",
                        visible=True
                    )

                with gr.Row():
                    submit_feedback_btn = gr.Button("üìù ‡∏™‡πà‡∏á Feedback", variant="primary", visible=False)
                    feedback_status = gr.Textbox(label="‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞", interactive=False, visible=False)

        # Clear button
        clear_chat = gr.Button("‡∏•‡πâ‡∏≤‡∏á")
        # Submit function 
        msg.submit(
            fn=user,
            inputs=[msg, chatbot],
            outputs=[msg, chatbot],
            queue=False
        ).then(
            fn=chatbot_interface,
            inputs=[chatbot, selected_model, selected_provider, show_source_checkbox, formal_style_checkbox,
                   send_to_discord_checkbox, send_to_line_checkbox, send_to_facebook_checkbox,
                   line_user_id_input, fb_user_id_input, use_graph_reasoning, reasoning_mode,
                   multi_hop_enabled, hop_count],
            outputs=[chatbot, current_question, current_answer, current_sources]
        )
        clear_chat.click(lambda: [], None, chatbot, queue=False)

        # ==================== FEEDBACK EVENT HANDLERS ====================

        def on_feedback_category_change(category):
            """‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó feedback"""
            if category in ["incorrect", "incomplete"]:
                return (
                    gr.update(visible=True),   # corrected_answer
                    gr.update(visible=True),   # submit_feedback_btn
                    gr.update(visible=True),   # feedback_status
                    "‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏£‡∏∞‡∏ö‡∏∏‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á..."
                )
            else:
                return (
                    gr.update(visible=False),  # corrected_answer
                    gr.update(visible=True),   # submit_feedback_btn
                    gr.update(visible=True),   # feedback_status
                    "‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏™‡πà‡∏á feedback..."
                )

        def on_good_feedback():
            """‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏Å‡∏î‡∏õ‡∏∏‡πà‡∏° üëç"""
            return (
                gr.update(value="correct"),    # feedback_category
                gr.update(visible=False),      # corrected_answer
                gr.update(visible=True),       # submit_feedback_btn
                gr.update(visible=True),       # feedback_status
                "‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏™‡πà‡∏á feedback ‡∏ß‡πà‡∏≤‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á..."
            )

        def on_bad_feedback():
            """‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏Å‡∏î‡∏õ‡∏∏‡πà‡∏° üëé"""
            return (
                gr.update(value="incorrect"),  # feedback_category
                gr.update(visible=True),       # corrected_answer
                gr.update(visible=True),       # submit_feedback_btn
                gr.update(visible=True),       # feedback_status
                "‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏£‡∏∞‡∏ö‡∏∏‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á..."
            )

        def submit_feedback_handler(category, rating, question, answer, user_comment, corrected_answer, model, source_relevance):
            """Enhanced feedback handler ‡∏™‡πà‡∏á feedback ‡πÑ‡∏õ‡∏¢‡∏±‡∏á‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•"""
            if not question or not answer:
                return "‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Å‡∏≤‡∏£‡∏™‡∏ô‡∏ó‡∏ô‡∏≤ ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏ñ‡∏≤‡∏°‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÉ‡∏´‡∏°‡πà"

            # ‡∏™‡∏£‡πâ‡∏≤‡∏á detailed feedback comment
            detailed_comment = f"Category: {category}, Rating: {rating}/5, Source Relevance: {source_relevance}"
            if user_comment.strip():
                detailed_comment += f", Comment: {user_comment}"

            # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó feedback ‡∏ï‡∏≤‡∏° category
            if category == "correct":
                f_type = "good"
                corrected = ""
            else:
                f_type = "bad"
                corrected = corrected_answer if corrected_answer else "‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏£‡∏∞‡∏ö‡∏∏"

            # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏•‡∏á‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
            if save_feedback(question, answer, f_type, detailed_comment, corrected, model, ""):

                # ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ corrected answer ‡πÉ‡∏´‡πâ‡∏ô‡∏≥‡πÑ‡∏õ‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á RAG ‡∏ó‡∏±‡∏ô‡∏ó‡∏µ
                if corrected and corrected != "‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏£‡∏∞‡∏ö‡∏∏":
                    apply_feedback_to_rag(question, corrected, confidence=rating/5.0)

                # ‡∏ñ‡πâ‡∏≤ rating ‡∏ï‡πà‡∏≥‡∏°‡∏≤‡∏Å ‡πÉ‡∏´‡πâ log ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå
                if rating <= 2:
                    logging.warning(f"‚ö†Ô∏è Low quality response detected: Rating={rating}, Category={category}")

                return f"‚úÖ ‡∏Ç‡∏≠‡∏ö‡∏Ñ‡∏∏‡∏ì‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö feedback ‡∏£‡∏∞‡∏î‡∏±‡∏ö {rating}/5! ‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏ô‡∏µ‡πâ‡∏ñ‡∏π‡∏Å‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏£‡∏∞‡∏ö‡∏ö‡πÅ‡∏•‡πâ‡∏ß"
            else:
                return "‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å feedback ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏•‡∏≠‡∏á‡πÉ‡∏´‡∏°‡πà"

        # Enhanced Feedback Event Handlers
        feedback_category.change(
            fn=on_feedback_category_change,
            inputs=[feedback_category],
            outputs=[corrected_answer, submit_feedback_btn, feedback_status, feedback_status]
        )

        good_feedback_btn.click(
            fn=on_good_feedback,
            inputs=[],
            outputs=[feedback_category, corrected_answer, submit_feedback_btn, feedback_status, feedback_status]
        )

        bad_feedback_btn.click(
            fn=on_bad_feedback,
            inputs=[],
            outputs=[feedback_category, corrected_answer, submit_feedback_btn, feedback_status, feedback_status]
        )

        submit_feedback_btn.click(
            fn=submit_feedback_handler,
            inputs=[feedback_category, rating_slider, current_question, current_answer,
                   user_comment, corrected_answer, selected_model, source_relevance],
            outputs=[feedback_status]
        ).then(
            fn=lambda: [gr.update(visible=False), gr.update(visible=False), gr.update(visible=False),
                       gr.update(value=3), gr.update(value="correct"), ""],
            outputs=[submit_feedback_btn, feedback_status, corrected_answer, rating_slider, feedback_category, user_comment]
        )

        # ==================== TAG MANAGEMENT EVENT HANDLERS ====================

        # Function to update all tag-related components
        def update_all_tag_components():
            """Update all tag components"""
            try:
                tag_data, tag_choices, status_html, _ = refresh_tags_list()
                popular_data, feedback_data = update_tag_statistics()

                # Create choices for dropdown (just the labels)
                dropdown_choices = [choice[0] for choice in tag_choices]

                return tag_data, dropdown_choices, status_html, "", popular_data, feedback_data
            except Exception as e:
                logging.error(f"‚ùå Failed to update tag components: {str(e)}")
                return [], [], gr.HTML(f'<div style="color: red;">‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î: {str(e)}</div>'), "", [], []

        # Initialize tag lists on load
        demo.load(
            fn=update_all_tag_components,
            inputs=[],
            outputs=[tags_list, selected_tags_search, tag_status, tag_status_display, popular_tags, feedback_tags]
        )

        # Create new tag
        def handle_create_tag(name, color, description):
            """Handle tag creation and update all components"""
            result = create_new_tag(name, color, description)
            if result[0]:  # If successful, update all components
                return update_all_tag_components()
            else:
                # Return the error message from create_new_tag
                tags = get_all_tags()
                tag_choices = [(f"üè∑Ô∏è {tag[1]}", tag[0]) for tag in tags]
                dropdown_choices = [choice[0] for choice in tag_choices]
                tag_data = [[tag[0], tag[1], tag[2], tag[3] or "", tag[4]] for tag in tags]
                popular_data, feedback_data = update_tag_statistics()
                return tag_data, dropdown_choices, result[2], "", popular_data, feedback_data

        create_tag_btn.click(
            fn=handle_create_tag,
            inputs=[tag_name_input, tag_color_input, tag_desc_input],
            outputs=[tags_list, selected_tags_search, tag_status, tag_status_display, popular_tags, feedback_tags]
        )

        # Refresh tags list
        refresh_tags_btn.click(
            fn=update_all_tag_components,
            inputs=[],
            outputs=[tags_list, selected_tags_search, tag_status, tag_status_display, popular_tags, feedback_tags]
        )

        # Delete selected tag
        def delete_and_refresh(selected_row):
            """Delete tag and refresh all components"""
            if not selected_row or not selected_row.get("ID"):
                return [], [], gr.HTML('<div style="color: orange;">‚ö†Ô∏è ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡πÄ‡∏•‡∏∑‡∏≠‡∏Å Tag ‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏•‡∏ö</div>'), "", [], []

            tag_id = selected_row["ID"]
            tag_name = selected_row.get("‡∏ä‡∏∑‡πà‡∏≠ Tag", "")

            success = delete_tag(tag_id)
            if success:
                return update_all_tag_components()
            else:
                return [], [], gr.HTML('<div style="color: red;">‚ùå ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏•‡∏ö Tag ‡πÑ‡∏î‡πâ</div>'), "", [], []

        delete_tag_btn.click(
            fn=delete_and_refresh,
            inputs=[tags_list],
            outputs=[tags_list, selected_tags_search, tag_status, tag_status_display, popular_tags, feedback_tags]
        )

        # Search documents by tags
        search_by_tags_btn.click(
            fn=search_documents_by_selected_tags,
            inputs=[selected_tags_search],
            outputs=[search_results, tag_status_display]
        )

        # Load feedback by tag
        load_feedback_by_tag_btn.click(
            fn=load_feedback_by_selected_tag,
            inputs=[tag_feedback_selector],
            outputs=[tag_feedback_display, tag_status_display]
        )

        # ==================== END TAG MANAGEMENT EVENT HANDLERS ====================

if __name__ == "__main__":
    # ‡∏•‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• ‡∏≠‡∏≠‡∏Å‡∏à‡∏≤‡∏Å‡∏£‡∏∞‡∏ö‡∏ö ‡∏Å‡πà‡∏≠‡∏ô ‡πÄ‡∏£‡∏¥‡πà‡∏° Start Web
    clear_vector_db_and_images()

    # Update LightRAG status on load
    try:
        initial_lightrag_status = update_lightrag_status()
        logging.info(f"Initial LightRAG Status: {initial_lightrag_status}")
    except Exception as e:
        logging.warning(f"Failed to update LightRAG status on load: {e}")

    # Create and launch appropriate interface
    app_interface = create_authenticated_interface()
    app_interface.launch()
# Wrapper class for authenticated application
class RAGPDFApplication:
    """Wrapper class for RAG PDF application"""

    def __init__(self):
        self.interface = demo

    def create_interface(self):
        """Create the main RAG PDF interface"""
        try:
            return self.interface
        except Exception as e:
            logging.error(f"‚ùå Error creating interface: {e}")
            return None

    def get_interface(self):
        """Get the Gradio interface"""
        return self.interface

